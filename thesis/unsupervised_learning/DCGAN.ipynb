{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DCGAN\n",
    "*Zhiang Chen, April 2017*\n",
    "\n",
    "Using the package: https://github.com/sugyan/tf-dcgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import deepdish as dd\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from math import *\n",
    "import time\n",
    "from dcgan import DCGAN\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training dataset', (427680, 11), (427680, 10), (427680, 48, 48))\n",
      "('validation dataset', (71226, 11), (71226, 10), (71226, 48, 48))\n",
      "('testing dataset', (71280, 11), (71280, 10), (71280, 48, 48))\n"
     ]
    }
   ],
   "source": [
    "wd = os.getcwd()\n",
    "os.chdir('..')\n",
    "file_name = 'resized_depth_data2.h5'\n",
    "\n",
    "save = dd.io.load(file_name)\n",
    "\n",
    "train_objects = save['train_objects']\n",
    "train_orientations = save['train_orientations']\n",
    "train_values = save['train_values']\n",
    "valid_objects = save['valid_objects']\n",
    "valid_orientations = save['valid_orientations']\n",
    "valid_values = save['valid_values']\n",
    "test_objects = save['test_objects']\n",
    "test_orientations = save['test_orientations']\n",
    "test_values = save['test_values']\n",
    "value2object = save['value2object']\n",
    "object2value = save['object2value']\n",
    "del save\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "print('training dataset', train_objects.shape, train_orientations.shape, train_values.shape)\n",
    "print('validation dataset', valid_objects.shape, valid_orientations.shape, valid_values.shape)\n",
    "print('testing dataset', test_objects.shape, test_orientations.shape, test_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size = 48\n",
    "\n",
    "def randomize(dataset, classes, angles):\n",
    "    permutation = np.random.permutation(classes.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_classes = classes[permutation]\n",
    "    shuffled_angles = angles[permutation]\n",
    "    return shuffled_dataset, shuffled_classes, shuffled_angles\n",
    "\n",
    "train_dataset, train_classes, train_angles = randomize(train_values, train_objects, train_orientations)\n",
    "valid_dataset, valid_classes, valid_angles = randomize(valid_values, valid_objects, valid_orientations)\n",
    "test_dataset, test_classes, test_angles = randomize(test_values, test_objects, test_orientations)\n",
    "\n",
    "train_dataset = train_dataset[:150000,:,:]\n",
    "train_angles = train_angles[:150000,:]\n",
    "train_classes = train_classes[:150000,:]\n",
    "\n",
    "valid_dataset = valid_dataset[:5000,:,:]\n",
    "valid_angles = valid_angles[:5000,:]\n",
    "valid_classes = valid_classes[:5000,:]\n",
    "\n",
    "test_dataset = test_dataset[:5000,:,:]\n",
    "test_angles = test_angles[:5000,:]\n",
    "test_classes = test_classes[:5000,:]\n",
    "\n",
    "train_dataset = train_dataset.reshape(-1,image_size,image_size,1)\n",
    "test_dataset = test_dataset.reshape(-1,image_size,image_size,1)\n",
    "n_samples = train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('logdir', 'logdir',\n",
    "                           \"\"\"Directory where to write event logs and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_string('images_dir', 'images',\n",
    "                           \"\"\"Directory where to write generated images.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "s_size = 3 # s_size*2**4 == image_size\n",
    "\n",
    "dcgan = DCGAN(s_size=s_size)\n",
    "batch_size = dcgan.batch_size #128\n",
    "min_queue_examples = 5000\n",
    "\n",
    "train_images = tf.train.shuffle_batch([train_dataset], \\\n",
    "                                      batch_size=batch_size, \\\n",
    "                                      capacity=min_queue_examples + 3 * batch_size, \\\n",
    "                                      min_after_dequeue=min_queue_examples, \\\n",
    "                                      enqueue_many = True)\n",
    "\n",
    "test_images = tf.train.shuffle_batch([test_dataset], \\\n",
    "                                     batch_size=batch_size, \\\n",
    "                                     capacity=min_queue_examples + 3 * batch_size, \\\n",
    "                                     min_after_dequeue=min_queue_examples, \\\n",
    "                                     enqueue_many = True)\n",
    "\n",
    "losses = dcgan.loss(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name g loss is illegal; using g_loss instead.\n",
      "INFO:tensorflow:Summary name d loss is illegal; using d_loss instead.\n",
      "2017-04-05 20:33:49.149573: step     0, loss = (G: 9.97990894, D: 2.04693985) (3.013 sec/batch)\n",
      "2017-04-05 20:33:53.858786: step    20, loss = (G: 12.43218422, D: 0.29259077) (0.220 sec/batch)\n",
      "2017-04-05 20:33:58.281982: step    40, loss = (G: 14.49178314, D: 0.08343107) (0.222 sec/batch)\n",
      "2017-04-05 20:34:02.709500: step    60, loss = (G: 10.28990936, D: 0.08344577) (0.220 sec/batch)\n",
      "2017-04-05 20:34:07.139253: step    80, loss = (G: 10.75581932, D: 0.04554806) (0.222 sec/batch)\n",
      "2017-04-05 20:34:11.571624: step   100, loss = (G: 12.22644615, D: 0.04770917) (0.221 sec/batch)\n",
      "2017-04-05 20:34:16.142816: step   120, loss = (G: 13.00467682, D: 0.01293395) (0.222 sec/batch)\n",
      "2017-04-05 20:34:20.574331: step   140, loss = (G: 11.69369698, D: 0.02723908) (0.222 sec/batch)\n",
      "2017-04-05 20:34:25.007952: step   160, loss = (G: 12.36931324, D: 0.01532711) (0.222 sec/batch)\n",
      "2017-04-05 20:34:29.444735: step   180, loss = (G: 13.79192162, D: 0.00889512) (0.222 sec/batch)\n",
      "2017-04-05 20:34:33.879127: step   200, loss = (G: 6.92281294, D: 0.18412730) (0.221 sec/batch)\n",
      "2017-04-05 20:34:38.456737: step   220, loss = (G: 5.53893328, D: 0.26758230) (0.221 sec/batch)\n",
      "2017-04-05 20:34:42.889394: step   240, loss = (G: 21.56180954, D: 0.19326998) (0.221 sec/batch)\n",
      "2017-04-05 20:34:47.321636: step   260, loss = (G: 11.38140297, D: 0.01054843) (0.222 sec/batch)\n",
      "2017-04-05 20:34:51.763664: step   280, loss = (G: 10.50124741, D: 0.01020031) (0.222 sec/batch)\n",
      "2017-04-05 20:34:56.207008: step   300, loss = (G: 6.55273151, D: 0.06403716) (0.222 sec/batch)\n",
      "2017-04-05 20:35:00.789326: step   320, loss = (G: 14.59638596, D: 0.00774455) (0.222 sec/batch)\n",
      "2017-04-05 20:35:05.234497: step   340, loss = (G: 3.92074513, D: 0.45787266) (0.223 sec/batch)\n",
      "2017-04-05 20:35:09.678314: step   360, loss = (G: 7.18760777, D: 0.11491432) (0.222 sec/batch)\n",
      "2017-04-05 20:35:14.122629: step   380, loss = (G: 8.40650368, D: 0.01990351) (0.222 sec/batch)\n",
      "2017-04-05 20:35:18.564338: step   400, loss = (G: 7.75057793, D: 0.03346853) (0.221 sec/batch)\n",
      "2017-04-05 20:35:23.143968: step   420, loss = (G: 9.07668209, D: 0.02528620) (0.223 sec/batch)\n",
      "2017-04-05 20:35:27.583330: step   440, loss = (G: 9.12912941, D: 0.00902819) (0.223 sec/batch)\n",
      "2017-04-05 20:35:32.029680: step   460, loss = (G: 16.96226120, D: 0.00419740) (0.222 sec/batch)\n",
      "2017-04-05 20:35:36.474683: step   480, loss = (G: 7.52511692, D: 0.02293150) (0.222 sec/batch)\n",
      "2017-04-05 20:35:40.919109: step   500, loss = (G: 26.23209000, D: 3.29972553) (0.222 sec/batch)\n",
      "2017-04-05 20:35:45.503296: step   520, loss = (G: 3.76595926, D: 0.73460209) (0.223 sec/batch)\n",
      "2017-04-05 20:35:49.944719: step   540, loss = (G: 3.81982684, D: 0.23038560) (0.222 sec/batch)\n",
      "2017-04-05 20:35:54.388996: step   560, loss = (G: 5.56755638, D: 0.10755947) (0.223 sec/batch)\n",
      "2017-04-05 20:35:58.843401: step   580, loss = (G: 3.33892775, D: 0.27824071) (0.223 sec/batch)\n",
      "2017-04-05 20:36:03.297467: step   600, loss = (G: 3.62806177, D: 0.23003820) (0.222 sec/batch)\n",
      "2017-04-05 20:36:07.889905: step   620, loss = (G: 4.46272755, D: 0.18211859) (0.224 sec/batch)\n",
      "2017-04-05 20:36:12.357571: step   640, loss = (G: 8.25167465, D: 0.03348829) (0.227 sec/batch)\n",
      "2017-04-05 20:36:16.814818: step   660, loss = (G: 6.73513031, D: 0.04195245) (0.223 sec/batch)\n",
      "2017-04-05 20:36:21.278877: step   680, loss = (G: 11.32595348, D: 0.01017217) (0.222 sec/batch)\n",
      "2017-04-05 20:36:25.731030: step   700, loss = (G: 13.84546089, D: 0.00263656) (0.224 sec/batch)\n",
      "2017-04-05 20:36:30.331443: step   720, loss = (G: 12.33127594, D: 0.00533251) (0.223 sec/batch)\n",
      "2017-04-05 20:36:34.786286: step   740, loss = (G: 7.98780060, D: 0.06927148) (0.225 sec/batch)\n",
      "2017-04-05 20:36:39.248082: step   760, loss = (G: 5.58139372, D: 0.08365175) (0.223 sec/batch)\n",
      "2017-04-05 20:36:43.701982: step   780, loss = (G: 8.91369629, D: 0.02335256) (0.222 sec/batch)\n",
      "2017-04-05 20:36:48.174802: step   800, loss = (G: 15.62707901, D: 0.01960331) (0.223 sec/batch)\n",
      "2017-04-05 20:36:52.786614: step   820, loss = (G: 2.46676064, D: 0.70991600) (0.222 sec/batch)\n",
      "2017-04-05 20:36:57.269745: step   840, loss = (G: 15.97999954, D: 0.02178221) (0.225 sec/batch)\n",
      "2017-04-05 20:37:01.730001: step   860, loss = (G: 14.89201450, D: 0.12759817) (0.223 sec/batch)\n",
      "2017-04-05 20:37:06.190669: step   880, loss = (G: 5.77666616, D: 0.05544378) (0.223 sec/batch)\n",
      "2017-04-05 20:37:10.654373: step   900, loss = (G: 19.16528130, D: 0.04241321) (0.223 sec/batch)\n",
      "2017-04-05 20:37:15.315652: step   920, loss = (G: 15.04479027, D: 0.00287034) (0.223 sec/batch)\n",
      "2017-04-05 20:37:19.772713: step   940, loss = (G: 12.08292770, D: 0.00366762) (0.226 sec/batch)\n",
      "2017-04-05 20:37:24.269777: step   960, loss = (G: 19.47913933, D: 0.02369824) (0.232 sec/batch)\n",
      "2017-04-05 20:37:28.763013: step   980, loss = (G: 8.90825844, D: 0.01213667) (0.223 sec/batch)\n",
      "2017-04-05 20:37:33.222442: step  1000, loss = (G: 21.58462906, D: 0.00056570) (0.222 sec/batch)\n",
      "2017-04-05 20:37:37.887283: step  1020, loss = (G: 21.01678276, D: 0.00098771) (0.222 sec/batch)\n",
      "2017-04-05 20:37:42.367505: step  1040, loss = (G: 8.06393051, D: 0.02191740) (0.223 sec/batch)\n",
      "2017-04-05 20:37:46.828391: step  1060, loss = (G: 5.99635601, D: 0.83653814) (0.223 sec/batch)\n",
      "2017-04-05 20:37:51.302013: step  1080, loss = (G: 9.13181877, D: 0.09279395) (0.225 sec/batch)\n",
      "2017-04-05 20:37:55.769180: step  1100, loss = (G: 2.68523598, D: 0.57593334) (0.226 sec/batch)\n",
      "2017-04-05 20:38:00.573791: step  1120, loss = (G: 2.62579894, D: 0.43305364) (0.229 sec/batch)\n",
      "2017-04-05 20:38:05.040772: step  1140, loss = (G: 12.19442940, D: 0.04120671) (0.225 sec/batch)\n",
      "2017-04-05 20:38:09.515738: step  1160, loss = (G: 5.67365837, D: 0.08279908) (0.223 sec/batch)\n",
      "2017-04-05 20:38:13.989400: step  1180, loss = (G: 2.23725581, D: 0.70870042) (0.223 sec/batch)\n",
      "2017-04-05 20:38:18.461806: step  1200, loss = (G: 15.72850323, D: 0.00299487) (0.223 sec/batch)\n",
      "2017-04-05 20:38:23.087241: step  1220, loss = (G: 5.89042425, D: 0.12526178) (0.225 sec/batch)\n",
      "2017-04-05 20:38:27.565370: step  1240, loss = (G: 15.54051781, D: 0.02449652) (0.224 sec/batch)\n",
      "2017-04-05 20:38:32.042047: step  1260, loss = (G: 13.42638493, D: 0.02854146) (0.224 sec/batch)\n",
      "2017-04-05 20:38:36.511265: step  1280, loss = (G: 19.05678749, D: 0.04684478) (0.226 sec/batch)\n",
      "2017-04-05 20:38:40.998672: step  1300, loss = (G: 17.07129288, D: 0.00060436) (0.224 sec/batch)\n",
      "2017-04-05 20:38:45.617633: step  1320, loss = (G: 13.26434326, D: 0.00378957) (0.224 sec/batch)\n",
      "2017-04-05 20:38:50.092905: step  1340, loss = (G: 11.38621330, D: 0.00515066) (0.223 sec/batch)\n",
      "2017-04-05 20:38:54.562156: step  1360, loss = (G: 14.22302628, D: 0.00261781) (0.223 sec/batch)\n",
      "2017-04-05 20:38:59.040780: step  1380, loss = (G: 15.02474976, D: 0.00620232) (0.225 sec/batch)\n",
      "2017-04-05 20:39:03.526035: step  1400, loss = (G: 25.01679993, D: 0.00165306) (0.224 sec/batch)\n",
      "2017-04-05 20:39:08.156752: step  1420, loss = (G: 13.78763962, D: 0.00385776) (0.224 sec/batch)\n",
      "2017-04-05 20:39:12.636405: step  1440, loss = (G: 3.27298164, D: 0.60147440) (0.223 sec/batch)\n",
      "2017-04-05 20:39:17.124497: step  1460, loss = (G: 2.19334674, D: 0.83745658) (0.224 sec/batch)\n",
      "2017-04-05 20:39:21.612745: step  1480, loss = (G: 2.73886228, D: 0.43525881) (0.227 sec/batch)\n",
      "2017-04-05 20:39:26.098517: step  1500, loss = (G: 7.93435526, D: 0.10225714) (0.225 sec/batch)\n",
      "2017-04-05 20:39:30.727053: step  1520, loss = (G: 1.61315393, D: 1.46585846) (0.223 sec/batch)\n",
      "2017-04-05 20:39:35.204369: step  1540, loss = (G: 4.86267900, D: 0.08538428) (0.224 sec/batch)\n",
      "2017-04-05 20:39:39.679229: step  1560, loss = (G: 2.28356099, D: 1.17263663) (0.223 sec/batch)\n",
      "2017-04-05 20:39:44.163172: step  1580, loss = (G: 10.75169468, D: 0.02513420) (0.225 sec/batch)\n",
      "2017-04-05 20:39:48.652114: step  1600, loss = (G: 11.06174278, D: 0.01176699) (0.224 sec/batch)\n",
      "2017-04-05 20:39:53.294231: step  1620, loss = (G: 6.70064163, D: 0.02808452) (0.225 sec/batch)\n",
      "2017-04-05 20:39:57.789577: step  1640, loss = (G: 13.19424248, D: 0.05717229) (0.225 sec/batch)\n",
      "2017-04-05 20:40:02.271297: step  1660, loss = (G: 14.42967987, D: 0.09836229) (0.224 sec/batch)\n",
      "2017-04-05 20:40:06.764501: step  1680, loss = (G: 7.46257305, D: 2.52536082) (0.224 sec/batch)\n",
      "2017-04-05 20:40:11.238066: step  1700, loss = (G: 6.69321108, D: 0.75220788) (0.224 sec/batch)\n",
      "2017-04-05 20:40:15.876332: step  1720, loss = (G: 1.67480826, D: 0.77587366) (0.224 sec/batch)\n",
      "2017-04-05 20:40:20.366475: step  1740, loss = (G: 6.83626699, D: 0.17240003) (0.224 sec/batch)\n",
      "2017-04-05 20:40:24.861682: step  1760, loss = (G: 12.25576210, D: 0.02316075) (0.224 sec/batch)\n",
      "2017-04-05 20:40:29.337769: step  1780, loss = (G: 10.12260056, D: 0.00995306) (0.224 sec/batch)\n",
      "2017-04-05 20:40:33.832347: step  1800, loss = (G: 10.64776421, D: 0.00725687) (0.224 sec/batch)\n",
      "2017-04-05 20:40:38.464874: step  1820, loss = (G: 4.94525909, D: 0.08034785) (0.230 sec/batch)\n",
      "2017-04-05 20:40:42.967319: step  1840, loss = (G: 16.00596619, D: 0.00405678) (0.227 sec/batch)\n",
      "2017-04-05 20:40:47.459150: step  1860, loss = (G: 12.72083282, D: 0.01036570) (0.223 sec/batch)\n",
      "2017-04-05 20:40:51.936918: step  1880, loss = (G: 14.67059326, D: 0.00055118) (0.224 sec/batch)\n",
      "2017-04-05 20:40:56.425393: step  1900, loss = (G: 12.44431686, D: 0.00185549) (0.224 sec/batch)\n",
      "2017-04-05 20:41:01.051358: step  1920, loss = (G: 19.82171249, D: 0.03390501) (0.225 sec/batch)\n",
      "2017-04-05 20:41:05.550869: step  1940, loss = (G: 16.97052574, D: 0.00298136) (0.226 sec/batch)\n",
      "2017-04-05 20:41:10.034329: step  1960, loss = (G: 11.17679024, D: 0.00257657) (0.225 sec/batch)\n",
      "2017-04-05 20:41:14.542235: step  1980, loss = (G: 4.72395658, D: 0.16193005) (0.225 sec/batch)\n",
      "2017-04-05 20:41:19.024123: step  2000, loss = (G: 9.24238586, D: 0.01212970) (0.224 sec/batch)\n",
      "2017-04-05 20:41:23.645602: step  2020, loss = (G: 18.22238922, D: 0.00113587) (0.223 sec/batch)\n",
      "2017-04-05 20:41:28.144289: step  2040, loss = (G: 20.11673164, D: 0.00094777) (0.223 sec/batch)\n",
      "2017-04-05 20:41:32.644954: step  2060, loss = (G: 16.74749184, D: 0.00103627) (0.228 sec/batch)\n",
      "2017-04-05 20:41:37.148572: step  2080, loss = (G: 14.47561455, D: 0.00107801) (0.225 sec/batch)\n",
      "2017-04-05 20:41:41.632548: step  2100, loss = (G: 9.65161133, D: 0.00963180) (0.225 sec/batch)\n",
      "2017-04-05 20:41:46.272456: step  2120, loss = (G: 27.27763557, D: 0.00037566) (0.229 sec/batch)\n",
      "2017-04-05 20:41:50.772221: step  2140, loss = (G: 4.86904573, D: 0.23958564) (0.225 sec/batch)\n",
      "2017-04-05 20:41:55.264544: step  2160, loss = (G: 3.59457254, D: 0.46425119) (0.226 sec/batch)\n",
      "2017-04-05 20:41:59.763337: step  2180, loss = (G: 6.97429466, D: 0.09076239) (0.226 sec/batch)\n",
      "2017-04-05 20:42:04.260142: step  2200, loss = (G: 7.72838306, D: 0.03787765) (0.225 sec/batch)\n",
      "2017-04-05 20:42:08.904866: step  2220, loss = (G: 5.86038780, D: 0.45616761) (0.225 sec/batch)\n",
      "2017-04-05 20:42:13.383378: step  2240, loss = (G: 4.45230055, D: 0.15924147) (0.225 sec/batch)\n",
      "2017-04-05 20:42:17.872476: step  2260, loss = (G: 6.70956230, D: 0.03036278) (0.225 sec/batch)\n",
      "2017-04-05 20:42:22.515426: step  2280, loss = (G: 13.86611462, D: 0.02763322) (0.225 sec/batch)\n",
      "2017-04-05 20:42:27.019206: step  2300, loss = (G: 12.20869923, D: 0.02225307) (0.224 sec/batch)\n",
      "2017-04-05 20:42:31.663013: step  2320, loss = (G: 15.11469078, D: 0.01146838) (0.224 sec/batch)\n",
      "2017-04-05 20:42:36.146464: step  2340, loss = (G: 15.67956448, D: 0.00060519) (0.224 sec/batch)\n",
      "2017-04-05 20:42:40.639796: step  2360, loss = (G: 9.04816055, D: 0.33844584) (0.224 sec/batch)\n",
      "2017-04-05 20:42:45.156904: step  2380, loss = (G: 3.17747068, D: 0.29445750) (0.226 sec/batch)\n",
      "2017-04-05 20:42:49.668203: step  2400, loss = (G: 2.17117548, D: 0.50546366) (0.224 sec/batch)\n",
      "2017-04-05 20:42:54.315382: step  2420, loss = (G: 5.66261625, D: 0.10376401) (0.224 sec/batch)\n",
      "2017-04-05 20:42:58.804287: step  2440, loss = (G: 9.05060959, D: 0.00868043) (0.224 sec/batch)\n",
      "2017-04-05 20:43:03.313355: step  2460, loss = (G: 3.70933676, D: 0.17920688) (0.224 sec/batch)\n",
      "2017-04-05 20:43:07.795796: step  2480, loss = (G: 7.31944942, D: 0.29342309) (0.223 sec/batch)\n",
      "2017-04-05 20:43:12.283280: step  2500, loss = (G: 2.56670403, D: 1.33158422) (0.225 sec/batch)\n",
      "2017-04-05 20:43:16.914062: step  2520, loss = (G: 4.98918486, D: 0.14237159) (0.225 sec/batch)\n",
      "2017-04-05 20:43:21.456423: step  2540, loss = (G: 9.52872562, D: 0.01609796) (0.224 sec/batch)\n",
      "2017-04-05 20:43:25.956782: step  2560, loss = (G: 5.54977417, D: 1.13538849) (0.225 sec/batch)\n",
      "2017-04-05 20:43:30.449130: step  2580, loss = (G: 13.54616833, D: 0.27612835) (0.230 sec/batch)\n",
      "2017-04-05 20:43:34.965126: step  2600, loss = (G: 8.98282909, D: 0.17738213) (0.225 sec/batch)\n",
      "2017-04-05 20:43:39.612710: step  2620, loss = (G: 12.99530411, D: 0.01072561) (0.225 sec/batch)\n",
      "2017-04-05 20:43:44.101568: step  2640, loss = (G: 9.77198601, D: 0.00720744) (0.224 sec/batch)\n",
      "2017-04-05 20:43:48.598172: step  2660, loss = (G: 12.69163322, D: 0.00887633) (0.223 sec/batch)\n",
      "2017-04-05 20:43:53.091122: step  2680, loss = (G: 11.41353035, D: 0.00562575) (0.225 sec/batch)\n",
      "2017-04-05 20:43:57.595668: step  2700, loss = (G: 14.67101097, D: 0.00177239) (0.224 sec/batch)\n",
      "2017-04-05 20:44:02.269336: step  2720, loss = (G: 7.65537500, D: 0.01498923) (0.224 sec/batch)\n",
      "2017-04-05 20:44:06.751076: step  2740, loss = (G: 18.93230820, D: 0.01010013) (0.225 sec/batch)\n",
      "2017-04-05 20:44:11.245771: step  2760, loss = (G: 9.75591946, D: 0.00754719) (0.224 sec/batch)\n",
      "2017-04-05 20:44:15.736323: step  2780, loss = (G: 13.42210197, D: 0.00172583) (0.224 sec/batch)\n",
      "2017-04-05 20:44:20.240882: step  2800, loss = (G: 14.17799950, D: 0.00191576) (0.230 sec/batch)\n",
      "2017-04-05 20:44:24.888831: step  2820, loss = (G: 13.61688614, D: 0.00791584) (0.224 sec/batch)\n",
      "2017-04-05 20:44:29.386730: step  2840, loss = (G: 8.35847092, D: 0.01745870) (0.225 sec/batch)\n",
      "2017-04-05 20:44:33.874964: step  2860, loss = (G: 5.48743439, D: 0.12337294) (0.223 sec/batch)\n",
      "2017-04-05 20:44:38.373458: step  2880, loss = (G: 9.32752705, D: 0.08919397) (0.224 sec/batch)\n",
      "2017-04-05 20:44:42.870599: step  2900, loss = (G: 5.30302048, D: 0.11893804) (0.224 sec/batch)\n",
      "2017-04-05 20:44:47.509961: step  2920, loss = (G: 5.25983906, D: 0.05448656) (0.229 sec/batch)\n",
      "2017-04-05 20:44:52.006140: step  2940, loss = (G: 8.27386951, D: 0.01342032) (0.224 sec/batch)\n",
      "2017-04-05 20:44:56.495034: step  2960, loss = (G: 13.10228539, D: 0.03446941) (0.225 sec/batch)\n",
      "2017-04-05 20:45:00.982704: step  2980, loss = (G: 13.90393925, D: 0.00272766) (0.224 sec/batch)\n",
      "2017-04-05 20:45:05.482301: step  3000, loss = (G: 12.04638290, D: 0.13055049) (0.228 sec/batch)\n",
      "2017-04-05 20:45:10.126826: step  3020, loss = (G: 10.69363785, D: 0.01046686) (0.225 sec/batch)\n",
      "2017-04-05 20:45:14.621143: step  3040, loss = (G: 10.19154835, D: 0.10384527) (0.225 sec/batch)\n",
      "2017-04-05 20:45:19.125230: step  3060, loss = (G: 2.83172321, D: 0.28196234) (0.225 sec/batch)\n",
      "2017-04-05 20:45:23.620194: step  3080, loss = (G: 1.74846041, D: 2.46620846) (0.225 sec/batch)\n",
      "2017-04-05 20:45:28.104894: step  3100, loss = (G: 2.24798894, D: 0.57510835) (0.224 sec/batch)\n",
      "2017-04-05 20:45:32.756724: step  3120, loss = (G: 9.94415569, D: 0.01069013) (0.224 sec/batch)\n",
      "2017-04-05 20:45:37.260795: step  3140, loss = (G: 3.12004566, D: 0.49093956) (0.225 sec/batch)\n",
      "2017-04-05 20:45:41.760982: step  3160, loss = (G: 7.30608177, D: 0.02190012) (0.224 sec/batch)\n",
      "2017-04-05 20:45:46.279016: step  3180, loss = (G: 7.00235796, D: 0.02928905) (0.225 sec/batch)\n",
      "2017-04-05 20:45:50.801473: step  3200, loss = (G: 5.17982483, D: 0.06381332) (0.224 sec/batch)\n",
      "2017-04-05 20:45:55.443612: step  3220, loss = (G: 5.63977766, D: 0.10053683) (0.227 sec/batch)\n",
      "2017-04-05 20:46:00.004095: step  3240, loss = (G: 5.10063076, D: 0.08546266) (0.224 sec/batch)\n",
      "2017-04-05 20:46:04.498331: step  3260, loss = (G: 4.92249298, D: 0.16966495) (0.230 sec/batch)\n",
      "2017-04-05 20:46:09.003608: step  3280, loss = (G: 4.80910397, D: 0.22868082) (0.228 sec/batch)\n",
      "2017-04-05 20:46:13.511331: step  3300, loss = (G: 3.36020184, D: 0.23641872) (0.225 sec/batch)\n",
      "2017-04-05 20:46:18.189137: step  3320, loss = (G: 11.97362041, D: 0.00784176) (0.224 sec/batch)\n",
      "2017-04-05 20:46:22.669000: step  3340, loss = (G: 11.82893372, D: 0.19983310) (0.223 sec/batch)\n",
      "2017-04-05 20:46:27.172283: step  3360, loss = (G: 5.04478836, D: 0.08925205) (0.224 sec/batch)\n",
      "2017-04-05 20:46:31.672024: step  3380, loss = (G: 9.04128456, D: 0.04062853) (0.225 sec/batch)\n",
      "2017-04-05 20:46:36.167893: step  3400, loss = (G: 2.92400455, D: 0.31216735) (0.224 sec/batch)\n",
      "2017-04-05 20:46:40.797963: step  3420, loss = (G: 8.33865547, D: 0.08233913) (0.225 sec/batch)\n",
      "2017-04-05 20:46:45.444735: step  3440, loss = (G: 8.54310513, D: 0.08837757) (0.223 sec/batch)\n",
      "2017-04-05 20:46:49.970008: step  3460, loss = (G: 10.26129150, D: 0.05444614) (0.225 sec/batch)\n",
      "2017-04-05 20:46:54.476953: step  3480, loss = (G: 9.21278572, D: 0.06609450) (0.230 sec/batch)\n",
      "2017-04-05 20:46:58.998329: step  3500, loss = (G: 5.80405235, D: 0.03697634) (0.227 sec/batch)\n",
      "2017-04-05 20:47:03.636175: step  3520, loss = (G: 15.15927505, D: 0.00283374) (0.225 sec/batch)\n",
      "2017-04-05 20:47:08.135229: step  3540, loss = (G: 9.24313641, D: 0.00790478) (0.223 sec/batch)\n",
      "2017-04-05 20:47:12.635040: step  3560, loss = (G: 3.65391707, D: 1.14532077) (0.225 sec/batch)\n",
      "2017-04-05 20:47:17.117760: step  3580, loss = (G: 12.44832230, D: 0.08390635) (0.223 sec/batch)\n",
      "2017-04-05 20:47:21.612489: step  3600, loss = (G: 2.91620779, D: 0.24462192) (0.225 sec/batch)\n",
      "2017-04-05 20:47:26.269383: step  3620, loss = (G: 10.68243980, D: 0.19070962) (0.224 sec/batch)\n",
      "2017-04-05 20:47:30.752433: step  3640, loss = (G: 8.85374451, D: 0.01165288) (0.223 sec/batch)\n",
      "2017-04-05 20:47:35.270417: step  3660, loss = (G: 5.98669815, D: 0.06010922) (0.226 sec/batch)\n",
      "2017-04-05 20:47:39.751694: step  3680, loss = (G: 1.37992370, D: 2.64923000) (0.224 sec/batch)\n",
      "2017-04-05 20:47:44.296500: step  3700, loss = (G: 8.30464935, D: 0.08385123) (0.226 sec/batch)\n",
      "2017-04-05 20:47:48.944120: step  3720, loss = (G: 12.30564785, D: 0.02431061) (0.225 sec/batch)\n",
      "2017-04-05 20:47:53.435033: step  3740, loss = (G: 7.95102930, D: 0.02995053) (0.224 sec/batch)\n",
      "2017-04-05 20:47:57.930943: step  3760, loss = (G: 14.66125679, D: 0.01372907) (0.223 sec/batch)\n",
      "2017-04-05 20:48:02.423450: step  3780, loss = (G: 10.44572449, D: 0.01175800) (0.225 sec/batch)\n",
      "2017-04-05 20:48:06.913671: step  3800, loss = (G: 5.57673502, D: 0.16727045) (0.224 sec/batch)\n",
      "2017-04-05 20:48:11.571651: step  3820, loss = (G: 17.60990524, D: 0.00617211) (0.227 sec/batch)\n",
      "2017-04-05 20:48:16.067608: step  3840, loss = (G: 9.31492996, D: 0.00616335) (0.226 sec/batch)\n",
      "2017-04-05 20:48:20.572193: step  3860, loss = (G: 8.53356075, D: 0.71345407) (0.225 sec/batch)\n",
      "2017-04-05 20:48:25.086742: step  3880, loss = (G: 13.73925400, D: 0.00595201) (0.227 sec/batch)\n",
      "2017-04-05 20:48:29.580710: step  3900, loss = (G: 4.66525698, D: 0.09482014) (0.224 sec/batch)\n",
      "2017-04-05 20:48:34.249139: step  3920, loss = (G: 6.99665499, D: 0.07965855) (0.224 sec/batch)\n",
      "2017-04-05 20:48:38.743403: step  3940, loss = (G: 10.42551899, D: 0.35100755) (0.224 sec/batch)\n",
      "2017-04-05 20:48:43.246010: step  3960, loss = (G: 13.32673264, D: 0.01629956) (0.225 sec/batch)\n",
      "2017-04-05 20:48:47.773657: step  3980, loss = (G: 4.88928413, D: 0.87316608) (0.227 sec/batch)\n",
      "2017-04-05 20:48:52.268696: step  4000, loss = (G: 7.76171780, D: 0.02417357) (0.224 sec/batch)\n",
      "2017-04-05 20:48:56.951152: step  4020, loss = (G: 11.31854820, D: 0.31293774) (0.226 sec/batch)\n",
      "2017-04-05 20:49:01.446221: step  4040, loss = (G: 6.54790640, D: 0.41205031) (0.228 sec/batch)\n",
      "2017-04-05 20:49:05.945362: step  4060, loss = (G: 6.56404543, D: 0.04106065) (0.223 sec/batch)\n",
      "2017-04-05 20:49:10.456134: step  4080, loss = (G: 2.51000357, D: 0.36497012) (0.223 sec/batch)\n",
      "2017-04-05 20:49:14.951936: step  4100, loss = (G: 3.85053158, D: 0.15756395) (0.225 sec/batch)\n",
      "2017-04-05 20:49:19.592046: step  4120, loss = (G: 14.02938175, D: 0.21112481) (0.225 sec/batch)\n",
      "2017-04-05 20:49:24.085883: step  4140, loss = (G: 11.37528515, D: 0.00686030) (0.224 sec/batch)\n",
      "2017-04-05 20:49:28.597981: step  4160, loss = (G: 9.11978149, D: 0.40249857) (0.228 sec/batch)\n",
      "2017-04-05 20:49:33.097584: step  4180, loss = (G: 5.00261831, D: 0.06429268) (0.225 sec/batch)\n",
      "2017-04-05 20:49:37.601536: step  4200, loss = (G: 9.57158947, D: 0.07608903) (0.224 sec/batch)\n",
      "2017-04-05 20:49:42.254882: step  4220, loss = (G: 7.70998669, D: 0.02696057) (0.225 sec/batch)\n",
      "2017-04-05 20:49:46.757226: step  4240, loss = (G: 9.63521004, D: 0.02655378) (0.224 sec/batch)\n",
      "2017-04-05 20:49:51.269888: step  4260, loss = (G: 4.87607336, D: 0.06486072) (0.227 sec/batch)\n",
      "2017-04-05 20:49:55.789203: step  4280, loss = (G: 13.04021740, D: 0.00690213) (0.227 sec/batch)\n",
      "2017-04-05 20:50:00.309714: step  4300, loss = (G: 3.21158171, D: 0.22971784) (0.225 sec/batch)\n",
      "2017-04-05 20:50:04.970563: step  4320, loss = (G: 6.34573841, D: 0.02706696) (0.225 sec/batch)\n",
      "2017-04-05 20:50:09.487930: step  4340, loss = (G: 17.28355026, D: 0.00340282) (0.227 sec/batch)\n",
      "2017-04-05 20:50:13.996036: step  4360, loss = (G: 14.30249500, D: 0.00086279) (0.224 sec/batch)\n",
      "2017-04-05 20:50:18.496292: step  4380, loss = (G: 6.90666580, D: 0.02709560) (0.224 sec/batch)\n",
      "2017-04-05 20:50:23.005108: step  4400, loss = (G: 3.98470831, D: 0.22403929) (0.225 sec/batch)\n",
      "2017-04-05 20:50:27.640730: step  4420, loss = (G: 7.76224136, D: 0.03330433) (0.224 sec/batch)\n",
      "2017-04-05 20:50:32.149560: step  4440, loss = (G: 15.64482117, D: 0.00383874) (0.224 sec/batch)\n",
      "2017-04-05 20:50:36.662535: step  4460, loss = (G: 8.98591232, D: 0.01070996) (0.224 sec/batch)\n",
      "2017-04-05 20:50:41.167122: step  4480, loss = (G: 2.45617771, D: 0.50253135) (0.226 sec/batch)\n",
      "2017-04-05 20:50:45.673806: step  4500, loss = (G: 14.29039192, D: 0.02205811) (0.226 sec/batch)\n",
      "2017-04-05 20:50:50.327774: step  4520, loss = (G: 9.80009270, D: 0.00548915) (0.225 sec/batch)\n",
      "2017-04-05 20:50:54.878800: step  4540, loss = (G: 6.09279060, D: 0.09328724) (0.230 sec/batch)\n",
      "2017-04-05 20:50:59.387305: step  4560, loss = (G: 5.29841757, D: 1.09872162) (0.227 sec/batch)\n",
      "2017-04-05 20:51:03.896290: step  4580, loss = (G: 12.08932114, D: 1.24660420) (0.225 sec/batch)\n",
      "2017-04-05 20:51:08.554583: step  4600, loss = (G: 3.54483485, D: 0.27198407) (0.224 sec/batch)\n",
      "2017-04-05 20:51:13.196031: step  4620, loss = (G: 12.88827705, D: 0.19595531) (0.224 sec/batch)\n",
      "2017-04-05 20:51:17.692358: step  4640, loss = (G: 4.33765221, D: 0.11496975) (0.224 sec/batch)\n",
      "2017-04-05 20:51:22.192066: step  4660, loss = (G: 3.91509485, D: 0.16498297) (0.225 sec/batch)\n",
      "2017-04-05 20:51:26.696692: step  4680, loss = (G: 9.56685352, D: 0.08330739) (0.224 sec/batch)\n",
      "2017-04-05 20:51:31.196534: step  4700, loss = (G: 12.58906555, D: 0.08473343) (0.226 sec/batch)\n",
      "2017-04-05 20:51:35.835180: step  4720, loss = (G: 3.40365124, D: 0.21790975) (0.225 sec/batch)\n",
      "2017-04-05 20:51:40.331162: step  4740, loss = (G: 13.04730892, D: 0.01135179) (0.224 sec/batch)\n",
      "2017-04-05 20:51:44.831026: step  4760, loss = (G: 8.34010315, D: 0.01619605) (0.227 sec/batch)\n",
      "2017-04-05 20:51:49.327531: step  4780, loss = (G: 5.49993229, D: 0.08630516) (0.225 sec/batch)\n",
      "2017-04-05 20:51:53.829214: step  4800, loss = (G: 9.14823532, D: 0.00897138) (0.226 sec/batch)\n",
      "2017-04-05 20:51:58.473990: step  4820, loss = (G: 9.91700554, D: 0.00871345) (0.224 sec/batch)\n",
      "2017-04-05 20:52:03.006888: step  4840, loss = (G: 3.12852383, D: 0.23839192) (0.225 sec/batch)\n",
      "2017-04-05 20:52:07.512994: step  4860, loss = (G: 2.92677307, D: 0.30000490) (0.224 sec/batch)\n",
      "2017-04-05 20:52:12.023550: step  4880, loss = (G: 6.74421406, D: 0.06173289) (0.225 sec/batch)\n",
      "2017-04-05 20:52:16.527245: step  4900, loss = (G: 6.63086319, D: 0.04053400) (0.224 sec/batch)\n",
      "2017-04-05 20:52:21.169812: step  4920, loss = (G: 9.15029907, D: 0.01513680) (0.224 sec/batch)\n",
      "2017-04-05 20:52:25.682753: step  4940, loss = (G: 4.84245872, D: 0.08437924) (0.223 sec/batch)\n",
      "2017-04-05 20:52:30.201627: step  4960, loss = (G: 12.27232456, D: 0.00781248) (0.226 sec/batch)\n",
      "2017-04-05 20:52:34.700142: step  4980, loss = (G: 11.32881165, D: 0.04252294) (0.225 sec/batch)\n",
      "2017-04-05 20:52:39.215348: step  5000, loss = (G: 17.48988724, D: 0.00468040) (0.226 sec/batch)\n",
      "2017-04-05 20:52:43.860149: step  5020, loss = (G: 2.07909656, D: 0.96738774) (0.226 sec/batch)\n",
      "2017-04-05 20:52:48.363268: step  5040, loss = (G: 6.97913170, D: 0.25760788) (0.225 sec/batch)\n",
      "2017-04-05 20:52:52.865285: step  5060, loss = (G: 5.64066219, D: 0.07584109) (0.226 sec/batch)\n",
      "2017-04-05 20:52:57.355409: step  5080, loss = (G: 6.04870844, D: 0.03632904) (0.226 sec/batch)\n",
      "2017-04-05 20:53:01.863292: step  5100, loss = (G: 5.66837311, D: 0.05984691) (0.231 sec/batch)\n",
      "2017-04-05 20:53:06.521299: step  5120, loss = (G: 5.48804998, D: 0.06105179) (0.223 sec/batch)\n",
      "2017-04-05 20:53:11.054863: step  5140, loss = (G: 2.33980727, D: 0.47827971) (0.227 sec/batch)\n",
      "2017-04-05 20:53:15.555027: step  5160, loss = (G: 7.36150074, D: 0.02196757) (0.225 sec/batch)\n",
      "2017-04-05 20:53:20.057969: step  5180, loss = (G: 3.26670551, D: 0.20844012) (0.224 sec/batch)\n",
      "2017-04-05 20:53:24.561532: step  5200, loss = (G: 8.46478844, D: 0.03051643) (0.224 sec/batch)\n",
      "2017-04-05 20:53:29.201170: step  5220, loss = (G: 12.29476070, D: 0.06295107) (0.225 sec/batch)\n",
      "2017-04-05 20:53:33.700237: step  5240, loss = (G: 2.30294561, D: 0.60127527) (0.225 sec/batch)\n",
      "2017-04-05 20:53:38.213019: step  5260, loss = (G: 15.39154339, D: 0.25280678) (0.225 sec/batch)\n",
      "2017-04-05 20:53:42.716204: step  5280, loss = (G: 12.83052349, D: 0.14003380) (0.225 sec/batch)\n",
      "2017-04-05 20:53:47.229087: step  5300, loss = (G: 4.61671782, D: 0.13796592) (0.224 sec/batch)\n",
      "2017-04-05 20:53:51.875336: step  5320, loss = (G: 7.34236431, D: 0.02031976) (0.224 sec/batch)\n",
      "2017-04-05 20:53:56.371764: step  5340, loss = (G: 11.82305336, D: 0.00393106) (0.225 sec/batch)\n",
      "2017-04-05 20:54:00.883259: step  5360, loss = (G: 9.57261467, D: 0.00630918) (0.224 sec/batch)\n",
      "2017-04-05 20:54:05.395167: step  5380, loss = (G: 11.87950993, D: 1.02043009) (0.226 sec/batch)\n",
      "2017-04-05 20:54:09.906468: step  5400, loss = (G: 2.40996909, D: 0.56399930) (0.228 sec/batch)\n",
      "2017-04-05 20:54:14.603020: step  5420, loss = (G: 11.29032898, D: 0.04828592) (0.224 sec/batch)\n",
      "2017-04-05 20:54:19.102499: step  5440, loss = (G: 10.83401966, D: 0.00582240) (0.225 sec/batch)\n",
      "2017-04-05 20:54:23.616260: step  5460, loss = (G: 4.11374903, D: 0.18668298) (0.225 sec/batch)\n",
      "2017-04-05 20:54:28.116214: step  5480, loss = (G: 1.34874344, D: 1.82915759) (0.227 sec/batch)\n",
      "2017-04-05 20:54:32.627200: step  5500, loss = (G: 7.65628529, D: 0.02312970) (0.224 sec/batch)\n",
      "2017-04-05 20:54:37.281791: step  5520, loss = (G: 7.05144882, D: 0.13975115) (0.224 sec/batch)\n",
      "2017-04-05 20:54:41.783094: step  5540, loss = (G: 6.72289848, D: 0.05112342) (0.226 sec/batch)\n",
      "2017-04-05 20:54:46.302950: step  5560, loss = (G: 9.52173901, D: 0.12913476) (0.231 sec/batch)\n",
      "2017-04-05 20:54:50.843414: step  5580, loss = (G: 14.12225342, D: 0.00827845) (0.231 sec/batch)\n",
      "2017-04-05 20:54:55.352576: step  5600, loss = (G: 3.30205917, D: 0.38686243) (0.225 sec/batch)\n",
      "2017-04-05 20:55:00.005173: step  5620, loss = (G: 3.72955227, D: 0.26775515) (0.231 sec/batch)\n",
      "2017-04-05 20:55:04.518216: step  5640, loss = (G: 17.53932190, D: 0.00768323) (0.226 sec/batch)\n",
      "2017-04-05 20:55:09.023586: step  5660, loss = (G: 13.65226746, D: 0.01029061) (0.225 sec/batch)\n",
      "2017-04-05 20:55:13.534383: step  5680, loss = (G: 4.40054941, D: 0.10059156) (0.225 sec/batch)\n",
      "2017-04-05 20:55:18.034895: step  5700, loss = (G: 9.91439247, D: 0.03097737) (0.225 sec/batch)\n",
      "2017-04-05 20:55:22.679239: step  5720, loss = (G: 4.01412582, D: 0.19263966) (0.224 sec/batch)\n",
      "2017-04-05 20:55:27.187473: step  5740, loss = (G: 13.06138611, D: 3.37217641) (0.225 sec/batch)\n",
      "2017-04-05 20:55:31.710474: step  5760, loss = (G: 6.20733452, D: 0.54366469) (0.224 sec/batch)\n",
      "2017-04-05 20:55:36.212994: step  5780, loss = (G: 3.12856126, D: 0.61065751) (0.224 sec/batch)\n",
      "2017-04-05 20:55:40.723901: step  5800, loss = (G: 15.91101646, D: 0.00329486) (0.224 sec/batch)\n",
      "2017-04-05 20:55:45.387372: step  5820, loss = (G: 6.18474960, D: 0.16510099) (0.224 sec/batch)\n",
      "2017-04-05 20:55:49.929682: step  5840, loss = (G: 8.82278633, D: 0.21398465) (0.229 sec/batch)\n",
      "2017-04-05 20:55:54.432875: step  5860, loss = (G: 15.44820213, D: 0.13479644) (0.231 sec/batch)\n",
      "2017-04-05 20:55:58.972187: step  5880, loss = (G: 17.11539841, D: 0.00529633) (0.225 sec/batch)\n",
      "2017-04-05 20:56:03.464879: step  5900, loss = (G: 5.78808546, D: 0.07162631) (0.224 sec/batch)\n",
      "2017-04-05 20:56:08.117367: step  5920, loss = (G: 6.66716862, D: 0.05084139) (0.225 sec/batch)\n",
      "2017-04-05 20:56:12.610296: step  5940, loss = (G: 8.29836464, D: 0.00936152) (0.223 sec/batch)\n",
      "2017-04-05 20:56:17.106214: step  5960, loss = (G: 10.78819370, D: 1.54519486) (0.224 sec/batch)\n",
      "2017-04-05 20:56:21.648761: step  5980, loss = (G: 8.02977371, D: 0.04178657) (0.224 sec/batch)\n",
      "2017-04-05 20:56:26.163019: step  6000, loss = (G: 12.24796104, D: 0.18936320) (0.224 sec/batch)\n",
      "2017-04-05 20:56:30.805118: step  6020, loss = (G: 5.53987217, D: 0.59741527) (0.225 sec/batch)\n",
      "2017-04-05 20:56:35.310006: step  6040, loss = (G: 15.80294895, D: 1.14041460) (0.226 sec/batch)\n",
      "2017-04-05 20:56:39.819423: step  6060, loss = (G: 12.88926792, D: 0.04612559) (0.224 sec/batch)\n",
      "2017-04-05 20:56:44.328537: step  6080, loss = (G: 9.42805290, D: 0.01118739) (0.224 sec/batch)\n",
      "2017-04-05 20:56:48.845431: step  6100, loss = (G: 8.10837555, D: 0.01451958) (0.228 sec/batch)\n",
      "2017-04-05 20:56:53.482859: step  6120, loss = (G: 16.28569603, D: 0.00365971) (0.226 sec/batch)\n",
      "2017-04-05 20:56:57.988708: step  6140, loss = (G: 8.59191322, D: 0.02687432) (0.225 sec/batch)\n",
      "2017-04-05 20:57:02.488082: step  6160, loss = (G: 13.72148132, D: 0.01627166) (0.224 sec/batch)\n",
      "2017-04-05 20:57:06.999837: step  6180, loss = (G: 4.98374176, D: 0.21703857) (0.225 sec/batch)\n",
      "2017-04-05 20:57:11.528361: step  6200, loss = (G: 8.54521084, D: 0.01168911) (0.230 sec/batch)\n",
      "2017-04-05 20:57:16.204249: step  6220, loss = (G: 4.74880838, D: 0.10848185) (0.227 sec/batch)\n",
      "2017-04-05 20:57:20.695772: step  6240, loss = (G: 4.07099533, D: 0.21208414) (0.224 sec/batch)\n",
      "2017-04-05 20:57:25.203669: step  6260, loss = (G: 5.75855255, D: 0.04306767) (0.225 sec/batch)\n",
      "2017-04-05 20:57:29.725320: step  6280, loss = (G: 7.21411467, D: 0.05721766) (0.225 sec/batch)\n",
      "2017-04-05 20:57:34.227174: step  6300, loss = (G: 12.48515511, D: 0.00990640) (0.224 sec/batch)\n",
      "2017-04-05 20:57:38.864821: step  6320, loss = (G: 5.86051416, D: 0.07937603) (0.224 sec/batch)\n",
      "2017-04-05 20:57:43.362511: step  6340, loss = (G: 8.29147339, D: 0.00666855) (0.224 sec/batch)\n",
      "2017-04-05 20:57:47.877571: step  6360, loss = (G: 3.57929754, D: 0.35563064) (0.226 sec/batch)\n",
      "2017-04-05 20:57:52.383414: step  6380, loss = (G: 9.60210228, D: 0.02889438) (0.224 sec/batch)\n",
      "2017-04-05 20:57:56.891706: step  6400, loss = (G: 6.72744083, D: 0.03619917) (0.225 sec/batch)\n",
      "2017-04-05 20:58:01.532867: step  6420, loss = (G: 13.45285988, D: 0.00828681) (0.226 sec/batch)\n",
      "2017-04-05 20:58:06.034422: step  6440, loss = (G: 4.08127308, D: 0.21893646) (0.225 sec/batch)\n",
      "2017-04-05 20:58:10.532450: step  6460, loss = (G: 10.88385773, D: 0.02103714) (0.225 sec/batch)\n",
      "2017-04-05 20:58:15.032970: step  6480, loss = (G: 13.51808929, D: 0.00933524) (0.225 sec/batch)\n",
      "2017-04-05 20:58:19.560310: step  6500, loss = (G: 17.68826485, D: 0.00387637) (0.224 sec/batch)\n",
      "2017-04-05 20:58:24.187286: step  6520, loss = (G: 3.59573793, D: 0.41598290) (0.224 sec/batch)\n",
      "2017-04-05 20:58:28.695724: step  6540, loss = (G: 13.44461536, D: 0.41095412) (0.225 sec/batch)\n",
      "2017-04-05 20:58:33.215528: step  6560, loss = (G: 3.35673237, D: 0.15941319) (0.224 sec/batch)\n",
      "2017-04-05 20:58:37.712761: step  6580, loss = (G: 4.87337208, D: 0.15806733) (0.225 sec/batch)\n",
      "2017-04-05 20:58:42.219993: step  6600, loss = (G: 9.45939827, D: 0.03031108) (0.226 sec/batch)\n",
      "2017-04-05 20:58:46.872631: step  6620, loss = (G: 4.62094927, D: 0.21073395) (0.225 sec/batch)\n",
      "2017-04-05 20:58:51.365110: step  6640, loss = (G: 12.32108307, D: 0.00368441) (0.225 sec/batch)\n",
      "2017-04-05 20:58:55.917044: step  6660, loss = (G: 5.95610666, D: 0.03529172) (0.225 sec/batch)\n",
      "2017-04-05 20:59:00.409773: step  6680, loss = (G: 6.09881449, D: 0.99668723) (0.225 sec/batch)\n",
      "2017-04-05 20:59:04.913723: step  6700, loss = (G: 12.11707783, D: 0.06472483) (0.224 sec/batch)\n",
      "2017-04-05 20:59:09.566650: step  6720, loss = (G: 12.85232544, D: 0.00570327) (0.226 sec/batch)\n",
      "2017-04-05 20:59:14.070138: step  6740, loss = (G: 10.40304852, D: 1.77525270) (0.226 sec/batch)\n",
      "2017-04-05 20:59:18.571343: step  6760, loss = (G: 17.62930870, D: 0.13997775) (0.224 sec/batch)\n",
      "2017-04-05 20:59:23.076690: step  6780, loss = (G: 7.51226616, D: 0.03306055) (0.225 sec/batch)\n",
      "2017-04-05 20:59:27.588821: step  6800, loss = (G: 13.22173595, D: 0.00629191) (0.226 sec/batch)\n",
      "2017-04-05 20:59:32.230475: step  6820, loss = (G: 10.86585522, D: 0.02520412) (0.225 sec/batch)\n",
      "2017-04-05 20:59:36.735084: step  6840, loss = (G: 6.57513714, D: 0.02071087) (0.224 sec/batch)\n",
      "2017-04-05 20:59:41.243055: step  6860, loss = (G: 4.40248108, D: 0.14737947) (0.225 sec/batch)\n",
      "2017-04-05 20:59:45.758146: step  6880, loss = (G: 5.26805401, D: 0.08863811) (0.226 sec/batch)\n",
      "2017-04-05 20:59:50.262623: step  6900, loss = (G: 8.55559444, D: 0.34510800) (0.226 sec/batch)\n",
      "2017-04-05 20:59:55.117272: step  6920, loss = (G: 7.57469654, D: 0.01207020) (0.224 sec/batch)\n",
      "2017-04-05 20:59:59.621007: step  6940, loss = (G: 20.12615395, D: 0.00810170) (0.225 sec/batch)\n",
      "2017-04-05 21:00:04.117010: step  6960, loss = (G: 5.10822582, D: 0.06923331) (0.225 sec/batch)\n",
      "2017-04-05 21:00:08.618919: step  6980, loss = (G: 12.32588005, D: 0.00603772) (0.225 sec/batch)\n",
      "2017-04-05 21:00:13.126631: step  7000, loss = (G: 14.79363060, D: 0.01262896) (0.226 sec/batch)\n",
      "2017-04-05 21:00:17.764424: step  7020, loss = (G: 15.70559883, D: 0.00396966) (0.223 sec/batch)\n",
      "2017-04-05 21:00:22.279022: step  7040, loss = (G: 14.61903286, D: 0.00737416) (0.226 sec/batch)\n",
      "2017-04-05 21:00:26.781970: step  7060, loss = (G: 12.84947300, D: 0.04076432) (0.228 sec/batch)\n",
      "2017-04-05 21:00:31.278178: step  7080, loss = (G: 11.23845863, D: 0.00383801) (0.230 sec/batch)\n",
      "2017-04-05 21:00:35.820908: step  7100, loss = (G: 9.22960472, D: 0.04032310) (0.224 sec/batch)\n",
      "2017-04-05 21:00:40.482558: step  7120, loss = (G: 17.71708298, D: 0.08832268) (0.225 sec/batch)\n",
      "2017-04-05 21:00:45.027264: step  7140, loss = (G: 8.02966022, D: 0.01426929) (0.224 sec/batch)\n",
      "2017-04-05 21:00:49.528626: step  7160, loss = (G: 4.89748478, D: 0.17344615) (0.224 sec/batch)\n",
      "2017-04-05 21:00:54.021664: step  7180, loss = (G: 16.55972672, D: 0.06960242) (0.226 sec/batch)\n",
      "2017-04-05 21:00:58.541244: step  7200, loss = (G: 6.16636801, D: 0.31326175) (0.225 sec/batch)\n",
      "2017-04-05 21:01:03.226229: step  7220, loss = (G: 3.93578482, D: 0.24065457) (0.225 sec/batch)\n",
      "2017-04-05 21:01:07.726394: step  7240, loss = (G: 9.71798420, D: 0.98955518) (0.224 sec/batch)\n",
      "2017-04-05 21:01:12.232836: step  7260, loss = (G: 4.05158710, D: 0.35062870) (0.223 sec/batch)\n",
      "2017-04-05 21:01:16.745019: step  7280, loss = (G: 16.09524155, D: 0.00527435) (0.225 sec/batch)\n",
      "2017-04-05 21:01:21.252643: step  7300, loss = (G: 11.38599014, D: 0.04357323) (0.227 sec/batch)\n",
      "2017-04-05 21:01:25.901658: step  7320, loss = (G: 4.41375017, D: 0.08752310) (0.226 sec/batch)\n",
      "2017-04-05 21:01:30.404688: step  7340, loss = (G: 10.42266750, D: 0.01018425) (0.223 sec/batch)\n",
      "2017-04-05 21:01:34.906874: step  7360, loss = (G: 7.89282131, D: 0.08975084) (0.224 sec/batch)\n",
      "2017-04-05 21:01:39.432747: step  7380, loss = (G: 15.88064766, D: 0.00886747) (0.225 sec/batch)\n",
      "2017-04-05 21:01:43.932989: step  7400, loss = (G: 12.66202736, D: 0.00312927) (0.225 sec/batch)\n",
      "2017-04-05 21:01:48.580444: step  7420, loss = (G: 4.84249210, D: 0.08108474) (0.225 sec/batch)\n",
      "2017-04-05 21:01:53.100455: step  7440, loss = (G: 14.41459274, D: 0.00578295) (0.229 sec/batch)\n",
      "2017-04-05 21:01:57.623505: step  7460, loss = (G: 9.24830914, D: 0.00592063) (0.230 sec/batch)\n",
      "2017-04-05 21:02:02.143479: step  7480, loss = (G: 10.79202175, D: 0.01222291) (0.225 sec/batch)\n",
      "2017-04-05 21:02:06.635078: step  7500, loss = (G: 16.96142960, D: 0.03744396) (0.225 sec/batch)\n",
      "2017-04-05 21:02:11.270399: step  7520, loss = (G: 16.06145477, D: 0.00430625) (0.225 sec/batch)\n",
      "2017-04-05 21:02:15.765880: step  7540, loss = (G: 2.53597355, D: 0.43422249) (0.223 sec/batch)\n",
      "2017-04-05 21:02:20.271831: step  7560, loss = (G: 14.29310894, D: 0.00296523) (0.224 sec/batch)\n",
      "2017-04-05 21:02:24.784307: step  7580, loss = (G: 20.32701492, D: 0.65410578) (0.226 sec/batch)\n",
      "2017-04-05 21:02:29.284542: step  7600, loss = (G: 8.64469624, D: 0.45158201) (0.225 sec/batch)\n",
      "2017-04-05 21:02:33.930846: step  7620, loss = (G: 9.34403229, D: 0.01965839) (0.225 sec/batch)\n",
      "2017-04-05 21:02:38.432799: step  7640, loss = (G: 6.37118149, D: 0.07402486) (0.225 sec/batch)\n",
      "2017-04-05 21:02:42.946410: step  7660, loss = (G: 13.15985584, D: 0.98630118) (0.226 sec/batch)\n",
      "2017-04-05 21:02:47.492549: step  7680, loss = (G: 9.96827126, D: 0.11129326) (0.225 sec/batch)\n",
      "2017-04-05 21:02:51.986020: step  7700, loss = (G: 6.30957603, D: 0.05700400) (0.225 sec/batch)\n",
      "2017-04-05 21:02:56.629769: step  7720, loss = (G: 7.11856937, D: 0.02443986) (0.225 sec/batch)\n",
      "2017-04-05 21:03:01.121680: step  7740, loss = (G: 5.01462936, D: 0.06221107) (0.225 sec/batch)\n",
      "2017-04-05 21:03:05.633466: step  7760, loss = (G: 6.42471170, D: 0.09243248) (0.225 sec/batch)\n",
      "2017-04-05 21:03:10.130412: step  7780, loss = (G: 5.38049221, D: 0.30520511) (0.225 sec/batch)\n",
      "2017-04-05 21:03:14.635732: step  7800, loss = (G: 9.51581383, D: 0.00957077) (0.224 sec/batch)\n",
      "2017-04-05 21:03:19.280958: step  7820, loss = (G: 5.64211178, D: 0.05629923) (0.226 sec/batch)\n",
      "2017-04-05 21:03:23.785667: step  7840, loss = (G: 10.70006943, D: 0.00965778) (0.225 sec/batch)\n",
      "2017-04-05 21:03:28.286808: step  7860, loss = (G: 6.22318888, D: 0.04002265) (0.224 sec/batch)\n",
      "2017-04-05 21:03:32.799979: step  7880, loss = (G: 6.05282974, D: 0.08913052) (0.225 sec/batch)\n",
      "2017-04-05 21:03:37.311906: step  7900, loss = (G: 10.26201248, D: 0.00803625) (0.225 sec/batch)\n",
      "2017-04-05 21:03:41.964500: step  7920, loss = (G: 10.77534103, D: 0.00583383) (0.226 sec/batch)\n",
      "2017-04-05 21:03:46.469489: step  7940, loss = (G: 18.04161072, D: 0.10651688) (0.225 sec/batch)\n",
      "2017-04-05 21:03:50.986842: step  7960, loss = (G: 8.61907196, D: 0.00781237) (0.227 sec/batch)\n",
      "2017-04-05 21:03:55.499610: step  7980, loss = (G: 15.65526009, D: 5.14683819) (0.226 sec/batch)\n",
      "2017-04-05 21:04:00.004624: step  8000, loss = (G: 12.22905445, D: 0.15468273) (0.225 sec/batch)\n",
      "2017-04-05 21:04:04.651158: step  8020, loss = (G: 13.03994942, D: 0.00989151) (0.225 sec/batch)\n",
      "2017-04-05 21:04:09.161794: step  8040, loss = (G: 5.21578979, D: 0.10758238) (0.225 sec/batch)\n",
      "2017-04-05 21:04:13.672313: step  8060, loss = (G: 15.91904831, D: 0.00290063) (0.226 sec/batch)\n",
      "2017-04-05 21:04:18.179362: step  8080, loss = (G: 17.74438858, D: 0.01608277) (0.224 sec/batch)\n",
      "2017-04-05 21:04:22.825283: step  8100, loss = (G: 19.96391296, D: 0.00509707) (0.224 sec/batch)\n",
      "2017-04-05 21:04:27.464818: step  8120, loss = (G: 9.77922726, D: 0.00356543) (0.225 sec/batch)\n",
      "2017-04-05 21:04:31.974779: step  8140, loss = (G: 8.39014721, D: 0.08160863) (0.229 sec/batch)\n",
      "2017-04-05 21:04:36.493025: step  8160, loss = (G: 21.71135712, D: 0.00361260) (0.225 sec/batch)\n",
      "2017-04-05 21:04:40.991325: step  8180, loss = (G: 5.03651142, D: 0.12923312) (0.225 sec/batch)\n",
      "2017-04-05 21:04:45.492894: step  8200, loss = (G: 6.57082176, D: 0.01739978) (0.226 sec/batch)\n",
      "2017-04-05 21:04:50.142578: step  8220, loss = (G: 11.59083366, D: 0.00835547) (0.225 sec/batch)\n",
      "2017-04-05 21:04:54.647640: step  8240, loss = (G: 12.84901524, D: 0.08796075) (0.226 sec/batch)\n",
      "2017-04-05 21:04:59.163888: step  8260, loss = (G: 7.17315578, D: 0.09488815) (0.225 sec/batch)\n",
      "2017-04-05 21:05:03.665563: step  8280, loss = (G: 7.72550869, D: 0.02446463) (0.224 sec/batch)\n",
      "2017-04-05 21:05:08.183652: step  8300, loss = (G: 14.71011925, D: 0.00245285) (0.225 sec/batch)\n",
      "2017-04-05 21:05:12.838033: step  8320, loss = (G: 6.65745497, D: 0.04865467) (0.225 sec/batch)\n",
      "2017-04-05 21:05:17.346203: step  8340, loss = (G: 6.77243519, D: 0.02488848) (0.228 sec/batch)\n",
      "2017-04-05 21:05:21.857678: step  8360, loss = (G: 3.98368931, D: 0.27499181) (0.225 sec/batch)\n",
      "2017-04-05 21:05:26.369973: step  8380, loss = (G: 15.60649300, D: 0.44478926) (0.229 sec/batch)\n",
      "2017-04-05 21:05:30.891796: step  8400, loss = (G: 6.85338593, D: 0.01522123) (0.225 sec/batch)\n",
      "2017-04-05 21:05:35.534275: step  8420, loss = (G: 9.59927750, D: 0.09019554) (0.224 sec/batch)\n",
      "2017-04-05 21:05:40.031212: step  8440, loss = (G: 17.16109276, D: 0.11792292) (0.225 sec/batch)\n",
      "2017-04-05 21:05:44.541922: step  8460, loss = (G: 11.26377773, D: 0.09961198) (0.225 sec/batch)\n",
      "2017-04-05 21:05:49.049180: step  8480, loss = (G: 5.71470451, D: 0.03060788) (0.226 sec/batch)\n",
      "2017-04-05 21:05:53.555730: step  8500, loss = (G: 2.17231584, D: 0.50003201) (0.226 sec/batch)\n",
      "2017-04-05 21:05:58.195430: step  8520, loss = (G: 10.55235100, D: 0.01654984) (0.224 sec/batch)\n",
      "2017-04-05 21:06:02.703968: step  8540, loss = (G: 13.52815056, D: 0.11601387) (0.226 sec/batch)\n",
      "2017-04-05 21:06:07.214577: step  8560, loss = (G: 11.82504559, D: 0.00542761) (0.226 sec/batch)\n",
      "2017-04-05 21:06:11.722714: step  8580, loss = (G: 4.41796827, D: 0.13940322) (0.225 sec/batch)\n",
      "2017-04-05 21:06:16.216829: step  8600, loss = (G: 3.92995667, D: 0.18271807) (0.225 sec/batch)\n",
      "2017-04-05 21:06:20.865141: step  8620, loss = (G: 15.52396202, D: 0.06381427) (0.224 sec/batch)\n",
      "2017-04-05 21:06:25.371699: step  8640, loss = (G: 8.15005684, D: 0.02274784) (0.225 sec/batch)\n",
      "2017-04-05 21:06:29.871397: step  8660, loss = (G: 12.47378159, D: 0.04231093) (0.225 sec/batch)\n",
      "2017-04-05 21:06:34.374445: step  8680, loss = (G: 3.26418829, D: 0.45028704) (0.224 sec/batch)\n",
      "2017-04-05 21:06:38.885303: step  8700, loss = (G: 4.75697708, D: 0.15143229) (0.226 sec/batch)\n",
      "2017-04-05 21:06:43.533419: step  8720, loss = (G: 6.10029840, D: 0.03343560) (0.225 sec/batch)\n",
      "2017-04-05 21:06:48.043167: step  8740, loss = (G: 8.73797798, D: 0.01783754) (0.225 sec/batch)\n",
      "2017-04-05 21:06:52.563211: step  8760, loss = (G: 2.71706867, D: 0.49727616) (0.226 sec/batch)\n",
      "2017-04-05 21:06:57.111301: step  8780, loss = (G: 7.11084843, D: 0.05227344) (0.224 sec/batch)\n",
      "2017-04-05 21:07:01.615955: step  8800, loss = (G: 14.11606979, D: 0.00771727) (0.224 sec/batch)\n",
      "2017-04-05 21:07:06.256598: step  8820, loss = (G: 16.41605949, D: 0.01468118) (0.228 sec/batch)\n",
      "2017-04-05 21:07:10.765002: step  8840, loss = (G: 4.91756725, D: 0.13424379) (0.225 sec/batch)\n",
      "2017-04-05 21:07:15.270900: step  8860, loss = (G: 9.11168575, D: 0.08439744) (0.224 sec/batch)\n",
      "2017-04-05 21:07:19.777593: step  8880, loss = (G: 6.05227995, D: 0.04410364) (0.225 sec/batch)\n",
      "2017-04-05 21:07:24.296962: step  8900, loss = (G: 14.70324516, D: 5.28938341) (0.225 sec/batch)\n",
      "2017-04-05 21:07:28.933745: step  8920, loss = (G: 10.09322548, D: 0.01795025) (0.227 sec/batch)\n",
      "2017-04-05 21:07:33.449969: step  8940, loss = (G: 18.46896553, D: 0.06160098) (0.225 sec/batch)\n",
      "2017-04-05 21:07:37.964039: step  8960, loss = (G: 5.71876049, D: 0.06441937) (0.226 sec/batch)\n",
      "2017-04-05 21:07:42.464418: step  8980, loss = (G: 11.85078812, D: 0.04272366) (0.224 sec/batch)\n",
      "2017-04-05 21:07:46.963240: step  9000, loss = (G: 9.62473106, D: 1.37366426) (0.226 sec/batch)\n",
      "2017-04-05 21:07:51.636086: step  9020, loss = (G: 11.99115086, D: 0.01424482) (0.224 sec/batch)\n",
      "2017-04-05 21:07:56.139968: step  9040, loss = (G: 10.44548225, D: 0.07615079) (0.225 sec/batch)\n",
      "2017-04-05 21:08:00.639452: step  9060, loss = (G: 7.84698391, D: 0.03422967) (0.225 sec/batch)\n",
      "2017-04-05 21:08:05.136699: step  9080, loss = (G: 4.81433201, D: 0.12110043) (0.231 sec/batch)\n",
      "2017-04-05 21:08:09.665965: step  9100, loss = (G: 12.38016319, D: 0.28801501) (0.225 sec/batch)\n",
      "2017-04-05 21:08:14.320808: step  9120, loss = (G: 16.01956558, D: 0.02334750) (0.228 sec/batch)\n",
      "2017-04-05 21:08:18.840047: step  9140, loss = (G: 3.88938427, D: 0.30879265) (0.226 sec/batch)\n",
      "2017-04-05 21:08:23.349511: step  9160, loss = (G: 6.10350800, D: 0.03305418) (0.225 sec/batch)\n",
      "2017-04-05 21:08:27.858336: step  9180, loss = (G: 2.90456438, D: 0.56375378) (0.224 sec/batch)\n",
      "2017-04-05 21:08:32.354127: step  9200, loss = (G: 7.74584150, D: 0.05314811) (0.225 sec/batch)\n",
      "2017-04-05 21:08:36.990811: step  9220, loss = (G: 13.79940605, D: 0.16096622) (0.225 sec/batch)\n",
      "2017-04-05 21:08:41.641882: step  9240, loss = (G: 10.13651371, D: 0.01636356) (0.224 sec/batch)\n",
      "2017-04-05 21:08:46.157690: step  9260, loss = (G: 2.97812676, D: 0.56794137) (0.229 sec/batch)\n",
      "2017-04-05 21:08:50.664022: step  9280, loss = (G: 7.46828413, D: 0.12264491) (0.226 sec/batch)\n",
      "2017-04-05 21:08:55.208553: step  9300, loss = (G: 10.34354210, D: 0.41277185) (0.230 sec/batch)\n",
      "2017-04-05 21:08:59.852592: step  9320, loss = (G: 13.03198719, D: 0.08040170) (0.225 sec/batch)\n",
      "2017-04-05 21:09:04.363606: step  9340, loss = (G: 5.94167805, D: 0.03943805) (0.225 sec/batch)\n",
      "2017-04-05 21:09:08.872369: step  9360, loss = (G: 4.10500383, D: 0.24599513) (0.226 sec/batch)\n",
      "2017-04-05 21:09:13.374392: step  9380, loss = (G: 7.82371950, D: 0.07220547) (0.224 sec/batch)\n",
      "2017-04-05 21:09:17.877847: step  9400, loss = (G: 3.27960491, D: 0.26793721) (0.225 sec/batch)\n",
      "2017-04-05 21:09:22.522099: step  9420, loss = (G: 9.23779011, D: 0.06897744) (0.224 sec/batch)\n",
      "2017-04-05 21:09:27.022311: step  9440, loss = (G: 3.78631306, D: 0.15930876) (0.225 sec/batch)\n",
      "2017-04-05 21:09:31.534070: step  9460, loss = (G: 5.40346336, D: 0.05422665) (0.226 sec/batch)\n",
      "2017-04-05 21:09:36.064254: step  9480, loss = (G: 4.06046724, D: 0.48073956) (0.228 sec/batch)\n",
      "2017-04-05 21:09:40.575783: step  9500, loss = (G: 5.00037575, D: 0.08532724) (0.224 sec/batch)\n",
      "2017-04-05 21:09:45.236177: step  9520, loss = (G: 14.78230286, D: 0.28055513) (0.225 sec/batch)\n",
      "2017-04-05 21:09:49.740773: step  9540, loss = (G: 4.03615189, D: 0.18838713) (0.224 sec/batch)\n",
      "2017-04-05 21:09:54.248026: step  9560, loss = (G: 7.37667322, D: 0.18040571) (0.224 sec/batch)\n",
      "2017-04-05 21:09:58.748686: step  9580, loss = (G: 4.82968426, D: 0.08457363) (0.225 sec/batch)\n",
      "2017-04-05 21:10:03.254459: step  9600, loss = (G: 15.04765320, D: 0.00423745) (0.225 sec/batch)\n",
      "2017-04-05 21:10:07.909673: step  9620, loss = (G: 5.44121742, D: 0.27250946) (0.227 sec/batch)\n",
      "2017-04-05 21:10:12.424737: step  9640, loss = (G: 6.25802231, D: 0.51716989) (0.225 sec/batch)\n",
      "2017-04-05 21:10:16.913953: step  9660, loss = (G: 3.22908545, D: 0.65383089) (0.224 sec/batch)\n",
      "2017-04-05 21:10:21.432166: step  9680, loss = (G: 11.79088688, D: 0.02034329) (0.231 sec/batch)\n",
      "2017-04-05 21:10:25.954768: step  9700, loss = (G: 6.45550823, D: 0.07313816) (0.226 sec/batch)\n",
      "2017-04-05 21:10:30.589599: step  9720, loss = (G: 10.15988731, D: 0.00942128) (0.224 sec/batch)\n",
      "2017-04-05 21:10:35.092709: step  9740, loss = (G: 8.73652554, D: 0.00579204) (0.225 sec/batch)\n",
      "2017-04-05 21:10:39.600363: step  9760, loss = (G: 6.87573814, D: 0.13354959) (0.224 sec/batch)\n",
      "2017-04-05 21:10:44.103586: step  9780, loss = (G: 15.80736446, D: 0.14799674) (0.224 sec/batch)\n",
      "2017-04-05 21:10:48.598192: step  9800, loss = (G: 4.48351192, D: 0.16831078) (0.223 sec/batch)\n",
      "2017-04-05 21:10:53.247227: step  9820, loss = (G: 12.97435951, D: 3.00268221) (0.226 sec/batch)\n",
      "2017-04-05 21:10:57.758916: step  9840, loss = (G: 8.25928879, D: 0.08761600) (0.225 sec/batch)\n",
      "2017-04-05 21:11:02.256817: step  9860, loss = (G: 9.56942177, D: 0.01107916) (0.230 sec/batch)\n",
      "2017-04-05 21:11:06.772708: step  9880, loss = (G: 5.42606544, D: 0.11272983) (0.224 sec/batch)\n",
      "2017-04-05 21:11:11.277887: step  9900, loss = (G: 4.25425577, D: 0.10940776) (0.224 sec/batch)\n",
      "2017-04-05 21:11:15.932685: step  9920, loss = (G: 11.32021236, D: 0.06232601) (0.229 sec/batch)\n",
      "2017-04-05 21:11:20.437832: step  9940, loss = (G: 7.86538649, D: 0.01844479) (0.224 sec/batch)\n",
      "2017-04-05 21:11:24.933103: step  9960, loss = (G: 6.31129885, D: 0.17897382) (0.224 sec/batch)\n",
      "2017-04-05 21:11:29.457369: step  9980, loss = (G: 11.12502098, D: 0.03249681) (0.225 sec/batch)\n",
      "2017-04-05 21:11:33.970243: step 10000, loss = (G: 10.11930466, D: 0.21575046) (0.226 sec/batch)\n",
      "2017-04-05 21:11:38.614879: step 10020, loss = (G: 8.05348969, D: 0.01232839) (0.225 sec/batch)\n",
      "2017-04-05 21:11:43.147740: step 10040, loss = (G: 7.23190546, D: 0.03708611) (0.231 sec/batch)\n",
      "2017-04-05 21:11:47.654648: step 10060, loss = (G: 7.57940531, D: 0.03437334) (0.225 sec/batch)\n",
      "2017-04-05 21:11:52.165433: step 10080, loss = (G: 10.30029011, D: 0.09610000) (0.224 sec/batch)\n",
      "2017-04-05 21:11:56.659114: step 10100, loss = (G: 3.09384108, D: 0.43015236) (0.226 sec/batch)\n",
      "2017-04-05 21:12:01.305485: step 10120, loss = (G: 8.84701538, D: 0.64687037) (0.225 sec/batch)\n",
      "2017-04-05 21:12:05.813714: step 10140, loss = (G: 5.79609394, D: 0.04429483) (0.226 sec/batch)\n",
      "2017-04-05 21:12:10.326491: step 10160, loss = (G: 5.93492937, D: 0.45643255) (0.224 sec/batch)\n",
      "2017-04-05 21:12:14.837531: step 10180, loss = (G: 11.06915092, D: 0.06151623) (0.225 sec/batch)\n",
      "2017-04-05 21:12:19.342384: step 10200, loss = (G: 2.12479758, D: 0.61392498) (0.225 sec/batch)\n",
      "2017-04-05 21:12:23.983131: step 10220, loss = (G: 6.25207710, D: 0.03384297) (0.225 sec/batch)\n",
      "2017-04-05 21:12:28.488913: step 10240, loss = (G: 5.97666168, D: 0.08196476) (0.225 sec/batch)\n",
      "2017-04-05 21:12:32.994278: step 10260, loss = (G: 7.48299217, D: 0.04726862) (0.225 sec/batch)\n",
      "2017-04-05 21:12:37.543134: step 10280, loss = (G: 8.68270397, D: 0.00943928) (0.224 sec/batch)\n",
      "2017-04-05 21:12:42.049734: step 10300, loss = (G: 8.75782871, D: 0.01118203) (0.225 sec/batch)\n",
      "2017-04-05 21:12:46.683740: step 10320, loss = (G: 4.36096048, D: 0.10778966) (0.225 sec/batch)\n",
      "2017-04-05 21:12:51.191152: step 10340, loss = (G: 19.30768204, D: 0.02624458) (0.226 sec/batch)\n",
      "2017-04-05 21:12:55.699391: step 10360, loss = (G: 6.26450872, D: 0.11073555) (0.224 sec/batch)\n",
      "2017-04-05 21:13:00.242211: step 10380, loss = (G: 7.52721119, D: 0.01663482) (0.225 sec/batch)\n",
      "2017-04-05 21:13:04.887985: step 10400, loss = (G: 12.40078640, D: 0.00165460) (0.370 sec/batch)\n",
      "2017-04-05 21:13:09.540500: step 10420, loss = (G: 8.33634758, D: 0.01978921) (0.224 sec/batch)\n",
      "2017-04-05 21:13:14.042959: step 10440, loss = (G: 10.71875381, D: 0.00266215) (0.224 sec/batch)\n",
      "2017-04-05 21:13:18.545349: step 10460, loss = (G: 6.69609070, D: 0.03552631) (0.225 sec/batch)\n",
      "2017-04-05 21:13:23.044983: step 10480, loss = (G: 7.03610945, D: 0.10888471) (0.226 sec/batch)\n",
      "2017-04-05 21:13:27.582465: step 10500, loss = (G: 19.73543167, D: 0.00426166) (0.226 sec/batch)\n",
      "2017-04-05 21:13:32.225999: step 10520, loss = (G: 3.78436327, D: 0.31547612) (0.225 sec/batch)\n",
      "2017-04-05 21:13:36.731097: step 10540, loss = (G: 4.15991688, D: 0.14435408) (0.223 sec/batch)\n",
      "2017-04-05 21:13:41.235800: step 10560, loss = (G: 5.67117262, D: 0.10941263) (0.224 sec/batch)\n",
      "2017-04-05 21:13:45.742557: step 10580, loss = (G: 6.05966234, D: 0.02957929) (0.225 sec/batch)\n",
      "2017-04-05 21:13:50.242035: step 10600, loss = (G: 7.36255884, D: 0.63642520) (0.225 sec/batch)\n",
      "2017-04-05 21:13:54.894550: step 10620, loss = (G: 10.16253757, D: 0.38774860) (0.224 sec/batch)\n",
      "2017-04-05 21:13:59.391629: step 10640, loss = (G: 4.72607374, D: 0.50324064) (0.225 sec/batch)\n",
      "2017-04-05 21:14:03.893981: step 10660, loss = (G: 4.68870831, D: 0.18700702) (0.224 sec/batch)\n",
      "2017-04-05 21:14:08.409308: step 10680, loss = (G: 7.13979101, D: 0.51496464) (0.225 sec/batch)\n",
      "2017-04-05 21:14:12.908032: step 10700, loss = (G: 4.04805803, D: 0.13913833) (0.226 sec/batch)\n",
      "2017-04-05 21:14:17.564816: step 10720, loss = (G: 7.23539495, D: 0.05592544) (0.226 sec/batch)\n",
      "2017-04-05 21:14:22.063772: step 10740, loss = (G: 9.78232002, D: 0.10558262) (0.229 sec/batch)\n",
      "2017-04-05 21:14:26.575069: step 10760, loss = (G: 6.34150410, D: 0.02524996) (0.225 sec/batch)\n",
      "2017-04-05 21:14:31.078597: step 10780, loss = (G: 10.20325565, D: 0.01160651) (0.225 sec/batch)\n",
      "2017-04-05 21:14:35.588313: step 10800, loss = (G: 4.71078491, D: 0.14156063) (0.227 sec/batch)\n",
      "2017-04-05 21:14:40.245703: step 10820, loss = (G: 2.95970154, D: 0.53981251) (0.226 sec/batch)\n",
      "2017-04-05 21:14:44.749238: step 10840, loss = (G: 8.13940525, D: 0.01947348) (0.225 sec/batch)\n",
      "2017-04-05 21:14:49.243241: step 10860, loss = (G: 6.45383739, D: 0.03830191) (0.225 sec/batch)\n",
      "2017-04-05 21:14:53.762179: step 10880, loss = (G: 5.64928484, D: 0.09638900) (0.224 sec/batch)\n",
      "2017-04-05 21:14:58.262927: step 10900, loss = (G: 5.18371296, D: 0.10408956) (0.225 sec/batch)\n",
      "2017-04-05 21:15:02.911218: step 10920, loss = (G: 6.43917751, D: 0.10792115) (0.225 sec/batch)\n",
      "2017-04-05 21:15:07.427303: step 10940, loss = (G: 14.63724041, D: 0.01115411) (0.231 sec/batch)\n",
      "2017-04-05 21:15:11.945884: step 10960, loss = (G: 11.91741943, D: 0.03831453) (0.224 sec/batch)\n",
      "2017-04-05 21:15:16.442334: step 10980, loss = (G: 11.16299820, D: 0.01846398) (0.226 sec/batch)\n",
      "2017-04-05 21:15:20.942979: step 11000, loss = (G: 7.84810925, D: 0.09378313) (0.224 sec/batch)\n",
      "2017-04-05 21:15:25.585734: step 11020, loss = (G: 8.24566174, D: 0.34749597) (0.225 sec/batch)\n",
      "2017-04-05 21:15:30.097555: step 11040, loss = (G: 6.76778126, D: 0.03190789) (0.226 sec/batch)\n",
      "2017-04-05 21:15:34.615097: step 11060, loss = (G: 10.42529202, D: 0.03429350) (0.226 sec/batch)\n",
      "2017-04-05 21:15:39.124717: step 11080, loss = (G: 9.11819172, D: 0.00470732) (0.224 sec/batch)\n",
      "2017-04-05 21:15:43.629874: step 11100, loss = (G: 4.54095364, D: 0.12852359) (0.225 sec/batch)\n",
      "2017-04-05 21:15:48.277446: step 11120, loss = (G: 5.55713892, D: 0.04317771) (0.225 sec/batch)\n",
      "2017-04-05 21:15:52.793879: step 11140, loss = (G: 4.03705931, D: 0.16325232) (0.225 sec/batch)\n",
      "2017-04-05 21:15:57.293074: step 11160, loss = (G: 14.24587917, D: 0.51503974) (0.228 sec/batch)\n",
      "2017-04-05 21:16:01.812751: step 11180, loss = (G: 10.89060974, D: 0.00198231) (0.225 sec/batch)\n",
      "2017-04-05 21:16:06.318922: step 11200, loss = (G: 3.55778098, D: 0.19922170) (0.224 sec/batch)\n",
      "2017-04-05 21:16:10.976902: step 11220, loss = (G: 9.07391167, D: 0.20212926) (0.225 sec/batch)\n",
      "2017-04-05 21:16:15.490598: step 11240, loss = (G: 8.90277958, D: 0.02425443) (0.225 sec/batch)\n",
      "2017-04-05 21:16:19.994129: step 11260, loss = (G: 5.94238377, D: 0.10686436) (0.225 sec/batch)\n",
      "2017-04-05 21:16:24.492840: step 11280, loss = (G: 6.31299448, D: 0.08071281) (0.225 sec/batch)\n",
      "2017-04-05 21:16:29.000227: step 11300, loss = (G: 7.42210913, D: 0.34564239) (0.225 sec/batch)\n",
      "2017-04-05 21:16:33.636898: step 11320, loss = (G: 6.78574944, D: 0.03548783) (0.225 sec/batch)\n",
      "2017-04-05 21:16:38.142611: step 11340, loss = (G: 11.63865376, D: 0.05229824) (0.231 sec/batch)\n",
      "2017-04-05 21:16:42.686338: step 11360, loss = (G: 5.88008404, D: 0.04746887) (0.223 sec/batch)\n",
      "2017-04-05 21:16:47.195299: step 11380, loss = (G: 10.62706184, D: 0.03859592) (0.226 sec/batch)\n",
      "2017-04-05 21:16:51.704023: step 11400, loss = (G: 7.20043850, D: 0.05586380) (0.225 sec/batch)\n",
      "2017-04-05 21:16:56.354727: step 11420, loss = (G: 7.72459412, D: 0.03035913) (0.226 sec/batch)\n",
      "2017-04-05 21:17:00.859831: step 11440, loss = (G: 5.51781368, D: 0.08256663) (0.226 sec/batch)\n",
      "2017-04-05 21:17:05.365359: step 11460, loss = (G: 9.06553364, D: 0.09782358) (0.225 sec/batch)\n",
      "2017-04-05 21:17:09.867029: step 11480, loss = (G: 10.76847649, D: 0.13556750) (0.225 sec/batch)\n",
      "2017-04-05 21:17:14.367480: step 11500, loss = (G: 7.50829935, D: 0.01043651) (0.224 sec/batch)\n",
      "2017-04-05 21:17:19.003822: step 11520, loss = (G: 9.14692688, D: 0.08273515) (0.226 sec/batch)\n",
      "2017-04-05 21:17:23.514041: step 11540, loss = (G: 7.11153316, D: 0.23190856) (0.228 sec/batch)\n",
      "2017-04-05 21:17:28.040392: step 11560, loss = (G: 6.61392689, D: 0.05900330) (0.225 sec/batch)\n",
      "2017-04-05 21:17:32.679760: step 11580, loss = (G: 13.09824467, D: 0.02055979) (0.224 sec/batch)\n",
      "2017-04-05 21:17:37.177166: step 11600, loss = (G: 6.06543016, D: 0.07886022) (0.225 sec/batch)\n",
      "2017-04-05 21:17:41.820209: step 11620, loss = (G: 8.13146210, D: 0.02997480) (0.224 sec/batch)\n",
      "2017-04-05 21:17:46.341174: step 11640, loss = (G: 8.23238277, D: 0.01181135) (0.225 sec/batch)\n",
      "2017-04-05 21:17:50.851462: step 11660, loss = (G: 13.19208622, D: 0.05004513) (0.224 sec/batch)\n",
      "2017-04-05 21:17:55.358888: step 11680, loss = (G: 7.24867010, D: 0.03400233) (0.226 sec/batch)\n",
      "2017-04-05 21:17:59.869647: step 11700, loss = (G: 5.68208981, D: 0.28187817) (0.226 sec/batch)\n",
      "2017-04-05 21:18:04.518245: step 11720, loss = (G: 8.51053143, D: 0.01393816) (0.225 sec/batch)\n",
      "2017-04-05 21:18:09.025590: step 11740, loss = (G: 7.06921196, D: 0.02397690) (0.230 sec/batch)\n",
      "2017-04-05 21:18:13.535452: step 11760, loss = (G: 12.11154556, D: 0.00331823) (0.226 sec/batch)\n",
      "2017-04-05 21:18:18.034338: step 11780, loss = (G: 11.54833889, D: 0.00380144) (0.224 sec/batch)\n",
      "2017-04-05 21:18:22.553442: step 11800, loss = (G: 11.98466396, D: 0.00988894) (0.225 sec/batch)\n",
      "2017-04-05 21:18:27.203231: step 11820, loss = (G: 8.41832161, D: 0.07316461) (0.224 sec/batch)\n",
      "2017-04-05 21:18:31.705530: step 11840, loss = (G: 6.27737141, D: 0.18892843) (0.225 sec/batch)\n",
      "2017-04-05 21:18:36.229542: step 11860, loss = (G: 11.34190941, D: 0.01007602) (0.224 sec/batch)\n",
      "2017-04-05 21:18:40.740315: step 11880, loss = (G: 6.71289206, D: 0.06326199) (0.225 sec/batch)\n",
      "2017-04-05 21:18:45.267735: step 11900, loss = (G: 9.33431339, D: 0.07908262) (0.225 sec/batch)\n",
      "2017-04-05 21:18:49.901264: step 11920, loss = (G: 6.84995174, D: 0.11547558) (0.225 sec/batch)\n",
      "2017-04-05 21:18:54.418322: step 11940, loss = (G: 3.45400214, D: 0.60381746) (0.226 sec/batch)\n",
      "2017-04-05 21:18:58.936207: step 11960, loss = (G: 12.92436790, D: 0.02914470) (0.225 sec/batch)\n",
      "2017-04-05 21:19:03.435977: step 11980, loss = (G: 5.06044292, D: 0.10286124) (0.226 sec/batch)\n",
      "2017-04-05 21:19:07.936896: step 12000, loss = (G: 8.73120403, D: 0.03530433) (0.226 sec/batch)\n",
      "2017-04-05 21:19:12.592198: step 12020, loss = (G: 4.65359497, D: 0.10945319) (0.225 sec/batch)\n",
      "2017-04-05 21:19:17.120702: step 12040, loss = (G: 3.70357323, D: 0.18646328) (0.225 sec/batch)\n",
      "2017-04-05 21:19:21.635119: step 12060, loss = (G: 7.16073990, D: 0.07663365) (0.224 sec/batch)\n",
      "2017-04-05 21:19:26.139952: step 12080, loss = (G: 6.77423382, D: 0.14755701) (0.225 sec/batch)\n",
      "2017-04-05 21:19:30.639278: step 12100, loss = (G: 9.42433643, D: 0.06131092) (0.225 sec/batch)\n",
      "2017-04-05 21:19:35.301179: step 12120, loss = (G: 8.93835926, D: 0.24918868) (0.226 sec/batch)\n",
      "2017-04-05 21:19:39.809669: step 12140, loss = (G: 12.70436668, D: 0.43791002) (0.228 sec/batch)\n",
      "2017-04-05 21:19:44.321474: step 12160, loss = (G: 6.51107883, D: 0.03139712) (0.226 sec/batch)\n",
      "2017-04-05 21:19:48.826671: step 12180, loss = (G: 9.68577862, D: 0.23862040) (0.227 sec/batch)\n",
      "2017-04-05 21:19:53.327460: step 12200, loss = (G: 12.53055286, D: 0.00855220) (0.226 sec/batch)\n",
      "2017-04-05 21:19:57.981142: step 12220, loss = (G: 5.90805054, D: 0.05359453) (0.227 sec/batch)\n",
      "2017-04-05 21:20:02.496349: step 12240, loss = (G: 7.57011604, D: 0.17260775) (0.224 sec/batch)\n",
      "2017-04-05 21:20:07.043805: step 12260, loss = (G: 8.54606152, D: 0.02414997) (0.226 sec/batch)\n",
      "2017-04-05 21:20:11.530972: step 12280, loss = (G: 8.57390499, D: 0.08768827) (0.225 sec/batch)\n",
      "2017-04-05 21:20:16.037338: step 12300, loss = (G: 9.34511185, D: 0.00883315) (0.225 sec/batch)\n",
      "2017-04-05 21:20:20.695347: step 12320, loss = (G: 4.76347446, D: 0.11861329) (0.225 sec/batch)\n",
      "2017-04-05 21:20:25.190721: step 12340, loss = (G: 7.12026119, D: 0.03027293) (0.228 sec/batch)\n",
      "2017-04-05 21:20:29.698496: step 12360, loss = (G: 10.03214550, D: 0.04643678) (0.223 sec/batch)\n",
      "2017-04-05 21:20:34.220609: step 12380, loss = (G: 10.53566647, D: 0.04235144) (0.226 sec/batch)\n",
      "2017-04-05 21:20:38.719208: step 12400, loss = (G: 5.29415226, D: 0.04416685) (0.226 sec/batch)\n",
      "2017-04-05 21:20:43.377855: step 12420, loss = (G: 8.28812313, D: 0.01174631) (0.225 sec/batch)\n",
      "2017-04-05 21:20:47.873171: step 12440, loss = (G: 13.82375431, D: 5.01873636) (0.224 sec/batch)\n",
      "2017-04-05 21:20:52.383272: step 12460, loss = (G: 13.93343163, D: 0.17088610) (0.227 sec/batch)\n",
      "2017-04-05 21:20:56.880247: step 12480, loss = (G: 5.57379198, D: 0.17753702) (0.224 sec/batch)\n",
      "2017-04-05 21:21:01.383845: step 12500, loss = (G: 8.66176605, D: 0.01328393) (0.225 sec/batch)\n",
      "2017-04-05 21:21:06.022990: step 12520, loss = (G: 3.66264439, D: 0.20924084) (0.226 sec/batch)\n",
      "2017-04-05 21:21:10.525893: step 12540, loss = (G: 7.32583332, D: 0.12507923) (0.226 sec/batch)\n",
      "2017-04-05 21:21:15.030399: step 12560, loss = (G: 9.37313271, D: 0.13923675) (0.225 sec/batch)\n",
      "2017-04-05 21:21:19.529875: step 12580, loss = (G: 4.46313667, D: 0.15853713) (0.225 sec/batch)\n",
      "2017-04-05 21:21:24.036715: step 12600, loss = (G: 6.66509151, D: 0.43596894) (0.224 sec/batch)\n",
      "2017-04-05 21:21:28.674471: step 12620, loss = (G: 6.71478558, D: 0.22660637) (0.224 sec/batch)\n",
      "2017-04-05 21:21:33.186935: step 12640, loss = (G: 4.25381231, D: 0.23744851) (0.226 sec/batch)\n",
      "2017-04-05 21:21:37.686006: step 12660, loss = (G: 6.56410217, D: 0.05135535) (0.225 sec/batch)\n",
      "2017-04-05 21:21:42.179024: step 12680, loss = (G: 6.90463638, D: 0.05498783) (0.227 sec/batch)\n",
      "2017-04-05 21:21:46.699391: step 12700, loss = (G: 6.37374687, D: 0.05105769) (0.228 sec/batch)\n",
      "2017-04-05 21:21:51.486143: step 12720, loss = (G: 5.69759083, D: 0.04914322) (0.370 sec/batch)\n",
      "2017-04-05 21:21:56.029222: step 12740, loss = (G: 7.91373301, D: 0.03721322) (0.226 sec/batch)\n",
      "2017-04-05 21:22:00.566278: step 12760, loss = (G: 12.60310555, D: 0.05580259) (0.228 sec/batch)\n",
      "2017-04-05 21:22:05.067533: step 12780, loss = (G: 4.58108282, D: 0.13351616) (0.225 sec/batch)\n",
      "2017-04-05 21:22:09.565399: step 12800, loss = (G: 4.63446903, D: 0.15303101) (0.225 sec/batch)\n",
      "2017-04-05 21:22:14.201630: step 12820, loss = (G: 6.80384922, D: 0.13187079) (0.224 sec/batch)\n",
      "2017-04-05 21:22:18.749147: step 12840, loss = (G: 6.49822330, D: 0.08418325) (0.226 sec/batch)\n",
      "2017-04-05 21:22:23.245245: step 12860, loss = (G: 6.42753315, D: 0.10136861) (0.225 sec/batch)\n",
      "2017-04-05 21:22:27.732864: step 12880, loss = (G: 11.15779305, D: 0.06694785) (0.223 sec/batch)\n",
      "2017-04-05 21:22:32.241184: step 12900, loss = (G: 11.08619690, D: 0.25891805) (0.225 sec/batch)\n",
      "2017-04-05 21:22:36.880395: step 12920, loss = (G: 4.93711853, D: 0.10194241) (0.225 sec/batch)\n",
      "2017-04-05 21:22:41.380318: step 12940, loss = (G: 11.79290962, D: 1.07395709) (0.225 sec/batch)\n",
      "2017-04-05 21:22:45.879855: step 12960, loss = (G: 5.28899145, D: 0.08442514) (0.225 sec/batch)\n",
      "2017-04-05 21:22:50.382108: step 12980, loss = (G: 5.39410400, D: 0.07135060) (0.226 sec/batch)\n",
      "2017-04-05 21:22:54.873282: step 13000, loss = (G: 5.81315041, D: 0.06130794) (0.224 sec/batch)\n",
      "2017-04-05 21:22:59.531054: step 13020, loss = (G: 8.17678738, D: 0.00763127) (0.227 sec/batch)\n",
      "2017-04-05 21:23:04.058947: step 13040, loss = (G: 9.78126717, D: 0.07505503) (0.226 sec/batch)\n",
      "2017-04-05 21:23:08.556568: step 13060, loss = (G: 9.08846664, D: 0.02247255) (0.224 sec/batch)\n",
      "2017-04-05 21:23:13.066071: step 13080, loss = (G: 11.53734970, D: 0.03987808) (0.225 sec/batch)\n",
      "2017-04-05 21:23:17.584844: step 13100, loss = (G: 10.93010330, D: 0.01987415) (0.225 sec/batch)\n",
      "2017-04-05 21:23:22.233139: step 13120, loss = (G: 5.60164213, D: 0.18514735) (0.225 sec/batch)\n",
      "2017-04-05 21:23:26.729391: step 13140, loss = (G: 10.36318874, D: 0.01677072) (0.227 sec/batch)\n",
      "2017-04-05 21:23:31.225539: step 13160, loss = (G: 8.24656677, D: 0.01294442) (0.225 sec/batch)\n",
      "2017-04-05 21:23:35.724677: step 13180, loss = (G: 6.76526260, D: 0.02651500) (0.226 sec/batch)\n",
      "2017-04-05 21:23:40.224094: step 13200, loss = (G: 4.82349777, D: 0.07625730) (0.226 sec/batch)\n",
      "2017-04-05 21:23:44.873397: step 13220, loss = (G: 7.65494156, D: 0.04869364) (0.226 sec/batch)\n",
      "2017-04-05 21:23:49.368394: step 13240, loss = (G: 7.05513763, D: 0.02533120) (0.225 sec/batch)\n",
      "2017-04-05 21:23:53.877661: step 13260, loss = (G: 6.71093750, D: 0.02965109) (0.231 sec/batch)\n",
      "2017-04-05 21:23:58.421005: step 13280, loss = (G: 6.05810022, D: 0.06274299) (0.224 sec/batch)\n",
      "2017-04-05 21:24:02.938279: step 13300, loss = (G: 6.75977421, D: 0.01888451) (0.223 sec/batch)\n",
      "2017-04-05 21:24:07.612096: step 13320, loss = (G: 3.22999811, D: 0.45607957) (0.227 sec/batch)\n",
      "2017-04-05 21:24:12.102253: step 13340, loss = (G: 7.62656784, D: 0.08049434) (0.224 sec/batch)\n",
      "2017-04-05 21:24:16.612814: step 13360, loss = (G: 9.08255577, D: 0.01125515) (0.225 sec/batch)\n",
      "2017-04-05 21:24:21.109684: step 13380, loss = (G: 6.98538017, D: 0.07577938) (0.224 sec/batch)\n",
      "2017-04-05 21:24:25.606023: step 13400, loss = (G: 6.21409130, D: 0.03668066) (0.224 sec/batch)\n",
      "2017-04-05 21:24:30.250650: step 13420, loss = (G: 8.27158546, D: 0.01253468) (0.225 sec/batch)\n",
      "2017-04-05 21:24:34.748360: step 13440, loss = (G: 5.15486288, D: 0.10235496) (0.223 sec/batch)\n",
      "2017-04-05 21:24:39.264900: step 13460, loss = (G: 7.99877453, D: 0.01281585) (0.225 sec/batch)\n",
      "2017-04-05 21:24:43.772604: step 13480, loss = (G: 4.49105072, D: 0.23809554) (0.225 sec/batch)\n",
      "2017-04-05 21:24:48.280683: step 13500, loss = (G: 7.72399902, D: 0.01325452) (0.224 sec/batch)\n",
      "2017-04-05 21:24:52.935933: step 13520, loss = (G: 4.34260941, D: 0.24725451) (0.226 sec/batch)\n",
      "2017-04-05 21:24:57.442310: step 13540, loss = (G: 7.06504059, D: 0.10901805) (0.229 sec/batch)\n",
      "2017-04-05 21:25:01.942929: step 13560, loss = (G: 5.46656322, D: 0.11255419) (0.224 sec/batch)\n",
      "2017-04-05 21:25:06.438185: step 13580, loss = (G: 7.50396538, D: 0.05613765) (0.225 sec/batch)\n",
      "2017-04-05 21:25:10.940451: step 13600, loss = (G: 4.21284103, D: 0.13607499) (0.226 sec/batch)\n",
      "2017-04-05 21:25:15.584686: step 13620, loss = (G: 8.69369411, D: 0.00858035) (0.224 sec/batch)\n",
      "2017-04-05 21:25:20.102726: step 13640, loss = (G: 10.50169945, D: 0.01849396) (0.226 sec/batch)\n",
      "2017-04-05 21:25:24.607605: step 13660, loss = (G: 7.31458950, D: 0.01276877) (0.226 sec/batch)\n",
      "2017-04-05 21:25:29.111160: step 13680, loss = (G: 3.26979494, D: 0.31500417) (0.225 sec/batch)\n",
      "2017-04-05 21:25:33.614365: step 13700, loss = (G: 5.29463196, D: 0.13093574) (0.225 sec/batch)\n",
      "2017-04-05 21:25:38.269997: step 13720, loss = (G: 9.32291222, D: 0.01473212) (0.226 sec/batch)\n",
      "2017-04-05 21:25:42.769920: step 13740, loss = (G: 5.61788654, D: 0.14185259) (0.226 sec/batch)\n",
      "2017-04-05 21:25:47.274924: step 13760, loss = (G: 5.28290844, D: 0.07990044) (0.227 sec/batch)\n",
      "2017-04-05 21:25:51.777172: step 13780, loss = (G: 5.23773003, D: 0.05996870) (0.224 sec/batch)\n",
      "2017-04-05 21:25:56.283927: step 13800, loss = (G: 7.59785175, D: 0.00816676) (0.225 sec/batch)\n",
      "2017-04-05 21:26:00.949020: step 13820, loss = (G: 11.13908768, D: 0.00701802) (0.230 sec/batch)\n",
      "2017-04-05 21:26:05.456329: step 13840, loss = (G: 7.46984768, D: 0.02688650) (0.224 sec/batch)\n",
      "2017-04-05 21:26:09.958355: step 13860, loss = (G: 6.42413902, D: 0.08445813) (0.225 sec/batch)\n",
      "2017-04-05 21:26:14.468023: step 13880, loss = (G: 6.87291622, D: 0.10091033) (0.223 sec/batch)\n",
      "2017-04-05 21:26:19.176005: step 13900, loss = (G: 5.50697327, D: 0.08843021) (0.225 sec/batch)\n",
      "2017-04-05 21:26:23.796809: step 13920, loss = (G: 18.58352852, D: 1.08154583) (0.225 sec/batch)\n",
      "2017-04-05 21:26:28.284084: step 13940, loss = (G: 6.23454285, D: 0.03872614) (0.225 sec/batch)\n",
      "2017-04-05 21:26:32.806381: step 13960, loss = (G: 8.60822582, D: 0.01000202) (0.225 sec/batch)\n",
      "2017-04-05 21:26:37.309307: step 13980, loss = (G: 5.95112658, D: 0.04373485) (0.225 sec/batch)\n",
      "2017-04-05 21:26:41.809805: step 14000, loss = (G: 7.49986172, D: 0.11946125) (0.226 sec/batch)\n",
      "2017-04-05 21:26:46.443011: step 14020, loss = (G: 8.85916615, D: 0.03454772) (0.225 sec/batch)\n",
      "2017-04-05 21:26:50.957059: step 14040, loss = (G: 7.76582956, D: 0.16740179) (0.223 sec/batch)\n",
      "2017-04-05 21:26:55.464193: step 14060, loss = (G: 11.58130646, D: 0.15697655) (0.225 sec/batch)\n",
      "2017-04-05 21:26:59.971099: step 14080, loss = (G: 5.71842384, D: 0.07119196) (0.223 sec/batch)\n",
      "2017-04-05 21:27:04.484782: step 14100, loss = (G: 11.72416306, D: 0.54460174) (0.224 sec/batch)\n",
      "2017-04-05 21:27:09.112705: step 14120, loss = (G: 7.14611673, D: 0.05782863) (0.224 sec/batch)\n",
      "2017-04-05 21:27:13.612457: step 14140, loss = (G: 5.06795788, D: 0.09314442) (0.225 sec/batch)\n",
      "2017-04-05 21:27:18.107751: step 14160, loss = (G: 6.96655798, D: 0.01507233) (0.224 sec/batch)\n",
      "2017-04-05 21:27:22.619229: step 14180, loss = (G: 5.21949625, D: 0.05920196) (0.224 sec/batch)\n",
      "2017-04-05 21:27:27.116116: step 14200, loss = (G: 21.08562469, D: 4.13066435) (0.226 sec/batch)\n",
      "2017-04-05 21:27:31.754257: step 14220, loss = (G: 5.46891499, D: 0.08585425) (0.228 sec/batch)\n",
      "2017-04-05 21:27:36.259285: step 14240, loss = (G: 5.31868649, D: 0.10786411) (0.224 sec/batch)\n",
      "2017-04-05 21:27:40.754862: step 14260, loss = (G: 7.45190334, D: 0.02897332) (0.224 sec/batch)\n",
      "2017-04-05 21:27:45.297058: step 14280, loss = (G: 10.08161640, D: 0.33828318) (0.230 sec/batch)\n",
      "2017-04-05 21:27:49.794553: step 14300, loss = (G: 9.00254536, D: 0.11292830) (0.225 sec/batch)\n",
      "2017-04-05 21:27:54.454763: step 14320, loss = (G: 7.21344852, D: 0.01342449) (0.226 sec/batch)\n",
      "2017-04-05 21:27:58.944988: step 14340, loss = (G: 7.13850498, D: 0.16704170) (0.223 sec/batch)\n",
      "2017-04-05 21:28:03.483392: step 14360, loss = (G: 5.44166088, D: 0.06756075) (0.227 sec/batch)\n",
      "2017-04-05 21:28:07.984548: step 14380, loss = (G: 4.65481949, D: 0.08715767) (0.224 sec/batch)\n",
      "2017-04-05 21:28:12.529458: step 14400, loss = (G: 6.22253656, D: 0.05911935) (0.226 sec/batch)\n",
      "2017-04-05 21:28:17.147817: step 14420, loss = (G: 12.31253624, D: 0.06084662) (0.224 sec/batch)\n",
      "2017-04-05 21:28:21.652310: step 14440, loss = (G: 12.69882107, D: 0.02587112) (0.225 sec/batch)\n",
      "2017-04-05 21:28:26.154708: step 14460, loss = (G: 11.46160126, D: 0.60281283) (0.224 sec/batch)\n",
      "2017-04-05 21:28:30.653906: step 14480, loss = (G: 8.34863472, D: 0.01750528) (0.227 sec/batch)\n",
      "2017-04-05 21:28:35.155313: step 14500, loss = (G: 9.94895554, D: 0.00373269) (0.225 sec/batch)\n",
      "2017-04-05 21:28:39.800294: step 14520, loss = (G: 8.42646599, D: 0.02733780) (0.226 sec/batch)\n",
      "2017-04-05 21:28:44.301218: step 14540, loss = (G: 11.86626530, D: 0.11481702) (0.226 sec/batch)\n",
      "2017-04-05 21:28:48.820301: step 14560, loss = (G: 6.38988495, D: 0.06137834) (0.230 sec/batch)\n",
      "2017-04-05 21:28:53.331258: step 14580, loss = (G: 7.49755955, D: 0.18509957) (0.223 sec/batch)\n",
      "2017-04-05 21:28:57.838569: step 14600, loss = (G: 7.31809139, D: 0.09571468) (0.225 sec/batch)\n",
      "2017-04-05 21:29:02.475526: step 14620, loss = (G: 7.40761328, D: 0.03012183) (0.224 sec/batch)\n",
      "2017-04-05 21:29:06.995901: step 14640, loss = (G: 6.81823635, D: 0.04737869) (0.224 sec/batch)\n",
      "2017-04-05 21:29:11.489150: step 14660, loss = (G: 6.68684483, D: 0.17684446) (0.224 sec/batch)\n",
      "2017-04-05 21:29:16.007756: step 14680, loss = (G: 6.14016533, D: 0.36246651) (0.225 sec/batch)\n",
      "2017-04-05 21:29:20.504762: step 14700, loss = (G: 4.71553230, D: 0.14265677) (0.230 sec/batch)\n",
      "2017-04-05 21:29:25.160228: step 14720, loss = (G: 12.03111172, D: 0.18561269) (0.225 sec/batch)\n",
      "2017-04-05 21:29:29.656994: step 14740, loss = (G: 9.75537872, D: 0.00406967) (0.225 sec/batch)\n",
      "2017-04-05 21:29:34.155644: step 14760, loss = (G: 5.39647007, D: 0.04974844) (0.225 sec/batch)\n",
      "2017-04-05 21:29:38.667755: step 14780, loss = (G: 7.46739054, D: 0.04195621) (0.225 sec/batch)\n",
      "2017-04-05 21:29:43.183129: step 14800, loss = (G: 9.71927071, D: 0.00324908) (0.226 sec/batch)\n",
      "2017-04-05 21:29:47.826567: step 14820, loss = (G: 6.66443777, D: 0.05382666) (0.227 sec/batch)\n",
      "2017-04-05 21:29:52.339957: step 14840, loss = (G: 16.43797112, D: 0.00413893) (0.231 sec/batch)\n",
      "2017-04-05 21:29:56.887908: step 14860, loss = (G: 7.05785561, D: 0.03924216) (0.226 sec/batch)\n",
      "2017-04-05 21:30:01.386336: step 14880, loss = (G: 7.62724113, D: 0.01152490) (0.225 sec/batch)\n",
      "2017-04-05 21:30:05.891768: step 14900, loss = (G: 9.69143105, D: 0.12568343) (0.225 sec/batch)\n",
      "2017-04-05 21:30:10.563322: step 14920, loss = (G: 13.48252773, D: 0.09527253) (0.225 sec/batch)\n",
      "2017-04-05 21:30:15.074800: step 14940, loss = (G: 8.63328648, D: 0.00584065) (0.226 sec/batch)\n",
      "2017-04-05 21:30:19.570903: step 14960, loss = (G: 3.44314361, D: 0.32302067) (0.226 sec/batch)\n",
      "2017-04-05 21:30:24.087599: step 14980, loss = (G: 4.27838373, D: 0.21073495) (0.227 sec/batch)\n",
      "2017-04-05 21:30:28.594270: step 15000, loss = (G: 8.97626209, D: 0.02976552) (0.226 sec/batch)\n",
      "2017-04-05 21:30:33.239995: step 15020, loss = (G: 5.31043482, D: 0.16684654) (0.225 sec/batch)\n",
      "2017-04-05 21:30:37.757176: step 15040, loss = (G: 11.67722321, D: 0.00396990) (0.225 sec/batch)\n",
      "2017-04-05 21:30:42.404747: step 15060, loss = (G: 2.93344307, D: 0.34832466) (0.224 sec/batch)\n",
      "2017-04-05 21:30:46.919681: step 15080, loss = (G: 6.90294933, D: 0.06818244) (0.224 sec/batch)\n",
      "2017-04-05 21:30:51.424860: step 15100, loss = (G: 7.16281223, D: 0.02611946) (0.227 sec/batch)\n",
      "2017-04-05 21:30:56.067018: step 15120, loss = (G: 5.88411570, D: 0.03119434) (0.225 sec/batch)\n",
      "2017-04-05 21:31:00.582962: step 15140, loss = (G: 10.16887760, D: 0.01374409) (0.227 sec/batch)\n",
      "2017-04-05 21:31:05.101599: step 15160, loss = (G: 6.49259377, D: 0.05553919) (0.225 sec/batch)\n",
      "2017-04-05 21:31:09.612897: step 15180, loss = (G: 6.07906103, D: 0.03075832) (0.225 sec/batch)\n",
      "2017-04-05 21:31:14.148310: step 15200, loss = (G: 6.81790638, D: 0.03674133) (0.225 sec/batch)\n",
      "2017-04-05 21:31:18.786313: step 15220, loss = (G: 13.47919559, D: 0.02500310) (0.224 sec/batch)\n",
      "2017-04-05 21:31:23.297352: step 15240, loss = (G: 7.19882822, D: 0.12660776) (0.224 sec/batch)\n",
      "2017-04-05 21:31:27.797283: step 15260, loss = (G: 11.08122635, D: 0.15815631) (0.225 sec/batch)\n",
      "2017-04-05 21:31:32.303190: step 15280, loss = (G: 7.32012558, D: 0.05317912) (0.226 sec/batch)\n",
      "2017-04-05 21:31:36.823890: step 15300, loss = (G: 4.77382183, D: 0.08039211) (0.225 sec/batch)\n",
      "2017-04-05 21:31:41.460299: step 15320, loss = (G: 4.89828110, D: 0.10690311) (0.225 sec/batch)\n",
      "2017-04-05 21:31:45.976840: step 15340, loss = (G: 8.81701851, D: 0.04452607) (0.228 sec/batch)\n",
      "2017-04-05 21:31:50.483153: step 15360, loss = (G: 6.79727936, D: 0.04520303) (0.227 sec/batch)\n",
      "2017-04-05 21:31:54.993676: step 15380, loss = (G: 7.38538790, D: 0.01921055) (0.224 sec/batch)\n",
      "2017-04-05 21:31:59.525861: step 15400, loss = (G: 11.85808849, D: 0.00778470) (0.226 sec/batch)\n",
      "2017-04-05 21:32:04.168534: step 15420, loss = (G: 4.44917393, D: 0.20893954) (0.224 sec/batch)\n",
      "2017-04-05 21:32:08.672690: step 15440, loss = (G: 9.46376228, D: 0.01711182) (0.223 sec/batch)\n",
      "2017-04-05 21:32:13.180960: step 15460, loss = (G: 8.82545471, D: 0.01255967) (0.225 sec/batch)\n",
      "2017-04-05 21:32:17.696599: step 15480, loss = (G: 7.03194666, D: 0.01931061) (0.225 sec/batch)\n",
      "2017-04-05 21:32:22.211337: step 15500, loss = (G: 6.31802940, D: 0.03427771) (0.225 sec/batch)\n",
      "2017-04-05 21:32:26.852813: step 15520, loss = (G: 7.08914661, D: 0.02404083) (0.225 sec/batch)\n",
      "2017-04-05 21:32:31.357918: step 15540, loss = (G: 7.22780228, D: 0.07160712) (0.225 sec/batch)\n",
      "2017-04-05 21:32:35.880787: step 15560, loss = (G: 9.03297043, D: 0.00404686) (0.224 sec/batch)\n",
      "2017-04-05 21:32:40.390584: step 15580, loss = (G: 7.72157764, D: 0.01456484) (0.224 sec/batch)\n",
      "2017-04-05 21:32:44.894793: step 15600, loss = (G: 6.81746578, D: 0.03568655) (0.225 sec/batch)\n",
      "2017-04-05 21:32:49.546333: step 15620, loss = (G: 3.25446320, D: 0.36013204) (0.229 sec/batch)\n",
      "2017-04-05 21:32:54.058907: step 15640, loss = (G: 13.19563293, D: 0.28454801) (0.226 sec/batch)\n",
      "2017-04-05 21:32:58.569494: step 15660, loss = (G: 7.05581570, D: 0.01230344) (0.224 sec/batch)\n",
      "2017-04-05 21:33:03.080843: step 15680, loss = (G: 6.13824368, D: 0.06210727) (0.225 sec/batch)\n",
      "2017-04-05 21:33:07.621247: step 15700, loss = (G: 8.19557953, D: 0.02410083) (0.228 sec/batch)\n",
      "2017-04-05 21:33:12.268593: step 15720, loss = (G: 5.68047523, D: 0.16518362) (0.226 sec/batch)\n",
      "2017-04-05 21:33:16.787916: step 15740, loss = (G: 6.52690697, D: 0.05282308) (0.225 sec/batch)\n",
      "2017-04-05 21:33:21.324003: step 15760, loss = (G: 6.57252264, D: 0.02910031) (0.225 sec/batch)\n",
      "2017-04-05 21:33:25.823753: step 15780, loss = (G: 10.84230042, D: 0.01263295) (0.224 sec/batch)\n",
      "2017-04-05 21:33:30.334544: step 15800, loss = (G: 8.52045536, D: 0.61864471) (0.224 sec/batch)\n",
      "2017-04-05 21:33:34.984812: step 15820, loss = (G: 4.27530861, D: 0.23989023) (0.227 sec/batch)\n",
      "2017-04-05 21:33:39.521695: step 15840, loss = (G: 7.21250820, D: 0.02112583) (0.228 sec/batch)\n",
      "2017-04-05 21:33:44.018710: step 15860, loss = (G: 9.76707745, D: 0.03867904) (0.224 sec/batch)\n",
      "2017-04-05 21:33:48.549304: step 15880, loss = (G: 5.13580275, D: 0.07021261) (0.224 sec/batch)\n",
      "2017-04-05 21:33:53.061944: step 15900, loss = (G: 7.41965294, D: 0.01770375) (0.225 sec/batch)\n",
      "2017-04-05 21:33:57.733344: step 15920, loss = (G: 11.98081303, D: 0.16071036) (0.230 sec/batch)\n",
      "2017-04-05 21:34:02.251884: step 15940, loss = (G: 7.48922157, D: 0.11236443) (0.226 sec/batch)\n",
      "2017-04-05 21:34:06.748756: step 15960, loss = (G: 13.41406822, D: 0.06915592) (0.225 sec/batch)\n",
      "2017-04-05 21:34:11.244659: step 15980, loss = (G: 11.00773811, D: 0.17366390) (0.225 sec/batch)\n",
      "2017-04-05 21:34:15.754233: step 16000, loss = (G: 10.22105694, D: 0.02914351) (0.227 sec/batch)\n",
      "2017-04-05 21:34:20.401604: step 16020, loss = (G: 5.87200832, D: 0.04752221) (0.227 sec/batch)\n",
      "2017-04-05 21:34:24.906615: step 16040, loss = (G: 15.47364807, D: 0.06065886) (0.226 sec/batch)\n",
      "2017-04-05 21:34:29.425554: step 16060, loss = (G: 9.12638283, D: 0.00826683) (0.228 sec/batch)\n",
      "2017-04-05 21:34:33.945372: step 16080, loss = (G: 8.63262939, D: 0.04599373) (0.230 sec/batch)\n",
      "2017-04-05 21:34:38.477692: step 16100, loss = (G: 14.80106354, D: 0.01786005) (0.226 sec/batch)\n",
      "2017-04-05 21:34:43.136571: step 16120, loss = (G: 13.13758087, D: 0.12070210) (0.228 sec/batch)\n",
      "2017-04-05 21:34:47.644826: step 16140, loss = (G: 10.32191372, D: 0.01517623) (0.224 sec/batch)\n",
      "2017-04-05 21:34:52.159664: step 16160, loss = (G: 4.79580688, D: 0.21669085) (0.226 sec/batch)\n",
      "2017-04-05 21:34:56.682460: step 16180, loss = (G: 4.06751156, D: 0.31608009) (0.228 sec/batch)\n",
      "2017-04-05 21:35:01.187133: step 16200, loss = (G: 8.40859699, D: 0.00590757) (0.225 sec/batch)\n",
      "2017-04-05 21:35:05.847769: step 16220, loss = (G: 5.69016790, D: 0.06893679) (0.226 sec/batch)\n",
      "2017-04-05 21:35:10.387291: step 16240, loss = (G: 7.36122274, D: 0.01861997) (0.227 sec/batch)\n",
      "2017-04-05 21:35:14.875759: step 16260, loss = (G: 7.22552490, D: 0.03854887) (0.224 sec/batch)\n",
      "2017-04-05 21:35:19.389627: step 16280, loss = (G: 13.58929634, D: 0.27253836) (0.225 sec/batch)\n",
      "2017-04-05 21:35:23.885787: step 16300, loss = (G: 3.82612824, D: 0.39490661) (0.227 sec/batch)\n",
      "2017-04-05 21:35:28.536587: step 16320, loss = (G: 8.35100937, D: 0.05746350) (0.224 sec/batch)\n",
      "2017-04-05 21:35:33.035688: step 16340, loss = (G: 11.12324524, D: 0.37705335) (0.227 sec/batch)\n",
      "2017-04-05 21:35:37.540987: step 16360, loss = (G: 8.77665329, D: 0.00355513) (0.224 sec/batch)\n",
      "2017-04-05 21:35:42.050459: step 16380, loss = (G: 18.65618324, D: 2.57490420) (0.224 sec/batch)\n",
      "2017-04-05 21:35:46.552430: step 16400, loss = (G: 5.32263756, D: 0.10212870) (0.224 sec/batch)\n",
      "2017-04-05 21:35:51.185677: step 16420, loss = (G: 5.10983992, D: 0.12197561) (0.225 sec/batch)\n",
      "2017-04-05 21:35:55.696718: step 16440, loss = (G: 6.96601772, D: 0.02973214) (0.226 sec/batch)\n",
      "2017-04-05 21:36:00.204075: step 16460, loss = (G: 6.70934010, D: 0.04174000) (0.228 sec/batch)\n",
      "2017-04-05 21:36:04.707556: step 16480, loss = (G: 5.47881269, D: 0.04566218) (0.225 sec/batch)\n",
      "2017-04-05 21:36:09.218210: step 16500, loss = (G: 8.92507458, D: 0.01200542) (0.225 sec/batch)\n",
      "2017-04-05 21:36:13.864785: step 16520, loss = (G: 3.22813892, D: 0.72492927) (0.225 sec/batch)\n",
      "2017-04-05 21:36:18.378059: step 16540, loss = (G: 8.57302284, D: 0.09516354) (0.226 sec/batch)\n",
      "2017-04-05 21:36:22.881354: step 16560, loss = (G: 7.44555950, D: 0.01240885) (0.225 sec/batch)\n",
      "2017-04-05 21:36:27.392701: step 16580, loss = (G: 9.41642094, D: 0.17237978) (0.225 sec/batch)\n",
      "2017-04-05 21:36:31.900188: step 16600, loss = (G: 5.91578913, D: 0.12378370) (0.224 sec/batch)\n",
      "2017-04-05 21:36:36.583241: step 16620, loss = (G: 5.26443720, D: 0.06042711) (0.226 sec/batch)\n",
      "2017-04-05 21:36:41.089069: step 16640, loss = (G: 5.70380211, D: 0.04249266) (0.226 sec/batch)\n",
      "2017-04-05 21:36:45.592675: step 16660, loss = (G: 7.29325676, D: 0.01944618) (0.225 sec/batch)\n",
      "2017-04-05 21:36:50.107679: step 16680, loss = (G: 9.36786938, D: 0.01719298) (0.225 sec/batch)\n",
      "2017-04-05 21:36:54.618039: step 16700, loss = (G: 7.88304901, D: 0.01771229) (0.224 sec/batch)\n",
      "2017-04-05 21:36:59.265899: step 16720, loss = (G: 8.21332550, D: 0.06978374) (0.224 sec/batch)\n",
      "2017-04-05 21:37:03.815880: step 16740, loss = (G: 5.95902634, D: 0.11143242) (0.225 sec/batch)\n",
      "2017-04-05 21:37:08.319602: step 16760, loss = (G: 10.47651386, D: 0.17572826) (0.224 sec/batch)\n",
      "2017-04-05 21:37:12.825965: step 16780, loss = (G: 4.86531448, D: 0.09425507) (0.226 sec/batch)\n",
      "2017-04-05 21:37:17.335982: step 16800, loss = (G: 8.58264160, D: 0.05823937) (0.226 sec/batch)\n",
      "2017-04-05 21:37:21.986915: step 16820, loss = (G: 14.97039223, D: 0.00643672) (0.226 sec/batch)\n",
      "2017-04-05 21:37:26.493399: step 16840, loss = (G: 4.42421436, D: 0.16218820) (0.226 sec/batch)\n",
      "2017-04-05 21:37:31.000180: step 16860, loss = (G: 6.83352518, D: 0.02989455) (0.225 sec/batch)\n",
      "2017-04-05 21:37:35.544916: step 16880, loss = (G: 9.40039825, D: 0.01629363) (0.230 sec/batch)\n",
      "2017-04-05 21:37:40.048220: step 16900, loss = (G: 8.58553600, D: 0.00785440) (0.226 sec/batch)\n",
      "2017-04-05 21:37:44.711660: step 16920, loss = (G: 5.89486980, D: 0.06277798) (0.229 sec/batch)\n",
      "2017-04-05 21:37:49.223769: step 16940, loss = (G: 7.29720783, D: 0.06543440) (0.225 sec/batch)\n",
      "2017-04-05 21:37:53.727373: step 16960, loss = (G: 5.51734352, D: 0.10007358) (0.224 sec/batch)\n",
      "2017-04-05 21:37:58.277930: step 16980, loss = (G: 12.20669651, D: 0.62991148) (0.224 sec/batch)\n",
      "2017-04-05 21:38:02.823972: step 17000, loss = (G: 6.96012402, D: 0.02251217) (0.226 sec/batch)\n",
      "2017-04-05 21:38:07.464845: step 17020, loss = (G: 8.54804993, D: 0.11424720) (0.225 sec/batch)\n",
      "2017-04-05 21:38:11.965030: step 17040, loss = (G: 4.88437939, D: 0.11660100) (0.227 sec/batch)\n",
      "2017-04-05 21:38:16.469499: step 17060, loss = (G: 10.28035450, D: 0.04916219) (0.226 sec/batch)\n",
      "2017-04-05 21:38:20.982465: step 17080, loss = (G: 7.89344549, D: 0.02598090) (0.226 sec/batch)\n",
      "2017-04-05 21:38:25.487888: step 17100, loss = (G: 3.90182495, D: 0.19229765) (0.224 sec/batch)\n",
      "2017-04-05 21:38:30.140371: step 17120, loss = (G: 5.37880611, D: 0.06127618) (0.226 sec/batch)\n",
      "2017-04-05 21:38:34.657652: step 17140, loss = (G: 7.78155518, D: 0.01405934) (0.226 sec/batch)\n",
      "2017-04-05 21:38:39.167873: step 17160, loss = (G: 7.80491686, D: 0.01014388) (0.224 sec/batch)\n",
      "2017-04-05 21:38:43.679340: step 17180, loss = (G: 6.09071016, D: 0.05850546) (0.227 sec/batch)\n",
      "2017-04-05 21:38:48.181708: step 17200, loss = (G: 17.67919731, D: 3.23154926) (0.225 sec/batch)\n",
      "2017-04-05 21:38:52.850787: step 17220, loss = (G: 6.96672773, D: 0.02274200) (0.228 sec/batch)\n",
      "2017-04-05 21:38:57.361120: step 17240, loss = (G: 9.16005421, D: 0.00591546) (0.226 sec/batch)\n",
      "2017-04-05 21:39:01.872002: step 17260, loss = (G: 13.79720116, D: 0.01089976) (0.227 sec/batch)\n",
      "2017-04-05 21:39:06.379577: step 17280, loss = (G: 4.88678265, D: 0.12874466) (0.228 sec/batch)\n",
      "2017-04-05 21:39:10.879392: step 17300, loss = (G: 6.14657354, D: 0.05087924) (0.226 sec/batch)\n",
      "2017-04-05 21:39:15.527181: step 17320, loss = (G: 5.79951286, D: 0.06941675) (0.223 sec/batch)\n",
      "2017-04-05 21:39:20.032182: step 17340, loss = (G: 11.88717937, D: 0.00770084) (0.225 sec/batch)\n",
      "2017-04-05 21:39:24.533698: step 17360, loss = (G: 4.76346302, D: 0.10267338) (0.228 sec/batch)\n",
      "2017-04-05 21:39:29.201977: step 17380, loss = (G: 6.38900185, D: 0.02524753) (0.225 sec/batch)\n",
      "2017-04-05 21:39:33.701209: step 17400, loss = (G: 3.71873903, D: 0.39477825) (0.225 sec/batch)\n",
      "2017-04-05 21:39:38.393249: step 17420, loss = (G: 6.12529373, D: 0.10748865) (0.225 sec/batch)\n",
      "2017-04-05 21:39:42.919876: step 17440, loss = (G: 6.26740456, D: 0.02518155) (0.223 sec/batch)\n",
      "2017-04-05 21:39:47.441950: step 17460, loss = (G: 7.15037346, D: 0.03975169) (0.224 sec/batch)\n",
      "2017-04-05 21:39:51.942452: step 17480, loss = (G: 10.75277710, D: 0.00533205) (0.224 sec/batch)\n",
      "2017-04-05 21:39:56.441323: step 17500, loss = (G: 7.11700821, D: 0.03437519) (0.225 sec/batch)\n",
      "2017-04-05 21:40:01.082856: step 17520, loss = (G: 6.30040836, D: 0.04219796) (0.224 sec/batch)\n",
      "2017-04-05 21:40:05.613203: step 17540, loss = (G: 4.78002548, D: 0.10053013) (0.226 sec/batch)\n",
      "2017-04-05 21:40:10.116063: step 17560, loss = (G: 7.00578690, D: 0.02416243) (0.224 sec/batch)\n",
      "2017-04-05 21:40:14.613759: step 17580, loss = (G: 6.02075052, D: 0.04259258) (0.226 sec/batch)\n",
      "2017-04-05 21:40:19.112590: step 17600, loss = (G: 5.59654951, D: 0.06677371) (0.224 sec/batch)\n",
      "2017-04-05 21:40:23.763467: step 17620, loss = (G: 9.22175980, D: 0.03619114) (0.229 sec/batch)\n",
      "2017-04-05 21:40:28.265061: step 17640, loss = (G: 5.10143280, D: 0.06868994) (0.227 sec/batch)\n",
      "2017-04-05 21:40:32.779633: step 17660, loss = (G: 3.36530018, D: 0.26361877) (0.227 sec/batch)\n",
      "2017-04-05 21:40:37.283868: step 17680, loss = (G: 7.66833019, D: 0.00673745) (0.226 sec/batch)\n",
      "2017-04-05 21:40:41.832337: step 17700, loss = (G: 8.02234173, D: 0.45786670) (0.224 sec/batch)\n",
      "2017-04-05 21:40:46.481548: step 17720, loss = (G: 2.69228768, D: 0.84121174) (0.224 sec/batch)\n",
      "2017-04-05 21:40:50.975048: step 17740, loss = (G: 8.52851486, D: 0.01515435) (0.225 sec/batch)\n",
      "2017-04-05 21:40:55.468220: step 17760, loss = (G: 4.29153776, D: 0.16721737) (0.224 sec/batch)\n",
      "2017-04-05 21:40:59.978786: step 17780, loss = (G: 4.09955692, D: 0.17194164) (0.225 sec/batch)\n",
      "2017-04-05 21:41:04.503945: step 17800, loss = (G: 8.39358521, D: 0.02328592) (0.229 sec/batch)\n",
      "2017-04-05 21:41:09.154169: step 17820, loss = (G: 6.98906994, D: 0.02411418) (0.224 sec/batch)\n",
      "2017-04-05 21:41:13.663095: step 17840, loss = (G: 8.84323597, D: 0.03164940) (0.230 sec/batch)\n",
      "2017-04-05 21:41:18.175537: step 17860, loss = (G: 4.08520699, D: 0.12547363) (0.228 sec/batch)\n",
      "2017-04-05 21:41:22.673640: step 17880, loss = (G: 10.62569237, D: 0.00425253) (0.224 sec/batch)\n",
      "2017-04-05 21:41:27.183375: step 17900, loss = (G: 4.50263977, D: 0.24188991) (0.226 sec/batch)\n",
      "2017-04-05 21:41:31.833265: step 17920, loss = (G: 5.54408264, D: 0.04797345) (0.225 sec/batch)\n",
      "2017-04-05 21:41:36.344655: step 17940, loss = (G: 9.25270081, D: 0.11386858) (0.226 sec/batch)\n",
      "2017-04-05 21:41:40.861309: step 17960, loss = (G: 3.73056006, D: 0.39133504) (0.225 sec/batch)\n",
      "2017-04-05 21:41:45.369209: step 17980, loss = (G: 14.46381378, D: 0.07786804) (0.227 sec/batch)\n",
      "2017-04-05 21:41:49.873057: step 18000, loss = (G: 6.12960243, D: 0.04773325) (0.226 sec/batch)\n",
      "2017-04-05 21:41:54.516246: step 18020, loss = (G: 11.16872692, D: 0.00197596) (0.225 sec/batch)\n",
      "2017-04-05 21:41:59.037854: step 18040, loss = (G: 7.42971325, D: 0.01003737) (0.225 sec/batch)\n",
      "2017-04-05 21:42:03.526112: step 18060, loss = (G: 7.57458448, D: 0.10175762) (0.226 sec/batch)\n",
      "2017-04-05 21:42:08.030466: step 18080, loss = (G: 12.92285347, D: 0.01391398) (0.227 sec/batch)\n",
      "2017-04-05 21:42:12.536856: step 18100, loss = (G: 7.05096245, D: 0.02777793) (0.226 sec/batch)\n",
      "2017-04-05 21:42:17.184922: step 18120, loss = (G: 6.09761477, D: 0.03026026) (0.224 sec/batch)\n",
      "2017-04-05 21:42:21.699708: step 18140, loss = (G: 8.50490379, D: 0.00850114) (0.225 sec/batch)\n",
      "2017-04-05 21:42:26.224741: step 18160, loss = (G: 9.55257034, D: 0.00781881) (0.223 sec/batch)\n",
      "2017-04-05 21:42:30.724748: step 18180, loss = (G: 8.08188534, D: 0.30721912) (0.227 sec/batch)\n",
      "2017-04-05 21:42:35.235648: step 18200, loss = (G: 6.50535107, D: 0.03623443) (0.226 sec/batch)\n",
      "2017-04-05 21:42:39.917211: step 18220, loss = (G: 6.98809052, D: 0.01992154) (0.231 sec/batch)\n",
      "2017-04-05 21:42:44.433068: step 18240, loss = (G: 10.15879536, D: 0.00210133) (0.226 sec/batch)\n",
      "2017-04-05 21:42:48.950788: step 18260, loss = (G: 7.36139202, D: 0.19488378) (0.225 sec/batch)\n",
      "2017-04-05 21:42:53.457972: step 18280, loss = (G: 8.96309757, D: 0.00828431) (0.225 sec/batch)\n",
      "2017-04-05 21:42:57.983368: step 18300, loss = (G: 7.63968945, D: 0.04010136) (0.225 sec/batch)\n",
      "2017-04-05 21:43:02.636530: step 18320, loss = (G: 7.33206177, D: 0.16936894) (0.226 sec/batch)\n",
      "2017-04-05 21:43:07.142227: step 18340, loss = (G: 9.32302570, D: 0.01252024) (0.224 sec/batch)\n",
      "2017-04-05 21:43:11.652870: step 18360, loss = (G: 5.03966188, D: 0.07982419) (0.225 sec/batch)\n",
      "2017-04-05 21:43:16.155428: step 18380, loss = (G: 6.28547430, D: 0.11900955) (0.226 sec/batch)\n",
      "2017-04-05 21:43:20.657322: step 18400, loss = (G: 13.78671455, D: 0.62381858) (0.225 sec/batch)\n",
      "2017-04-05 21:43:25.301944: step 18420, loss = (G: 10.45890331, D: 0.00849954) (0.226 sec/batch)\n",
      "2017-04-05 21:43:29.820331: step 18440, loss = (G: 5.99690247, D: 0.05053394) (0.225 sec/batch)\n",
      "2017-04-05 21:43:34.324172: step 18460, loss = (G: 4.64158630, D: 0.14885467) (0.225 sec/batch)\n",
      "2017-04-05 21:43:38.822961: step 18480, loss = (G: 5.32027483, D: 0.07886094) (0.225 sec/batch)\n",
      "2017-04-05 21:43:43.336734: step 18500, loss = (G: 6.13557148, D: 0.08231985) (0.225 sec/batch)\n",
      "2017-04-05 21:43:47.991943: step 18520, loss = (G: 6.65622044, D: 0.01809913) (0.227 sec/batch)\n",
      "2017-04-05 21:43:52.640314: step 18540, loss = (G: 7.09979677, D: 0.02150228) (0.224 sec/batch)\n",
      "2017-04-05 21:43:57.149903: step 18560, loss = (G: 3.76697922, D: 0.23728316) (0.225 sec/batch)\n",
      "2017-04-05 21:44:01.653156: step 18580, loss = (G: 6.94408035, D: 0.04207730) (0.225 sec/batch)\n",
      "2017-04-05 21:44:06.174773: step 18600, loss = (G: 8.36390305, D: 0.00859287) (0.228 sec/batch)\n",
      "2017-04-05 21:44:10.826393: step 18620, loss = (G: 9.49523640, D: 0.03010179) (0.224 sec/batch)\n",
      "2017-04-05 21:44:15.328943: step 18640, loss = (G: 6.64684677, D: 0.05613973) (0.225 sec/batch)\n",
      "2017-04-05 21:44:19.840411: step 18660, loss = (G: 5.54519749, D: 0.12193243) (0.225 sec/batch)\n",
      "2017-04-05 21:44:24.349936: step 18680, loss = (G: 2.78977942, D: 1.27249467) (0.225 sec/batch)\n",
      "2017-04-05 21:44:28.854804: step 18700, loss = (G: 5.64834929, D: 0.05506542) (0.225 sec/batch)\n",
      "2017-04-05 21:44:33.496219: step 18720, loss = (G: 6.73411083, D: 0.11896098) (0.225 sec/batch)\n",
      "2017-04-05 21:44:38.048179: step 18740, loss = (G: 3.28301239, D: 0.43565363) (0.225 sec/batch)\n",
      "2017-04-05 21:44:42.560758: step 18760, loss = (G: 8.43699741, D: 0.04901757) (0.224 sec/batch)\n",
      "2017-04-05 21:44:47.075372: step 18780, loss = (G: 10.18173504, D: 0.24928674) (0.225 sec/batch)\n",
      "2017-04-05 21:44:51.593945: step 18800, loss = (G: 3.66847491, D: 0.17659336) (0.226 sec/batch)\n",
      "2017-04-05 21:44:56.229544: step 18820, loss = (G: 5.75412321, D: 0.03656211) (0.225 sec/batch)\n",
      "2017-04-05 21:45:00.750145: step 18840, loss = (G: 5.73176432, D: 0.08405286) (0.224 sec/batch)\n",
      "2017-04-05 21:45:05.256789: step 18860, loss = (G: 5.06428576, D: 0.10250138) (0.225 sec/batch)\n",
      "2017-04-05 21:45:09.761363: step 18880, loss = (G: 4.72963142, D: 0.06498696) (0.225 sec/batch)\n",
      "2017-04-05 21:45:14.265121: step 18900, loss = (G: 6.17079782, D: 0.04964582) (0.227 sec/batch)\n",
      "2017-04-05 21:45:18.917957: step 18920, loss = (G: 3.69266844, D: 0.36361760) (0.225 sec/batch)\n",
      "2017-04-05 21:45:23.432764: step 18940, loss = (G: 12.62950516, D: 0.01080057) (0.226 sec/batch)\n",
      "2017-04-05 21:45:27.938912: step 18960, loss = (G: 12.23040485, D: 0.00359627) (0.227 sec/batch)\n",
      "2017-04-05 21:45:32.450629: step 18980, loss = (G: 6.95355606, D: 0.05651448) (0.224 sec/batch)\n",
      "2017-04-05 21:45:36.963865: step 19000, loss = (G: 5.13937473, D: 0.07684344) (0.225 sec/batch)\n",
      "2017-04-05 21:45:41.615090: step 19020, loss = (G: 7.40998554, D: 0.01890920) (0.230 sec/batch)\n",
      "2017-04-05 21:45:46.137163: step 19040, loss = (G: 14.20484352, D: 1.05748737) (0.228 sec/batch)\n",
      "2017-04-05 21:45:50.650165: step 19060, loss = (G: 10.17008972, D: 0.13104962) (0.228 sec/batch)\n",
      "2017-04-05 21:45:55.145093: step 19080, loss = (G: 5.91878319, D: 0.05705162) (0.226 sec/batch)\n",
      "2017-04-05 21:45:59.646348: step 19100, loss = (G: 3.06462479, D: 0.69852483) (0.224 sec/batch)\n",
      "2017-04-05 21:46:04.290315: step 19120, loss = (G: 7.74890518, D: 0.03870766) (0.226 sec/batch)\n",
      "2017-04-05 21:46:08.795310: step 19140, loss = (G: 7.52676773, D: 0.02321763) (0.224 sec/batch)\n",
      "2017-04-05 21:46:13.311579: step 19160, loss = (G: 9.81080055, D: 0.27379280) (0.225 sec/batch)\n",
      "2017-04-05 21:46:17.814378: step 19180, loss = (G: 5.72605801, D: 0.03201406) (0.225 sec/batch)\n",
      "2017-04-05 21:46:22.322768: step 19200, loss = (G: 6.07506371, D: 0.03373534) (0.226 sec/batch)\n",
      "2017-04-05 21:46:26.969606: step 19220, loss = (G: 6.32988453, D: 0.06209048) (0.225 sec/batch)\n",
      "2017-04-05 21:46:31.490040: step 19240, loss = (G: 3.79242754, D: 0.30223379) (0.226 sec/batch)\n",
      "2017-04-05 21:46:35.982508: step 19260, loss = (G: 5.18022251, D: 0.06964400) (0.225 sec/batch)\n",
      "2017-04-05 21:46:40.482415: step 19280, loss = (G: 7.26452923, D: 0.05505922) (0.224 sec/batch)\n",
      "2017-04-05 21:46:45.001867: step 19300, loss = (G: 5.02258587, D: 0.08758104) (0.227 sec/batch)\n",
      "2017-04-05 21:46:49.674675: step 19320, loss = (G: 5.25510883, D: 0.11937764) (0.228 sec/batch)\n",
      "2017-04-05 21:46:54.176083: step 19340, loss = (G: 5.95472240, D: 0.11633302) (0.225 sec/batch)\n",
      "2017-04-05 21:46:58.672732: step 19360, loss = (G: 14.65662384, D: 0.29736733) (0.226 sec/batch)\n",
      "2017-04-05 21:47:03.183238: step 19380, loss = (G: 5.68042946, D: 0.18494637) (0.227 sec/batch)\n",
      "2017-04-05 21:47:07.697475: step 19400, loss = (G: 5.58668041, D: 0.07933480) (0.226 sec/batch)\n",
      "2017-04-05 21:47:12.351170: step 19420, loss = (G: 4.93681908, D: 0.09646761) (0.225 sec/batch)\n",
      "2017-04-05 21:47:16.861582: step 19440, loss = (G: 3.66377640, D: 0.17787597) (0.225 sec/batch)\n",
      "2017-04-05 21:47:21.378483: step 19460, loss = (G: 10.72285080, D: 0.01264475) (0.226 sec/batch)\n",
      "2017-04-05 21:47:25.875679: step 19480, loss = (G: 6.35355663, D: 0.01649094) (0.225 sec/batch)\n",
      "2017-04-05 21:47:30.371875: step 19500, loss = (G: 5.73565006, D: 0.02694090) (0.224 sec/batch)\n",
      "2017-04-05 21:47:35.037918: step 19520, loss = (G: 10.67713356, D: 0.08740285) (0.225 sec/batch)\n",
      "2017-04-05 21:47:39.546857: step 19540, loss = (G: 5.64915752, D: 0.04499481) (0.226 sec/batch)\n",
      "2017-04-05 21:47:44.055249: step 19560, loss = (G: 6.76593065, D: 0.03525209) (0.224 sec/batch)\n",
      "2017-04-05 21:47:48.554404: step 19580, loss = (G: 4.95717621, D: 0.05700785) (0.225 sec/batch)\n",
      "2017-04-05 21:47:53.075528: step 19600, loss = (G: 7.14607334, D: 0.06785922) (0.225 sec/batch)\n",
      "2017-04-05 21:47:57.725963: step 19620, loss = (G: 4.97731209, D: 0.08866648) (0.224 sec/batch)\n",
      "2017-04-05 21:48:02.226122: step 19640, loss = (G: 6.31668806, D: 0.02857196) (0.225 sec/batch)\n",
      "2017-04-05 21:48:06.728144: step 19660, loss = (G: 14.18614483, D: 0.27161142) (0.224 sec/batch)\n",
      "2017-04-05 21:48:11.239017: step 19680, loss = (G: 9.11150742, D: 0.00679469) (0.225 sec/batch)\n",
      "2017-04-05 21:48:15.867966: step 19700, loss = (G: 5.94109344, D: 0.03713308) (0.225 sec/batch)\n",
      "2017-04-05 21:48:20.518662: step 19720, loss = (G: 3.55255651, D: 0.22103459) (0.225 sec/batch)\n",
      "2017-04-05 21:48:25.024362: step 19740, loss = (G: 2.15673780, D: 3.24103379) (0.224 sec/batch)\n",
      "2017-04-05 21:48:29.515355: step 19760, loss = (G: 5.51464367, D: 0.13077298) (0.225 sec/batch)\n",
      "2017-04-05 21:48:34.024369: step 19780, loss = (G: 20.27283859, D: 0.00753409) (0.225 sec/batch)\n",
      "2017-04-05 21:48:38.518153: step 19800, loss = (G: 4.97519779, D: 0.10862388) (0.226 sec/batch)\n",
      "2017-04-05 21:48:43.180812: step 19820, loss = (G: 7.57506895, D: 0.01677312) (0.230 sec/batch)\n",
      "2017-04-05 21:48:47.691713: step 19840, loss = (G: 7.40032530, D: 0.03065983) (0.225 sec/batch)\n",
      "2017-04-05 21:48:52.188174: step 19860, loss = (G: 6.98036957, D: 0.36034012) (0.226 sec/batch)\n",
      "2017-04-05 21:48:56.685675: step 19880, loss = (G: 7.97882748, D: 0.02838965) (0.226 sec/batch)\n",
      "2017-04-05 21:49:01.201072: step 19900, loss = (G: 6.99446583, D: 0.06261153) (0.225 sec/batch)\n",
      "2017-04-05 21:49:05.839162: step 19920, loss = (G: 7.07190514, D: 0.01597147) (0.226 sec/batch)\n",
      "2017-04-05 21:49:10.356570: step 19940, loss = (G: 6.74943352, D: 0.01657298) (0.225 sec/batch)\n",
      "2017-04-05 21:49:14.864202: step 19960, loss = (G: 5.49374914, D: 0.08935223) (0.225 sec/batch)\n",
      "2017-04-05 21:49:19.377006: step 19980, loss = (G: 6.31721783, D: 0.01653168) (0.226 sec/batch)\n",
      "2017-04-05 21:49:23.886983: step 20000, loss = (G: 7.18454361, D: 0.06213772) (0.225 sec/batch)\n",
      "2017-04-05 21:49:28.542037: step 20020, loss = (G: 5.82416010, D: 0.05465970) (0.224 sec/batch)\n",
      "2017-04-05 21:49:33.045034: step 20040, loss = (G: 5.32577229, D: 0.05682638) (0.225 sec/batch)\n",
      "2017-04-05 21:49:37.553641: step 20060, loss = (G: 6.16195202, D: 0.03846645) (0.227 sec/batch)\n",
      "2017-04-05 21:49:42.067967: step 20080, loss = (G: 7.22110796, D: 0.01481221) (0.225 sec/batch)\n",
      "2017-04-05 21:49:46.599866: step 20100, loss = (G: 5.69411469, D: 0.03383619) (0.231 sec/batch)\n",
      "2017-04-05 21:49:51.255234: step 20120, loss = (G: 4.38794994, D: 0.20738621) (0.228 sec/batch)\n",
      "2017-04-05 21:49:55.768317: step 20140, loss = (G: 9.78084660, D: 0.00991021) (0.230 sec/batch)\n",
      "2017-04-05 21:50:00.276420: step 20160, loss = (G: 6.53701782, D: 0.02685035) (0.225 sec/batch)\n",
      "2017-04-05 21:50:04.783140: step 20180, loss = (G: 6.46806908, D: 0.65694487) (0.224 sec/batch)\n",
      "2017-04-05 21:50:09.273777: step 20200, loss = (G: 6.29552984, D: 0.03269058) (0.225 sec/batch)\n",
      "2017-04-05 21:50:13.930053: step 20220, loss = (G: 6.33815813, D: 0.02280964) (0.224 sec/batch)\n",
      "2017-04-05 21:50:18.430203: step 20240, loss = (G: 5.13213921, D: 0.04608734) (0.226 sec/batch)\n",
      "2017-04-05 21:50:22.937335: step 20260, loss = (G: 5.29614401, D: 0.07335117) (0.225 sec/batch)\n",
      "2017-04-05 21:50:27.431851: step 20280, loss = (G: 10.02659798, D: 0.23736541) (0.225 sec/batch)\n",
      "2017-04-05 21:50:31.947487: step 20300, loss = (G: 6.03461933, D: 0.06268936) (0.226 sec/batch)\n",
      "2017-04-05 21:50:36.603932: step 20320, loss = (G: 2.57507229, D: 1.00709891) (0.225 sec/batch)\n",
      "2017-04-05 21:50:41.106135: step 20340, loss = (G: 5.20526695, D: 0.05779881) (0.225 sec/batch)\n",
      "2017-04-05 21:50:45.623686: step 20360, loss = (G: 5.33240080, D: 0.05527927) (0.227 sec/batch)\n",
      "2017-04-05 21:50:50.134490: step 20380, loss = (G: 6.74050045, D: 0.03108525) (0.226 sec/batch)\n",
      "2017-04-05 21:50:54.624544: step 20400, loss = (G: 10.78658009, D: 0.04222299) (0.224 sec/batch)\n",
      "2017-04-05 21:50:59.280801: step 20420, loss = (G: 6.80080128, D: 0.01462063) (0.230 sec/batch)\n",
      "2017-04-05 21:51:03.807764: step 20440, loss = (G: 3.67035127, D: 0.87349916) (0.225 sec/batch)\n",
      "2017-04-05 21:51:08.318842: step 20460, loss = (G: 4.70493460, D: 0.16299756) (0.224 sec/batch)\n",
      "2017-04-05 21:51:12.845140: step 20480, loss = (G: 5.60342884, D: 0.12866861) (0.224 sec/batch)\n",
      "2017-04-05 21:51:17.341165: step 20500, loss = (G: 5.76290512, D: 0.18957475) (0.224 sec/batch)\n",
      "2017-04-05 21:51:22.001653: step 20520, loss = (G: 5.10711002, D: 0.07253081) (0.225 sec/batch)\n",
      "2017-04-05 21:51:26.496637: step 20540, loss = (G: 9.81021404, D: 0.01486172) (0.225 sec/batch)\n",
      "2017-04-05 21:51:31.026618: step 20560, loss = (G: 6.83381176, D: 0.06278758) (0.231 sec/batch)\n",
      "2017-04-05 21:51:35.542475: step 20580, loss = (G: 2.37822342, D: 0.61742193) (0.226 sec/batch)\n",
      "2017-04-05 21:51:40.036227: step 20600, loss = (G: 11.72493362, D: 0.06177242) (0.225 sec/batch)\n",
      "2017-04-05 21:51:44.675079: step 20620, loss = (G: 6.63479996, D: 0.04528331) (0.225 sec/batch)\n",
      "2017-04-05 21:51:49.175147: step 20640, loss = (G: 6.11829233, D: 0.02091998) (0.227 sec/batch)\n",
      "2017-04-05 21:51:53.671957: step 20660, loss = (G: 8.22620010, D: 0.13564798) (0.224 sec/batch)\n",
      "2017-04-05 21:51:58.182848: step 20680, loss = (G: 10.43818665, D: 0.01093092) (0.226 sec/batch)\n",
      "2017-04-05 21:52:02.726826: step 20700, loss = (G: 11.29576492, D: 0.52060544) (0.224 sec/batch)\n",
      "2017-04-05 21:52:07.364113: step 20720, loss = (G: 6.02813625, D: 0.02169263) (0.225 sec/batch)\n",
      "2017-04-05 21:52:11.872925: step 20740, loss = (G: 5.33699799, D: 0.05036807) (0.225 sec/batch)\n",
      "2017-04-05 21:52:16.364701: step 20760, loss = (G: 6.80035305, D: 0.03467846) (0.224 sec/batch)\n",
      "2017-04-05 21:52:20.906412: step 20780, loss = (G: 5.99345732, D: 0.03270176) (0.224 sec/batch)\n",
      "2017-04-05 21:52:25.396565: step 20800, loss = (G: 8.03982925, D: 0.02034692) (0.230 sec/batch)\n",
      "2017-04-05 21:52:30.049239: step 20820, loss = (G: 4.25390291, D: 0.23106018) (0.225 sec/batch)\n",
      "2017-04-05 21:52:34.544658: step 20840, loss = (G: 6.10324287, D: 0.12213361) (0.225 sec/batch)\n",
      "2017-04-05 21:52:39.198923: step 20860, loss = (G: 10.37329006, D: 0.00204821) (0.225 sec/batch)\n",
      "2017-04-05 21:52:43.688720: step 20880, loss = (G: 7.56377029, D: 0.03999781) (0.225 sec/batch)\n",
      "2017-04-05 21:52:48.200625: step 20900, loss = (G: 7.95261908, D: 0.03187615) (0.231 sec/batch)\n",
      "2017-04-05 21:52:52.871025: step 20920, loss = (G: 4.45104599, D: 0.11073048) (0.224 sec/batch)\n",
      "2017-04-05 21:52:57.358374: step 20940, loss = (G: 8.45950127, D: 0.10679118) (0.225 sec/batch)\n",
      "2017-04-05 21:53:01.867749: step 20960, loss = (G: 8.41931152, D: 0.03783227) (0.226 sec/batch)\n",
      "2017-04-05 21:53:06.367191: step 20980, loss = (G: 7.41594982, D: 0.05405628) (0.225 sec/batch)\n",
      "2017-04-05 21:53:10.868088: step 21000, loss = (G: 7.86874199, D: 0.02034208) (0.225 sec/batch)\n",
      "2017-04-05 21:53:15.510858: step 21020, loss = (G: 7.84221506, D: 0.02836161) (0.225 sec/batch)\n",
      "2017-04-05 21:53:20.011538: step 21040, loss = (G: 6.15847588, D: 0.07855672) (0.226 sec/batch)\n",
      "2017-04-05 21:53:24.515598: step 21060, loss = (G: 4.45451641, D: 0.11949035) (0.225 sec/batch)\n",
      "2017-04-05 21:53:29.033106: step 21080, loss = (G: 4.40760946, D: 0.20728143) (0.224 sec/batch)\n",
      "2017-04-05 21:53:33.552059: step 21100, loss = (G: 3.65789270, D: 0.41721687) (0.227 sec/batch)\n",
      "2017-04-05 21:53:38.203172: step 21120, loss = (G: 4.97733545, D: 0.15336508) (0.226 sec/batch)\n",
      "2017-04-05 21:53:42.708259: step 21140, loss = (G: 6.16303253, D: 0.04928043) (0.224 sec/batch)\n",
      "2017-04-05 21:53:47.214227: step 21160, loss = (G: 9.19139290, D: 0.11008543) (0.224 sec/batch)\n",
      "2017-04-05 21:53:51.734569: step 21180, loss = (G: 9.13263512, D: 0.01431424) (0.229 sec/batch)\n",
      "2017-04-05 21:53:56.232367: step 21200, loss = (G: 6.52108049, D: 0.08946563) (0.225 sec/batch)\n",
      "2017-04-05 21:54:00.870527: step 21220, loss = (G: 9.96710110, D: 0.01014246) (0.225 sec/batch)\n",
      "2017-04-05 21:54:05.382890: step 21240, loss = (G: 5.91839457, D: 0.06449023) (0.226 sec/batch)\n",
      "2017-04-05 21:54:09.880101: step 21260, loss = (G: 7.99977779, D: 0.09922677) (0.225 sec/batch)\n",
      "2017-04-05 21:54:14.392021: step 21280, loss = (G: 4.46012306, D: 0.06493398) (0.228 sec/batch)\n",
      "2017-04-05 21:54:18.892668: step 21300, loss = (G: 9.00739479, D: 0.00630606) (0.225 sec/batch)\n",
      "2017-04-05 21:54:23.531935: step 21320, loss = (G: 6.79659653, D: 0.04228554) (0.224 sec/batch)\n",
      "2017-04-05 21:54:28.042813: step 21340, loss = (G: 11.40923786, D: 0.00121762) (0.225 sec/batch)\n",
      "2017-04-05 21:54:32.538689: step 21360, loss = (G: 3.79261875, D: 0.31353787) (0.225 sec/batch)\n",
      "2017-04-05 21:54:37.042315: step 21380, loss = (G: 7.24478674, D: 0.02574825) (0.225 sec/batch)\n",
      "2017-04-05 21:54:41.546141: step 21400, loss = (G: 7.18430614, D: 0.04556077) (0.225 sec/batch)\n",
      "2017-04-05 21:54:46.189184: step 21420, loss = (G: 6.25924444, D: 0.02430406) (0.225 sec/batch)\n",
      "2017-04-05 21:54:50.689500: step 21440, loss = (G: 7.73730659, D: 0.01389044) (0.225 sec/batch)\n",
      "2017-04-05 21:54:55.209918: step 21460, loss = (G: 8.52552891, D: 0.00629923) (0.225 sec/batch)\n",
      "2017-04-05 21:54:59.717730: step 21480, loss = (G: 6.28205681, D: 0.67117649) (0.225 sec/batch)\n",
      "2017-04-05 21:55:04.224398: step 21500, loss = (G: 6.65651703, D: 0.02968292) (0.224 sec/batch)\n",
      "2017-04-05 21:55:08.869216: step 21520, loss = (G: 6.34004688, D: 0.03323413) (0.226 sec/batch)\n",
      "2017-04-05 21:55:13.377418: step 21540, loss = (G: 7.94654799, D: 0.01422287) (0.226 sec/batch)\n",
      "2017-04-05 21:55:17.876658: step 21560, loss = (G: 3.68710589, D: 0.20867777) (0.225 sec/batch)\n",
      "2017-04-05 21:55:22.406375: step 21580, loss = (G: 5.57440567, D: 0.15365036) (0.225 sec/batch)\n",
      "2017-04-05 21:55:26.902994: step 21600, loss = (G: 5.36874771, D: 0.05474309) (0.226 sec/batch)\n",
      "2017-04-05 21:55:31.550232: step 21620, loss = (G: 5.68100643, D: 0.06667913) (0.224 sec/batch)\n",
      "2017-04-05 21:55:36.060873: step 21640, loss = (G: 7.87114096, D: 0.00793990) (0.226 sec/batch)\n",
      "2017-04-05 21:55:40.567289: step 21660, loss = (G: 5.66548824, D: 0.03359060) (0.225 sec/batch)\n",
      "2017-04-05 21:55:45.079953: step 21680, loss = (G: 7.51060057, D: 0.19360246) (0.225 sec/batch)\n",
      "2017-04-05 21:55:49.581804: step 21700, loss = (G: 2.88434005, D: 1.11669338) (0.224 sec/batch)\n",
      "2017-04-05 21:55:54.217902: step 21720, loss = (G: 4.43825436, D: 0.12695825) (0.225 sec/batch)\n",
      "2017-04-05 21:55:58.731919: step 21740, loss = (G: 4.00991821, D: 0.17759298) (0.224 sec/batch)\n",
      "2017-04-05 21:56:03.229012: step 21760, loss = (G: 5.73176193, D: 0.02715674) (0.226 sec/batch)\n",
      "2017-04-05 21:56:07.726230: step 21780, loss = (G: 4.46503210, D: 0.08613573) (0.225 sec/batch)\n",
      "2017-04-05 21:56:12.235609: step 21800, loss = (G: 5.08614206, D: 0.12589307) (0.226 sec/batch)\n",
      "2017-04-05 21:56:16.880221: step 21820, loss = (G: 2.48196220, D: 0.92700523) (0.226 sec/batch)\n",
      "2017-04-05 21:56:21.376301: step 21840, loss = (G: 6.00036192, D: 0.01767344) (0.225 sec/batch)\n",
      "2017-04-05 21:56:25.890880: step 21860, loss = (G: 4.89069653, D: 0.07874275) (0.226 sec/batch)\n",
      "2017-04-05 21:56:30.400584: step 21880, loss = (G: 6.08663225, D: 0.20688808) (0.225 sec/batch)\n",
      "2017-04-05 21:56:34.898817: step 21900, loss = (G: 5.75773621, D: 0.07297090) (0.223 sec/batch)\n",
      "2017-04-05 21:56:39.555767: step 21920, loss = (G: 9.07960033, D: 0.07310234) (0.225 sec/batch)\n",
      "2017-04-05 21:56:44.048126: step 21940, loss = (G: 4.11115408, D: 0.19096836) (0.225 sec/batch)\n",
      "2017-04-05 21:56:48.555156: step 21960, loss = (G: 9.34593105, D: 0.45659986) (0.225 sec/batch)\n",
      "2017-04-05 21:56:53.056953: step 21980, loss = (G: 6.03143883, D: 0.03283575) (0.225 sec/batch)\n",
      "2017-04-05 21:56:57.561192: step 22000, loss = (G: 7.46869755, D: 0.03105461) (0.226 sec/batch)\n",
      "2017-04-05 21:57:02.340140: step 22020, loss = (G: 5.45461226, D: 0.09607744) (0.226 sec/batch)\n",
      "2017-04-05 21:57:06.845069: step 22040, loss = (G: 9.64717579, D: 0.00511879) (0.226 sec/batch)\n",
      "2017-04-05 21:57:11.334747: step 22060, loss = (G: 7.10290194, D: 0.00865225) (0.226 sec/batch)\n",
      "2017-04-05 21:57:15.846252: step 22080, loss = (G: 5.87160873, D: 0.04061240) (0.225 sec/batch)\n",
      "2017-04-05 21:57:20.352929: step 22100, loss = (G: 6.58738661, D: 0.05308641) (0.225 sec/batch)\n",
      "2017-04-05 21:57:24.991981: step 22120, loss = (G: 5.34441710, D: 0.08302899) (0.225 sec/batch)\n",
      "2017-04-05 21:57:29.496753: step 22140, loss = (G: 3.63126945, D: 0.41229832) (0.225 sec/batch)\n",
      "2017-04-05 21:57:33.995805: step 22160, loss = (G: 4.41610622, D: 0.18262300) (0.225 sec/batch)\n",
      "2017-04-05 21:57:38.510075: step 22180, loss = (G: 14.16006184, D: 2.21904874) (0.225 sec/batch)\n",
      "2017-04-05 21:57:43.039106: step 22200, loss = (G: 6.63985682, D: 0.02384121) (0.226 sec/batch)\n",
      "2017-04-05 21:57:47.688996: step 22220, loss = (G: 6.10178471, D: 0.03750029) (0.225 sec/batch)\n",
      "2017-04-05 21:57:52.195231: step 22240, loss = (G: 6.58785343, D: 0.06862708) (0.226 sec/batch)\n",
      "2017-04-05 21:57:56.687650: step 22260, loss = (G: 8.37872219, D: 0.34997642) (0.224 sec/batch)\n",
      "2017-04-05 21:58:01.222647: step 22280, loss = (G: 7.29122591, D: 0.02818990) (0.224 sec/batch)\n",
      "2017-04-05 21:58:05.716287: step 22300, loss = (G: 10.50467110, D: 0.09853069) (0.226 sec/batch)\n",
      "2017-04-05 21:58:10.344899: step 22320, loss = (G: 6.06015348, D: 0.03083719) (0.225 sec/batch)\n",
      "2017-04-05 21:58:14.853346: step 22340, loss = (G: 7.32938385, D: 0.01046137) (0.225 sec/batch)\n",
      "2017-04-05 21:58:19.350795: step 22360, loss = (G: 6.46131992, D: 0.04338461) (0.225 sec/batch)\n",
      "2017-04-05 21:58:23.880177: step 22380, loss = (G: 8.09428406, D: 0.00863333) (0.225 sec/batch)\n",
      "2017-04-05 21:58:28.372126: step 22400, loss = (G: 5.55792332, D: 0.03073095) (0.223 sec/batch)\n",
      "2017-04-05 21:58:33.003432: step 22420, loss = (G: 6.12656116, D: 0.03026029) (0.225 sec/batch)\n",
      "2017-04-05 21:58:37.510275: step 22440, loss = (G: 4.83498383, D: 0.04296190) (0.226 sec/batch)\n",
      "2017-04-05 21:58:42.006744: step 22460, loss = (G: 5.13873482, D: 0.07981212) (0.230 sec/batch)\n",
      "2017-04-05 21:58:46.514881: step 22480, loss = (G: 6.41636467, D: 0.02470605) (0.225 sec/batch)\n",
      "2017-04-05 21:58:51.008142: step 22500, loss = (G: 4.38280869, D: 0.14984944) (0.226 sec/batch)\n",
      "2017-04-05 21:58:55.645721: step 22520, loss = (G: 10.84414101, D: 0.11280679) (0.226 sec/batch)\n",
      "2017-04-05 21:59:00.139866: step 22540, loss = (G: 8.15620518, D: 0.12499645) (0.225 sec/batch)\n",
      "2017-04-05 21:59:04.635944: step 22560, loss = (G: 7.64543295, D: 0.00775240) (0.225 sec/batch)\n",
      "2017-04-05 21:59:09.144554: step 22580, loss = (G: 15.92652225, D: 0.96084028) (0.225 sec/batch)\n",
      "2017-04-05 21:59:13.641673: step 22600, loss = (G: 7.54902315, D: 0.08686060) (0.224 sec/batch)\n",
      "2017-04-05 21:59:18.288854: step 22620, loss = (G: 7.21583748, D: 0.01155758) (0.225 sec/batch)\n",
      "2017-04-05 21:59:22.792641: step 22640, loss = (G: 5.44147110, D: 0.04288364) (0.224 sec/batch)\n",
      "2017-04-05 21:59:27.308075: step 22660, loss = (G: 5.43076706, D: 0.09670220) (0.224 sec/batch)\n",
      "2017-04-05 21:59:31.806270: step 22680, loss = (G: 7.27460146, D: 0.01647180) (0.225 sec/batch)\n",
      "2017-04-05 21:59:36.305747: step 22700, loss = (G: 8.03947449, D: 0.01578888) (0.224 sec/batch)\n",
      "2017-04-05 21:59:40.981474: step 22720, loss = (G: 5.60848808, D: 0.04750586) (0.229 sec/batch)\n",
      "2017-04-05 21:59:45.483015: step 22740, loss = (G: 6.78847742, D: 0.09871231) (0.225 sec/batch)\n",
      "2017-04-05 21:59:49.991785: step 22760, loss = (G: 6.38032150, D: 0.06053102) (0.223 sec/batch)\n",
      "2017-04-05 21:59:54.496869: step 22780, loss = (G: 11.78309631, D: 0.00779587) (0.225 sec/batch)\n",
      "2017-04-05 21:59:59.008652: step 22800, loss = (G: 5.49323750, D: 0.04622876) (0.225 sec/batch)\n",
      "2017-04-05 22:00:03.653046: step 22820, loss = (G: 22.70487595, D: 0.00001113) (0.224 sec/batch)\n",
      "2017-04-05 22:00:08.169566: step 22840, loss = (G: 6.96084023, D: 0.04982658) (0.224 sec/batch)\n",
      "2017-04-05 22:00:12.657776: step 22860, loss = (G: 7.18297291, D: 0.01612598) (0.223 sec/batch)\n",
      "2017-04-05 22:00:17.159365: step 22880, loss = (G: 7.48635483, D: 0.07622036) (0.225 sec/batch)\n",
      "2017-04-05 22:00:21.653135: step 22900, loss = (G: 3.62605143, D: 0.29934937) (0.223 sec/batch)\n",
      "2017-04-05 22:00:26.282748: step 22920, loss = (G: 8.71179771, D: 0.00266483) (0.225 sec/batch)\n",
      "2017-04-05 22:00:30.791855: step 22940, loss = (G: 9.45477104, D: 0.03573927) (0.224 sec/batch)\n",
      "2017-04-05 22:00:35.282158: step 22960, loss = (G: 6.02703953, D: 0.03295799) (0.223 sec/batch)\n",
      "2017-04-05 22:00:39.780225: step 22980, loss = (G: 7.05703592, D: 0.01852541) (0.225 sec/batch)\n",
      "2017-04-05 22:00:44.283592: step 23000, loss = (G: 6.84173107, D: 0.12660910) (0.225 sec/batch)\n",
      "2017-04-05 22:00:48.927587: step 23020, loss = (G: 8.08274174, D: 0.01379037) (0.224 sec/batch)\n",
      "2017-04-05 22:00:53.429786: step 23040, loss = (G: 11.54988670, D: 0.03790647) (0.225 sec/batch)\n",
      "2017-04-05 22:00:57.933516: step 23060, loss = (G: 4.77765465, D: 0.06812122) (0.225 sec/batch)\n",
      "2017-04-05 22:01:02.431861: step 23080, loss = (G: 8.44982243, D: 0.04896513) (0.226 sec/batch)\n",
      "2017-04-05 22:01:06.929717: step 23100, loss = (G: 3.90553665, D: 0.13221930) (0.224 sec/batch)\n",
      "2017-04-05 22:01:11.574333: step 23120, loss = (G: 7.44866371, D: 0.25176102) (0.224 sec/batch)\n",
      "2017-04-05 22:01:16.076783: step 23140, loss = (G: 7.04429150, D: 0.02051431) (0.224 sec/batch)\n",
      "2017-04-05 22:01:20.580665: step 23160, loss = (G: 2.89288902, D: 0.70716703) (0.225 sec/batch)\n",
      "2017-04-05 22:01:25.206653: step 23180, loss = (G: 5.59053898, D: 0.04157178) (0.225 sec/batch)\n",
      "2017-04-05 22:01:29.709145: step 23200, loss = (G: 6.71109772, D: 0.03511964) (0.224 sec/batch)\n",
      "2017-04-05 22:01:34.348483: step 23220, loss = (G: 7.16172552, D: 0.02426353) (0.227 sec/batch)\n",
      "2017-04-05 22:01:38.850456: step 23240, loss = (G: 7.76733494, D: 0.02638724) (0.224 sec/batch)\n",
      "2017-04-05 22:01:43.355363: step 23260, loss = (G: 7.64402676, D: 0.00677049) (0.225 sec/batch)\n",
      "2017-04-05 22:01:47.861502: step 23280, loss = (G: 5.58266401, D: 0.03831089) (0.224 sec/batch)\n",
      "2017-04-05 22:01:52.367398: step 23300, loss = (G: 7.86826944, D: 0.01213168) (0.226 sec/batch)\n",
      "2017-04-05 22:01:57.007150: step 23320, loss = (G: 7.36465693, D: 0.01927912) (0.225 sec/batch)\n",
      "2017-04-05 22:02:01.546123: step 23340, loss = (G: 6.87414837, D: 0.02388073) (0.225 sec/batch)\n",
      "2017-04-05 22:02:06.030196: step 23360, loss = (G: 6.91891527, D: 0.03766348) (0.226 sec/batch)\n",
      "2017-04-05 22:02:10.526739: step 23380, loss = (G: 6.08000946, D: 0.05874990) (0.226 sec/batch)\n",
      "2017-04-05 22:02:15.020381: step 23400, loss = (G: 7.15339279, D: 0.01424393) (0.225 sec/batch)\n",
      "2017-04-05 22:02:19.669823: step 23420, loss = (G: 8.10842228, D: 0.02194709) (0.224 sec/batch)\n",
      "2017-04-05 22:02:24.187459: step 23440, loss = (G: 7.41471481, D: 0.03035402) (0.225 sec/batch)\n",
      "2017-04-05 22:02:28.700845: step 23460, loss = (G: 5.56667328, D: 0.05480811) (0.225 sec/batch)\n",
      "2017-04-05 22:02:33.196484: step 23480, loss = (G: 11.64414406, D: 0.00854228) (0.225 sec/batch)\n",
      "2017-04-05 22:02:37.740460: step 23500, loss = (G: 8.30978775, D: 0.01959710) (0.229 sec/batch)\n",
      "2017-04-05 22:02:42.382395: step 23520, loss = (G: 9.61654663, D: 0.01919462) (0.228 sec/batch)\n",
      "2017-04-05 22:02:46.880777: step 23540, loss = (G: 4.54731369, D: 0.05863364) (0.224 sec/batch)\n",
      "2017-04-05 22:02:51.386899: step 23560, loss = (G: 8.34624481, D: 0.00850025) (0.225 sec/batch)\n",
      "2017-04-05 22:02:55.878998: step 23580, loss = (G: 5.83285952, D: 0.13373208) (0.224 sec/batch)\n",
      "2017-04-05 22:03:00.371354: step 23600, loss = (G: 8.01823235, D: 0.00756016) (0.224 sec/batch)\n",
      "2017-04-05 22:03:05.024423: step 23620, loss = (G: 5.24811029, D: 0.03561846) (0.226 sec/batch)\n",
      "2017-04-05 22:03:09.519955: step 23640, loss = (G: 7.96270132, D: 0.42221797) (0.226 sec/batch)\n",
      "2017-04-05 22:03:14.025840: step 23660, loss = (G: 6.65627623, D: 0.03308796) (0.226 sec/batch)\n",
      "2017-04-05 22:03:18.526183: step 23680, loss = (G: 8.10658550, D: 0.03146027) (0.225 sec/batch)\n",
      "2017-04-05 22:03:23.029263: step 23700, loss = (G: 5.81529713, D: 0.03590317) (0.224 sec/batch)\n",
      "2017-04-05 22:03:27.681825: step 23720, loss = (G: 5.88740492, D: 0.04688448) (0.230 sec/batch)\n",
      "2017-04-05 22:03:32.199813: step 23740, loss = (G: 5.85681581, D: 0.02449032) (0.225 sec/batch)\n",
      "2017-04-05 22:03:36.699344: step 23760, loss = (G: 5.79002810, D: 0.20621109) (0.225 sec/batch)\n",
      "2017-04-05 22:03:41.205324: step 23780, loss = (G: 7.10795021, D: 0.20918593) (0.225 sec/batch)\n",
      "2017-04-05 22:03:45.704459: step 23800, loss = (G: 8.45052719, D: 0.02922035) (0.224 sec/batch)\n",
      "2017-04-05 22:03:50.356485: step 23820, loss = (G: 6.86192417, D: 0.01463516) (0.225 sec/batch)\n",
      "2017-04-05 22:03:54.859688: step 23840, loss = (G: 7.41077805, D: 0.04076169) (0.225 sec/batch)\n",
      "2017-04-05 22:03:59.373074: step 23860, loss = (G: 7.08674049, D: 0.10255478) (0.230 sec/batch)\n",
      "2017-04-05 22:04:03.878976: step 23880, loss = (G: 7.51034641, D: 0.04615628) (0.224 sec/batch)\n",
      "2017-04-05 22:04:08.433808: step 23900, loss = (G: 8.39291286, D: 0.00680970) (0.225 sec/batch)\n",
      "2017-04-05 22:04:13.070919: step 23920, loss = (G: 5.97990799, D: 0.12211435) (0.225 sec/batch)\n",
      "2017-04-05 22:04:17.578242: step 23940, loss = (G: 3.14709330, D: 0.46463042) (0.226 sec/batch)\n",
      "2017-04-05 22:04:22.084004: step 23960, loss = (G: 6.78279209, D: 0.02309372) (0.225 sec/batch)\n",
      "2017-04-05 22:04:26.587050: step 23980, loss = (G: 4.47404528, D: 0.10886965) (0.225 sec/batch)\n",
      "2017-04-05 22:04:31.087805: step 24000, loss = (G: 5.88327408, D: 0.03089150) (0.227 sec/batch)\n",
      "2017-04-05 22:04:35.743966: step 24020, loss = (G: 8.20820236, D: 0.00796801) (0.230 sec/batch)\n",
      "2017-04-05 22:04:40.259615: step 24040, loss = (G: 6.61416245, D: 0.03193974) (0.225 sec/batch)\n",
      "2017-04-05 22:04:44.750450: step 24060, loss = (G: 6.94417572, D: 0.08194731) (0.224 sec/batch)\n",
      "2017-04-05 22:04:49.265137: step 24080, loss = (G: 12.93500710, D: 0.09592263) (0.224 sec/batch)\n",
      "2017-04-05 22:04:53.764955: step 24100, loss = (G: 6.26781082, D: 0.05841622) (0.225 sec/batch)\n",
      "2017-04-05 22:04:58.412715: step 24120, loss = (G: 8.01556873, D: 0.01257136) (0.227 sec/batch)\n",
      "2017-04-05 22:05:02.920478: step 24140, loss = (G: 4.92545509, D: 0.16611901) (0.226 sec/batch)\n",
      "2017-04-05 22:05:07.431837: step 24160, loss = (G: 6.06376696, D: 0.04677211) (0.228 sec/batch)\n",
      "2017-04-05 22:05:11.935862: step 24180, loss = (G: 4.16009474, D: 0.38611275) (0.225 sec/batch)\n",
      "2017-04-05 22:05:16.447715: step 24200, loss = (G: 4.24295807, D: 0.11240187) (0.225 sec/batch)\n",
      "2017-04-05 22:05:21.084549: step 24220, loss = (G: 6.76801443, D: 0.06846562) (0.224 sec/batch)\n",
      "2017-04-05 22:05:25.583702: step 24240, loss = (G: 11.12345791, D: 0.63085836) (0.225 sec/batch)\n",
      "2017-04-05 22:05:30.096137: step 24260, loss = (G: 4.34156895, D: 0.08719803) (0.226 sec/batch)\n",
      "2017-04-05 22:05:34.596071: step 24280, loss = (G: 4.14132071, D: 0.19851291) (0.225 sec/batch)\n",
      "2017-04-05 22:05:39.095418: step 24300, loss = (G: 6.40441656, D: 0.02617699) (0.228 sec/batch)\n",
      "2017-04-05 22:05:43.742885: step 24320, loss = (G: 7.03518057, D: 0.18647286) (0.225 sec/batch)\n",
      "2017-04-05 22:05:48.388220: step 24340, loss = (G: 3.91582084, D: 0.48633441) (0.225 sec/batch)\n",
      "2017-04-05 22:05:52.885131: step 24360, loss = (G: 5.58119202, D: 0.04706740) (0.224 sec/batch)\n",
      "2017-04-05 22:05:57.388678: step 24380, loss = (G: 5.33929539, D: 0.08393081) (0.225 sec/batch)\n",
      "2017-04-05 22:06:01.884927: step 24400, loss = (G: 6.52682161, D: 0.06611274) (0.225 sec/batch)\n",
      "2017-04-05 22:06:06.520490: step 24420, loss = (G: 5.66598701, D: 0.06358127) (0.223 sec/batch)\n",
      "2017-04-05 22:06:11.032061: step 24440, loss = (G: 5.44575119, D: 0.06833072) (0.225 sec/batch)\n",
      "2017-04-05 22:06:15.535399: step 24460, loss = (G: 6.42959404, D: 0.05250507) (0.225 sec/batch)\n",
      "2017-04-05 22:06:20.041486: step 24480, loss = (G: 14.16475868, D: 0.04381070) (0.227 sec/batch)\n",
      "2017-04-05 22:06:24.547994: step 24500, loss = (G: 7.70797300, D: 0.02792811) (0.225 sec/batch)\n",
      "2017-04-05 22:06:29.185592: step 24520, loss = (G: 4.82672453, D: 0.13556366) (0.225 sec/batch)\n",
      "2017-04-05 22:06:33.677675: step 24540, loss = (G: 13.76829147, D: 0.00750658) (0.224 sec/batch)\n",
      "2017-04-05 22:06:38.193711: step 24560, loss = (G: 5.02216530, D: 0.08434004) (0.231 sec/batch)\n",
      "2017-04-05 22:06:42.742508: step 24580, loss = (G: 8.37372875, D: 0.15044712) (0.224 sec/batch)\n",
      "2017-04-05 22:06:47.228222: step 24600, loss = (G: 5.57646751, D: 0.13234526) (0.224 sec/batch)\n",
      "2017-04-05 22:06:51.878986: step 24620, loss = (G: 6.88377571, D: 0.13439505) (0.224 sec/batch)\n",
      "2017-04-05 22:06:56.385775: step 24640, loss = (G: 7.13384104, D: 0.03302977) (0.224 sec/batch)\n",
      "2017-04-05 22:07:00.878921: step 24660, loss = (G: 6.94459772, D: 0.02494866) (0.225 sec/batch)\n",
      "2017-04-05 22:07:05.375931: step 24680, loss = (G: 6.56354332, D: 0.01909155) (0.224 sec/batch)\n",
      "2017-04-05 22:07:09.880688: step 24700, loss = (G: 5.93983746, D: 0.03879261) (0.223 sec/batch)\n",
      "2017-04-05 22:07:14.525214: step 24720, loss = (G: 7.69713211, D: 0.01327326) (0.225 sec/batch)\n",
      "2017-04-05 22:07:19.021211: step 24740, loss = (G: 6.61832094, D: 0.05975395) (0.225 sec/batch)\n",
      "2017-04-05 22:07:23.510200: step 24760, loss = (G: 14.71801567, D: 0.29377165) (0.224 sec/batch)\n",
      "2017-04-05 22:07:28.004677: step 24780, loss = (G: 9.51489735, D: 0.03114465) (0.224 sec/batch)\n",
      "2017-04-05 22:07:32.495572: step 24800, loss = (G: 6.53597689, D: 0.05856606) (0.225 sec/batch)\n",
      "2017-04-05 22:07:37.153927: step 24820, loss = (G: 7.28207874, D: 0.01536702) (0.226 sec/batch)\n",
      "2017-04-05 22:07:41.651532: step 24840, loss = (G: 8.43209362, D: 0.04805073) (0.226 sec/batch)\n",
      "2017-04-05 22:07:46.153194: step 24860, loss = (G: 7.75719357, D: 0.01601189) (0.225 sec/batch)\n",
      "2017-04-05 22:07:50.642969: step 24880, loss = (G: 5.73614740, D: 0.05702427) (0.226 sec/batch)\n",
      "2017-04-05 22:07:55.148820: step 24900, loss = (G: 5.05755997, D: 0.08024107) (0.225 sec/batch)\n",
      "2017-04-05 22:07:59.810490: step 24920, loss = (G: 9.18025494, D: 0.01480730) (0.225 sec/batch)\n",
      "2017-04-05 22:08:04.310500: step 24940, loss = (G: 6.00861549, D: 0.02729904) (0.224 sec/batch)\n",
      "2017-04-05 22:08:08.800774: step 24960, loss = (G: 5.72022963, D: 0.06478874) (0.224 sec/batch)\n",
      "2017-04-05 22:08:13.299415: step 24980, loss = (G: 5.45627689, D: 0.04088402) (0.225 sec/batch)\n",
      "2017-04-05 22:08:17.791639: step 25000, loss = (G: 5.93207979, D: 0.10471040) (0.224 sec/batch)\n",
      "2017-04-05 22:08:22.435125: step 25020, loss = (G: 4.66904163, D: 0.23169163) (0.228 sec/batch)\n",
      "2017-04-05 22:08:26.928491: step 25040, loss = (G: 4.94822741, D: 0.11742869) (0.224 sec/batch)\n",
      "2017-04-05 22:08:31.422105: step 25060, loss = (G: 14.46434975, D: 0.15146430) (0.226 sec/batch)\n",
      "2017-04-05 22:08:35.946266: step 25080, loss = (G: 5.46651268, D: 0.03065762) (0.226 sec/batch)\n",
      "2017-04-05 22:08:40.431747: step 25100, loss = (G: 5.38173056, D: 0.06025558) (0.224 sec/batch)\n",
      "2017-04-05 22:08:45.091664: step 25120, loss = (G: 6.33048058, D: 0.04179116) (0.224 sec/batch)\n",
      "2017-04-05 22:08:49.586571: step 25140, loss = (G: 5.63766623, D: 0.04386374) (0.226 sec/batch)\n",
      "2017-04-05 22:08:54.081496: step 25160, loss = (G: 4.68121719, D: 0.08921011) (0.224 sec/batch)\n",
      "2017-04-05 22:08:58.582832: step 25180, loss = (G: 18.73063660, D: 2.82133675) (0.224 sec/batch)\n",
      "2017-04-05 22:09:03.085166: step 25200, loss = (G: 6.52483130, D: 0.01890042) (0.225 sec/batch)\n",
      "2017-04-05 22:09:07.722126: step 25220, loss = (G: 6.27663660, D: 0.03053482) (0.224 sec/batch)\n",
      "2017-04-05 22:09:12.219043: step 25240, loss = (G: 9.61760426, D: 0.01420754) (0.226 sec/batch)\n",
      "2017-04-05 22:09:16.723879: step 25260, loss = (G: 8.32019329, D: 0.02685712) (0.226 sec/batch)\n",
      "2017-04-05 22:09:21.239388: step 25280, loss = (G: 5.25814342, D: 0.10691854) (0.224 sec/batch)\n",
      "2017-04-05 22:09:25.735218: step 25300, loss = (G: 12.49985504, D: 0.11910957) (0.226 sec/batch)\n",
      "2017-04-05 22:09:30.376126: step 25320, loss = (G: 5.71874189, D: 0.08005958) (0.224 sec/batch)\n",
      "2017-04-05 22:09:34.872813: step 25340, loss = (G: 6.49170542, D: 0.10571745) (0.223 sec/batch)\n",
      "2017-04-05 22:09:39.386254: step 25360, loss = (G: 7.85501909, D: 0.00750309) (0.226 sec/batch)\n",
      "2017-04-05 22:09:43.874076: step 25380, loss = (G: 4.13189840, D: 0.49169612) (0.225 sec/batch)\n",
      "2017-04-05 22:09:48.369665: step 25400, loss = (G: 6.28918123, D: 0.02694920) (0.225 sec/batch)\n",
      "2017-04-05 22:09:53.011166: step 25420, loss = (G: 5.06718063, D: 0.10608318) (0.226 sec/batch)\n",
      "2017-04-05 22:09:57.517314: step 25440, loss = (G: 5.84483528, D: 0.04206501) (0.224 sec/batch)\n",
      "2017-04-05 22:10:02.015394: step 25460, loss = (G: 5.32492256, D: 0.07847250) (0.225 sec/batch)\n",
      "2017-04-05 22:10:06.553991: step 25480, loss = (G: 9.50735188, D: 0.03569473) (0.223 sec/batch)\n",
      "2017-04-05 22:10:11.203804: step 25500, loss = (G: 9.86345100, D: 0.02461018) (0.225 sec/batch)\n",
      "2017-04-05 22:10:15.843715: step 25520, loss = (G: 5.20698118, D: 0.06571424) (0.225 sec/batch)\n",
      "2017-04-05 22:10:20.373832: step 25540, loss = (G: 5.58660221, D: 0.25873500) (0.229 sec/batch)\n",
      "2017-04-05 22:10:24.866005: step 25560, loss = (G: 3.86297226, D: 0.16395259) (0.224 sec/batch)\n",
      "2017-04-05 22:10:29.358180: step 25580, loss = (G: 8.02064037, D: 0.00940185) (0.224 sec/batch)\n",
      "2017-04-05 22:10:33.891044: step 25600, loss = (G: 5.08988047, D: 0.06217463) (0.228 sec/batch)\n",
      "2017-04-05 22:10:38.525799: step 25620, loss = (G: 8.08438015, D: 0.01618159) (0.230 sec/batch)\n",
      "2017-04-05 22:10:43.029698: step 25640, loss = (G: 6.08989334, D: 0.08997390) (0.228 sec/batch)\n",
      "2017-04-05 22:10:47.522714: step 25660, loss = (G: 9.02197552, D: 0.03025802) (0.224 sec/batch)\n",
      "2017-04-05 22:10:52.005300: step 25680, loss = (G: 4.42827845, D: 0.10163669) (0.224 sec/batch)\n",
      "2017-04-05 22:10:56.497437: step 25700, loss = (G: 3.59705019, D: 0.18460287) (0.225 sec/batch)\n",
      "2017-04-05 22:11:01.148192: step 25720, loss = (G: 3.10528922, D: 0.45177260) (0.226 sec/batch)\n",
      "2017-04-05 22:11:05.643051: step 25740, loss = (G: 7.87016869, D: 0.08392029) (0.224 sec/batch)\n",
      "2017-04-05 22:11:10.131952: step 25760, loss = (G: 6.28461170, D: 0.02013036) (0.224 sec/batch)\n",
      "2017-04-05 22:11:14.639288: step 25780, loss = (G: 6.45302963, D: 0.08904541) (0.224 sec/batch)\n",
      "2017-04-05 22:11:19.137363: step 25800, loss = (G: 8.95937920, D: 0.00219685) (0.224 sec/batch)\n",
      "2017-04-05 22:11:23.787832: step 25820, loss = (G: 14.69025421, D: 0.05233413) (0.224 sec/batch)\n",
      "2017-04-05 22:11:28.275804: step 25840, loss = (G: 7.23736906, D: 0.04707471) (0.225 sec/batch)\n",
      "2017-04-05 22:11:32.769201: step 25860, loss = (G: 3.22089624, D: 0.37193999) (0.224 sec/batch)\n",
      "2017-04-05 22:11:37.266105: step 25880, loss = (G: 6.92805099, D: 0.04536055) (0.225 sec/batch)\n",
      "2017-04-05 22:11:41.758506: step 25900, loss = (G: 6.41918039, D: 0.04777317) (0.225 sec/batch)\n",
      "2017-04-05 22:11:46.406476: step 25920, loss = (G: 16.43014526, D: 0.00384793) (0.225 sec/batch)\n",
      "2017-04-05 22:11:50.894422: step 25940, loss = (G: 6.81533003, D: 0.01894633) (0.224 sec/batch)\n",
      "2017-04-05 22:11:55.401690: step 25960, loss = (G: 6.25522280, D: 0.10429303) (0.224 sec/batch)\n",
      "2017-04-05 22:11:59.891007: step 25980, loss = (G: 8.19205761, D: 0.01233774) (0.224 sec/batch)\n",
      "2017-04-05 22:12:04.392944: step 26000, loss = (G: 10.63444233, D: 0.11006907) (0.226 sec/batch)\n",
      "2017-04-05 22:12:09.038005: step 26020, loss = (G: 5.56532955, D: 0.12058735) (0.224 sec/batch)\n",
      "2017-04-05 22:12:13.546366: step 26040, loss = (G: 6.06580830, D: 0.19384587) (0.225 sec/batch)\n",
      "2017-04-05 22:12:18.050505: step 26060, loss = (G: 5.94646025, D: 0.02067370) (0.225 sec/batch)\n",
      "2017-04-05 22:12:22.562266: step 26080, loss = (G: 5.93121481, D: 0.02573610) (0.226 sec/batch)\n",
      "2017-04-05 22:12:27.056361: step 26100, loss = (G: 6.18097401, D: 0.01808783) (0.226 sec/batch)\n",
      "2017-04-05 22:12:31.687214: step 26120, loss = (G: 6.36594343, D: 0.08920796) (0.224 sec/batch)\n",
      "2017-04-05 22:12:36.188496: step 26140, loss = (G: 5.23515129, D: 0.08822814) (0.226 sec/batch)\n",
      "2017-04-05 22:12:40.687105: step 26160, loss = (G: 8.37397957, D: 0.02170491) (0.225 sec/batch)\n",
      "2017-04-05 22:12:45.181763: step 26180, loss = (G: 10.39457226, D: 0.00552442) (0.224 sec/batch)\n",
      "2017-04-05 22:12:49.687384: step 26200, loss = (G: 10.40041447, D: 0.00681051) (0.229 sec/batch)\n",
      "2017-04-05 22:12:54.347480: step 26220, loss = (G: 6.89465952, D: 0.04890641) (0.227 sec/batch)\n",
      "2017-04-05 22:12:58.840646: step 26240, loss = (G: 5.07958984, D: 0.16319115) (0.224 sec/batch)\n",
      "2017-04-05 22:13:03.354996: step 26260, loss = (G: 5.86571980, D: 0.16615385) (0.226 sec/batch)\n",
      "2017-04-05 22:13:07.863036: step 26280, loss = (G: 5.90048218, D: 0.04064242) (0.227 sec/batch)\n",
      "2017-04-05 22:13:12.394726: step 26300, loss = (G: 4.61591673, D: 0.32351056) (0.224 sec/batch)\n",
      "2017-04-05 22:13:17.038397: step 26320, loss = (G: 6.76977491, D: 0.09337454) (0.226 sec/batch)\n",
      "2017-04-05 22:13:21.533499: step 26340, loss = (G: 5.14910793, D: 0.07911043) (0.225 sec/batch)\n",
      "2017-04-05 22:13:26.029325: step 26360, loss = (G: 17.49265862, D: 0.00238514) (0.225 sec/batch)\n",
      "2017-04-05 22:13:30.538570: step 26380, loss = (G: 4.44139576, D: 0.09347019) (0.224 sec/batch)\n",
      "2017-04-05 22:13:35.025254: step 26400, loss = (G: 9.67967892, D: 0.01110317) (0.224 sec/batch)\n",
      "2017-04-05 22:13:39.661256: step 26420, loss = (G: 6.52899885, D: 0.03561145) (0.225 sec/batch)\n",
      "2017-04-05 22:13:44.153670: step 26440, loss = (G: 6.84689903, D: 0.02402636) (0.224 sec/batch)\n",
      "2017-04-05 22:13:48.646265: step 26460, loss = (G: 10.49849701, D: 0.19671357) (0.224 sec/batch)\n",
      "2017-04-05 22:13:53.152836: step 26480, loss = (G: 5.06392384, D: 0.20311403) (0.226 sec/batch)\n",
      "2017-04-05 22:13:57.642367: step 26500, loss = (G: 7.21160698, D: 0.07646571) (0.225 sec/batch)\n",
      "2017-04-05 22:14:02.286676: step 26520, loss = (G: 5.29418135, D: 0.06127773) (0.223 sec/batch)\n",
      "2017-04-05 22:14:06.782370: step 26540, loss = (G: 9.74801159, D: 1.05759406) (0.224 sec/batch)\n",
      "2017-04-05 22:14:11.294149: step 26560, loss = (G: 5.30632401, D: 0.10282904) (0.224 sec/batch)\n",
      "2017-04-05 22:14:15.803988: step 26580, loss = (G: 6.20463943, D: 0.02492854) (0.225 sec/batch)\n",
      "2017-04-05 22:14:20.299005: step 26600, loss = (G: 6.12068129, D: 0.02141505) (0.225 sec/batch)\n",
      "2017-04-05 22:14:24.940241: step 26620, loss = (G: 5.44154215, D: 0.08923718) (0.225 sec/batch)\n",
      "2017-04-05 22:14:29.425207: step 26640, loss = (G: 5.36115837, D: 0.09666664) (0.223 sec/batch)\n",
      "2017-04-05 22:14:34.088004: step 26660, loss = (G: 6.34091282, D: 0.01771769) (0.226 sec/batch)\n",
      "2017-04-05 22:14:38.582166: step 26680, loss = (G: 13.61461163, D: 1.80410790) (0.224 sec/batch)\n",
      "2017-04-05 22:14:43.084680: step 26700, loss = (G: 6.41806841, D: 0.03413351) (0.225 sec/batch)\n",
      "2017-04-05 22:14:47.706296: step 26720, loss = (G: 10.56933594, D: 0.07861617) (0.224 sec/batch)\n",
      "2017-04-05 22:14:52.213390: step 26740, loss = (G: 7.41987991, D: 0.21823783) (0.224 sec/batch)\n",
      "2017-04-05 22:14:56.701003: step 26760, loss = (G: 7.24121809, D: 0.02571394) (0.224 sec/batch)\n",
      "2017-04-05 22:15:01.197129: step 26780, loss = (G: 5.92729425, D: 0.05213455) (0.228 sec/batch)\n",
      "2017-04-05 22:15:05.695904: step 26800, loss = (G: 6.19631863, D: 0.03635282) (0.223 sec/batch)\n",
      "2017-04-05 22:15:10.340993: step 26820, loss = (G: 6.08418036, D: 0.07240845) (0.226 sec/batch)\n",
      "2017-04-05 22:15:14.826949: step 26840, loss = (G: 4.50823832, D: 0.08215032) (0.223 sec/batch)\n",
      "2017-04-05 22:15:19.361932: step 26860, loss = (G: 8.84856606, D: 0.23542817) (0.226 sec/batch)\n",
      "2017-04-05 22:15:23.850645: step 26880, loss = (G: 4.83701468, D: 0.05194975) (0.224 sec/batch)\n",
      "2017-04-05 22:15:28.341926: step 26900, loss = (G: 5.19080162, D: 0.05465402) (0.226 sec/batch)\n",
      "2017-04-05 22:15:32.978595: step 26920, loss = (G: 7.41575432, D: 0.00972167) (0.225 sec/batch)\n",
      "2017-04-05 22:15:37.464404: step 26940, loss = (G: 5.69267464, D: 0.11387905) (0.225 sec/batch)\n",
      "2017-04-05 22:15:41.956648: step 26960, loss = (G: 4.69144821, D: 0.18176264) (0.224 sec/batch)\n",
      "2017-04-05 22:15:46.446791: step 26980, loss = (G: 7.09351540, D: 0.01331470) (0.224 sec/batch)\n",
      "2017-04-05 22:15:50.949621: step 27000, loss = (G: 7.57732964, D: 0.03138367) (0.225 sec/batch)\n",
      "2017-04-05 22:15:55.592640: step 27020, loss = (G: 4.23754358, D: 0.12396881) (0.227 sec/batch)\n",
      "2017-04-05 22:16:00.080875: step 27040, loss = (G: 5.76124334, D: 0.02592796) (0.223 sec/batch)\n",
      "2017-04-05 22:16:04.612394: step 27060, loss = (G: 7.23834801, D: 0.04493561) (0.225 sec/batch)\n",
      "2017-04-05 22:16:09.096005: step 27080, loss = (G: 7.96236324, D: 0.00573914) (0.224 sec/batch)\n",
      "2017-04-05 22:16:13.595518: step 27100, loss = (G: 5.39257145, D: 0.09363993) (0.224 sec/batch)\n",
      "2017-04-05 22:16:18.224994: step 27120, loss = (G: 6.19391489, D: 0.02729382) (0.225 sec/batch)\n",
      "2017-04-05 22:16:22.720589: step 27140, loss = (G: 11.97304726, D: 0.16216849) (0.226 sec/batch)\n",
      "2017-04-05 22:16:27.216835: step 27160, loss = (G: 10.94890785, D: 0.35445079) (0.225 sec/batch)\n",
      "2017-04-05 22:16:31.708708: step 27180, loss = (G: 4.61612082, D: 0.05424604) (0.224 sec/batch)\n",
      "2017-04-05 22:16:36.226639: step 27200, loss = (G: 7.57854271, D: 0.07339770) (0.225 sec/batch)\n",
      "2017-04-05 22:16:40.860109: step 27220, loss = (G: 5.35619640, D: 0.02849489) (0.225 sec/batch)\n",
      "2017-04-05 22:16:45.353255: step 27240, loss = (G: 5.80862856, D: 0.02824766) (0.225 sec/batch)\n",
      "2017-04-05 22:16:49.845393: step 27260, loss = (G: 9.01309872, D: 0.21132757) (0.225 sec/batch)\n",
      "2017-04-05 22:16:54.339470: step 27280, loss = (G: 7.51610184, D: 0.01477481) (0.225 sec/batch)\n",
      "2017-04-05 22:16:58.832134: step 27300, loss = (G: 9.53765583, D: 0.05764100) (0.225 sec/batch)\n",
      "2017-04-05 22:17:03.490332: step 27320, loss = (G: 7.23894215, D: 0.00801445) (0.227 sec/batch)\n",
      "2017-04-05 22:17:07.999992: step 27340, loss = (G: 6.39350891, D: 0.04050847) (0.226 sec/batch)\n",
      "2017-04-05 22:17:12.488672: step 27360, loss = (G: 7.17121124, D: 0.01267336) (0.224 sec/batch)\n",
      "2017-04-05 22:17:16.972149: step 27380, loss = (G: 7.73871326, D: 0.03846491) (0.225 sec/batch)\n",
      "2017-04-05 22:17:21.466201: step 27400, loss = (G: 5.93266010, D: 0.11367016) (0.223 sec/batch)\n",
      "2017-04-05 22:17:26.122576: step 27420, loss = (G: 5.08376884, D: 0.04837124) (0.225 sec/batch)\n",
      "2017-04-05 22:17:30.618386: step 27440, loss = (G: 5.85716248, D: 0.04377087) (0.225 sec/batch)\n",
      "2017-04-05 22:17:35.128053: step 27460, loss = (G: 5.75567675, D: 0.02550778) (0.225 sec/batch)\n",
      "2017-04-05 22:17:39.618970: step 27480, loss = (G: 8.48227692, D: 0.96530026) (0.224 sec/batch)\n",
      "2017-04-05 22:17:44.109024: step 27500, loss = (G: 1.29618895, D: 2.78664994) (0.225 sec/batch)\n",
      "2017-04-05 22:17:48.744094: step 27520, loss = (G: 5.34925222, D: 0.04335882) (0.225 sec/batch)\n",
      "2017-04-05 22:17:53.251968: step 27540, loss = (G: 7.12162447, D: 0.11812208) (0.224 sec/batch)\n",
      "2017-04-05 22:17:57.746124: step 27560, loss = (G: 4.83288002, D: 0.05593173) (0.225 sec/batch)\n",
      "2017-04-05 22:18:02.251342: step 27580, loss = (G: 4.82460403, D: 0.91884142) (0.225 sec/batch)\n",
      "2017-04-05 22:18:06.740582: step 27600, loss = (G: 4.08486032, D: 0.14574006) (0.224 sec/batch)\n",
      "2017-04-05 22:18:11.377732: step 27620, loss = (G: 3.43547606, D: 0.27658993) (0.224 sec/batch)\n",
      "2017-04-05 22:18:15.868403: step 27640, loss = (G: 9.63692760, D: 0.07607304) (0.225 sec/batch)\n",
      "2017-04-05 22:18:20.358520: step 27660, loss = (G: 5.21301174, D: 0.07490508) (0.225 sec/batch)\n",
      "2017-04-05 22:18:24.848887: step 27680, loss = (G: 7.49415302, D: 0.02007556) (0.223 sec/batch)\n",
      "2017-04-05 22:18:29.363966: step 27700, loss = (G: 5.79277134, D: 0.06730388) (0.224 sec/batch)\n",
      "2017-04-05 22:18:34.000748: step 27720, loss = (G: 7.84577370, D: 0.13432597) (0.225 sec/batch)\n",
      "2017-04-05 22:18:38.504097: step 27740, loss = (G: 5.81711435, D: 0.05703574) (0.225 sec/batch)\n",
      "2017-04-05 22:18:43.003565: step 27760, loss = (G: 6.34826756, D: 0.02504869) (0.225 sec/batch)\n",
      "2017-04-05 22:18:47.496255: step 27780, loss = (G: 8.45136738, D: 0.00738325) (0.225 sec/batch)\n",
      "2017-04-05 22:18:52.006458: step 27800, loss = (G: 19.70285416, D: 1.00222600) (0.224 sec/batch)\n",
      "2017-04-05 22:18:56.778322: step 27820, loss = (G: 6.29769325, D: 0.03893854) (0.226 sec/batch)\n",
      "2017-04-05 22:19:01.280262: step 27840, loss = (G: 8.30613613, D: 0.00576370) (0.224 sec/batch)\n",
      "2017-04-05 22:19:05.765873: step 27860, loss = (G: 5.49073744, D: 0.15643606) (0.224 sec/batch)\n",
      "2017-04-05 22:19:10.262825: step 27880, loss = (G: 7.25183868, D: 0.00845611) (0.224 sec/batch)\n",
      "2017-04-05 22:19:14.751000: step 27900, loss = (G: 11.11278248, D: 0.00437681) (0.225 sec/batch)\n",
      "2017-04-05 22:19:19.398883: step 27920, loss = (G: 5.84151268, D: 0.09564911) (0.224 sec/batch)\n",
      "2017-04-05 22:19:23.890284: step 27940, loss = (G: 3.58977556, D: 0.24560459) (0.224 sec/batch)\n",
      "2017-04-05 22:19:28.405388: step 27960, loss = (G: 6.82400894, D: 0.06111777) (0.227 sec/batch)\n",
      "2017-04-05 22:19:32.904586: step 27980, loss = (G: 5.81876802, D: 0.04296792) (0.226 sec/batch)\n",
      "2017-04-05 22:19:37.406478: step 28000, loss = (G: 3.36433077, D: 0.31368434) (0.225 sec/batch)\n",
      "2017-04-05 22:19:42.038291: step 28020, loss = (G: 9.53053474, D: 0.02771707) (0.224 sec/batch)\n",
      "2017-04-05 22:19:46.528705: step 28040, loss = (G: 9.05824852, D: 0.01279123) (0.225 sec/batch)\n",
      "2017-04-05 22:19:51.031276: step 28060, loss = (G: 4.44410419, D: 0.07163317) (0.224 sec/batch)\n",
      "2017-04-05 22:19:55.521945: step 28080, loss = (G: 8.64934826, D: 0.10113238) (0.225 sec/batch)\n",
      "2017-04-05 22:20:00.026861: step 28100, loss = (G: 7.35148907, D: 0.02468086) (0.225 sec/batch)\n",
      "2017-04-05 22:20:04.665131: step 28120, loss = (G: 5.13978004, D: 0.06800546) (0.224 sec/batch)\n",
      "2017-04-05 22:20:09.182348: step 28140, loss = (G: 6.48118973, D: 0.02784253) (0.225 sec/batch)\n",
      "2017-04-05 22:20:13.670126: step 28160, loss = (G: 11.19227409, D: 0.17903100) (0.224 sec/batch)\n",
      "2017-04-05 22:20:18.161112: step 28180, loss = (G: 5.20351887, D: 0.03730209) (0.224 sec/batch)\n",
      "2017-04-05 22:20:22.650267: step 28200, loss = (G: 7.29507351, D: 0.03078457) (0.224 sec/batch)\n",
      "2017-04-05 22:20:27.270278: step 28220, loss = (G: 7.59273291, D: 0.01535680) (0.223 sec/batch)\n",
      "2017-04-05 22:20:31.783029: step 28240, loss = (G: 12.41660690, D: 0.55895138) (0.224 sec/batch)\n",
      "2017-04-05 22:20:36.273230: step 28260, loss = (G: 7.05092621, D: 0.03792620) (0.224 sec/batch)\n",
      "2017-04-05 22:20:40.804165: step 28280, loss = (G: 5.15226269, D: 0.10575543) (0.223 sec/batch)\n",
      "2017-04-05 22:20:45.312962: step 28300, loss = (G: 5.80645704, D: 0.03196838) (0.225 sec/batch)\n",
      "2017-04-05 22:20:49.931962: step 28320, loss = (G: 7.00630760, D: 0.04135947) (0.224 sec/batch)\n",
      "2017-04-05 22:20:54.431264: step 28340, loss = (G: 9.32013416, D: 0.03124225) (0.224 sec/batch)\n",
      "2017-04-05 22:20:58.919429: step 28360, loss = (G: 4.58662462, D: 0.21788993) (0.225 sec/batch)\n",
      "2017-04-05 22:21:03.435180: step 28380, loss = (G: 6.17755127, D: 0.03338607) (0.223 sec/batch)\n",
      "2017-04-05 22:21:07.918362: step 28400, loss = (G: 5.32995462, D: 0.05575854) (0.224 sec/batch)\n",
      "2017-04-05 22:21:12.572084: step 28420, loss = (G: 8.55022430, D: 0.06290071) (0.224 sec/batch)\n",
      "2017-04-05 22:21:17.076484: step 28440, loss = (G: 4.48297930, D: 0.10010561) (0.225 sec/batch)\n",
      "2017-04-05 22:21:21.572769: step 28460, loss = (G: 7.49269629, D: 0.86649328) (0.224 sec/batch)\n",
      "2017-04-05 22:21:26.068177: step 28480, loss = (G: 4.27852154, D: 0.09038293) (0.225 sec/batch)\n",
      "2017-04-05 22:21:30.565821: step 28500, loss = (G: 6.05829763, D: 0.05682236) (0.225 sec/batch)\n",
      "2017-04-05 22:21:35.191196: step 28520, loss = (G: 5.41520357, D: 0.05068104) (0.224 sec/batch)\n",
      "2017-04-05 22:21:39.679085: step 28540, loss = (G: 8.21717644, D: 0.00460895) (0.224 sec/batch)\n",
      "2017-04-05 22:21:44.166751: step 28560, loss = (G: 7.34321594, D: 0.01892246) (0.224 sec/batch)\n",
      "2017-04-05 22:21:48.671429: step 28580, loss = (G: 6.56846142, D: 0.05177314) (0.223 sec/batch)\n",
      "2017-04-05 22:21:53.177822: step 28600, loss = (G: 7.96619797, D: 0.01181761) (0.225 sec/batch)\n",
      "2017-04-05 22:21:57.805901: step 28620, loss = (G: 8.32708645, D: 0.03787655) (0.225 sec/batch)\n",
      "2017-04-05 22:22:02.321047: step 28640, loss = (G: 15.92263889, D: 0.07497518) (0.228 sec/batch)\n",
      "2017-04-05 22:22:06.808796: step 28660, loss = (G: 13.21499634, D: 0.00934383) (0.223 sec/batch)\n",
      "2017-04-05 22:22:11.315296: step 28680, loss = (G: 9.59667683, D: 0.19494273) (0.228 sec/batch)\n",
      "2017-04-05 22:22:15.811928: step 28700, loss = (G: 14.86813450, D: 0.21855232) (0.224 sec/batch)\n",
      "2017-04-05 22:22:20.441432: step 28720, loss = (G: 4.94006729, D: 0.20883828) (0.224 sec/batch)\n",
      "2017-04-05 22:22:24.928651: step 28740, loss = (G: 7.90577888, D: 0.02986149) (0.224 sec/batch)\n",
      "2017-04-05 22:22:29.412163: step 28760, loss = (G: 5.09959364, D: 0.09135386) (0.225 sec/batch)\n",
      "2017-04-05 22:22:33.894155: step 28780, loss = (G: 9.45503235, D: 0.03161638) (0.225 sec/batch)\n",
      "2017-04-05 22:22:38.384722: step 28800, loss = (G: 6.19513369, D: 0.02192518) (0.224 sec/batch)\n",
      "2017-04-05 22:22:43.013461: step 28820, loss = (G: 5.61389208, D: 0.03103218) (0.224 sec/batch)\n",
      "2017-04-05 22:22:47.537485: step 28840, loss = (G: 11.01320553, D: 0.00459825) (0.224 sec/batch)\n",
      "2017-04-05 22:22:52.032808: step 28860, loss = (G: 7.68392944, D: 0.03653958) (0.225 sec/batch)\n",
      "2017-04-05 22:22:56.514280: step 28880, loss = (G: 5.91100979, D: 0.02270992) (0.224 sec/batch)\n",
      "2017-04-05 22:23:00.999904: step 28900, loss = (G: 6.54305649, D: 0.09685408) (0.227 sec/batch)\n",
      "2017-04-05 22:23:05.655858: step 28920, loss = (G: 5.17120981, D: 0.05860451) (0.225 sec/batch)\n",
      "2017-04-05 22:23:10.141543: step 28940, loss = (G: 5.01049328, D: 0.04431206) (0.225 sec/batch)\n",
      "2017-04-05 22:23:14.642842: step 28960, loss = (G: 5.67694092, D: 0.04168360) (0.224 sec/batch)\n",
      "2017-04-05 22:23:19.280669: step 28980, loss = (G: 6.73178768, D: 0.04333881) (0.224 sec/batch)\n",
      "2017-04-05 22:23:23.764998: step 29000, loss = (G: 4.85215521, D: 0.15829229) (0.223 sec/batch)\n",
      "2017-04-05 22:23:28.407364: step 29020, loss = (G: 6.44264746, D: 0.02138098) (0.229 sec/batch)\n",
      "2017-04-05 22:23:32.895839: step 29040, loss = (G: 7.60171318, D: 0.09085733) (0.223 sec/batch)\n",
      "2017-04-05 22:23:37.395548: step 29060, loss = (G: 5.08470440, D: 0.16535157) (0.225 sec/batch)\n",
      "2017-04-05 22:23:41.902188: step 29080, loss = (G: 10.02394295, D: 0.09442414) (0.225 sec/batch)\n",
      "2017-04-05 22:23:46.421907: step 29100, loss = (G: 8.89185619, D: 0.04228305) (0.224 sec/batch)\n",
      "2017-04-05 22:23:51.051556: step 29120, loss = (G: 10.62980652, D: 0.00355471) (0.224 sec/batch)\n",
      "2017-04-05 22:23:55.540482: step 29140, loss = (G: 6.29694176, D: 0.08179448) (0.223 sec/batch)\n",
      "2017-04-05 22:24:00.028356: step 29160, loss = (G: 4.10242558, D: 0.10153042) (0.224 sec/batch)\n",
      "2017-04-05 22:24:04.513936: step 29180, loss = (G: 5.87882662, D: 0.02572364) (0.225 sec/batch)\n",
      "2017-04-05 22:24:09.006088: step 29200, loss = (G: 10.02252388, D: 0.01644788) (0.225 sec/batch)\n",
      "2017-04-05 22:24:13.640733: step 29220, loss = (G: 6.05947447, D: 0.05211556) (0.225 sec/batch)\n",
      "2017-04-05 22:24:18.121673: step 29240, loss = (G: 7.39884138, D: 0.02790892) (0.223 sec/batch)\n",
      "2017-04-05 22:24:22.648040: step 29260, loss = (G: 8.62337685, D: 0.23895721) (0.229 sec/batch)\n",
      "2017-04-05 22:24:27.136039: step 29280, loss = (G: 6.50066280, D: 0.02014481) (0.223 sec/batch)\n",
      "2017-04-05 22:24:31.621604: step 29300, loss = (G: 10.39345264, D: 0.10130358) (0.225 sec/batch)\n",
      "2017-04-05 22:24:36.260720: step 29320, loss = (G: 4.31732988, D: 0.17648737) (0.226 sec/batch)\n",
      "2017-04-05 22:24:40.753571: step 29340, loss = (G: 3.40732765, D: 0.19739157) (0.224 sec/batch)\n",
      "2017-04-05 22:24:45.242701: step 29360, loss = (G: 4.15434980, D: 0.19921443) (0.224 sec/batch)\n",
      "2017-04-05 22:24:49.730639: step 29380, loss = (G: 8.63723278, D: 0.14877094) (0.223 sec/batch)\n",
      "2017-04-05 22:24:54.251474: step 29400, loss = (G: 6.99055529, D: 0.01580083) (0.225 sec/batch)\n",
      "2017-04-05 22:24:58.888168: step 29420, loss = (G: 5.67957735, D: 0.05044649) (0.226 sec/batch)\n",
      "2017-04-05 22:25:03.427669: step 29440, loss = (G: 7.44196129, D: 0.15165478) (0.223 sec/batch)\n",
      "2017-04-05 22:25:07.913736: step 29460, loss = (G: 8.32027626, D: 0.00613244) (0.225 sec/batch)\n",
      "2017-04-05 22:25:12.438703: step 29480, loss = (G: 5.73180914, D: 0.40558100) (0.226 sec/batch)\n",
      "2017-04-05 22:25:16.926297: step 29500, loss = (G: 8.09369850, D: 0.04711499) (0.225 sec/batch)\n",
      "2017-04-05 22:25:21.543364: step 29520, loss = (G: 6.53173256, D: 0.09833135) (0.223 sec/batch)\n",
      "2017-04-05 22:25:26.032449: step 29540, loss = (G: 5.56916332, D: 0.06828838) (0.223 sec/batch)\n",
      "2017-04-05 22:25:30.521560: step 29560, loss = (G: 5.82135487, D: 0.07950547) (0.224 sec/batch)\n",
      "2017-04-05 22:25:35.008413: step 29580, loss = (G: 6.90316057, D: 0.04313290) (0.225 sec/batch)\n",
      "2017-04-05 22:25:39.497481: step 29600, loss = (G: 6.98691511, D: 0.01882016) (0.225 sec/batch)\n",
      "2017-04-05 22:25:44.128241: step 29620, loss = (G: 5.19349861, D: 0.06455898) (0.223 sec/batch)\n",
      "2017-04-05 22:25:48.621112: step 29640, loss = (G: 6.24173450, D: 0.07396046) (0.225 sec/batch)\n",
      "2017-04-05 22:25:53.103487: step 29660, loss = (G: 6.08052444, D: 0.10503244) (0.225 sec/batch)\n",
      "2017-04-05 22:25:57.596994: step 29680, loss = (G: 7.03096819, D: 0.11750668) (0.224 sec/batch)\n",
      "2017-04-05 22:26:02.102186: step 29700, loss = (G: 4.70215416, D: 0.07847088) (0.227 sec/batch)\n",
      "2017-04-05 22:26:06.737944: step 29720, loss = (G: 6.85138035, D: 0.08199215) (0.224 sec/batch)\n",
      "2017-04-05 22:26:11.229974: step 29740, loss = (G: 5.26297760, D: 0.05215979) (0.225 sec/batch)\n",
      "2017-04-05 22:26:15.724713: step 29760, loss = (G: 6.36029816, D: 0.04571896) (0.227 sec/batch)\n",
      "2017-04-05 22:26:20.210128: step 29780, loss = (G: 2.52878118, D: 1.15392530) (0.225 sec/batch)\n",
      "2017-04-05 22:26:24.723972: step 29800, loss = (G: 5.55698061, D: 0.19237073) (0.229 sec/batch)\n",
      "2017-04-05 22:26:29.353455: step 29820, loss = (G: 5.78998470, D: 0.02952937) (0.224 sec/batch)\n",
      "2017-04-05 22:26:33.836223: step 29840, loss = (G: 6.06373882, D: 0.02736283) (0.224 sec/batch)\n",
      "2017-04-05 22:26:38.317795: step 29860, loss = (G: 15.75408363, D: 2.41251254) (0.224 sec/batch)\n",
      "2017-04-05 22:26:42.804396: step 29880, loss = (G: 6.32623196, D: 0.02842789) (0.223 sec/batch)\n",
      "2017-04-05 22:26:47.294535: step 29900, loss = (G: 4.05915070, D: 0.15327828) (0.225 sec/batch)\n",
      "2017-04-05 22:26:51.978348: step 29920, loss = (G: 6.14691544, D: 0.03071205) (0.224 sec/batch)\n",
      "2017-04-05 22:26:56.466037: step 29940, loss = (G: 3.16913509, D: 0.92513913) (0.225 sec/batch)\n",
      "2017-04-05 22:27:00.975498: step 29960, loss = (G: 8.97353458, D: 0.39094627) (0.226 sec/batch)\n",
      "2017-04-05 22:27:05.482093: step 29980, loss = (G: 3.80310845, D: 0.13405354) (0.224 sec/batch)\n",
      "2017-04-05 22:27:09.961041: step 30000, loss = (G: 5.98162651, D: 0.05967085) (0.225 sec/batch)\n",
      "2017-04-05 22:27:14.588496: step 30020, loss = (G: 9.28999615, D: 0.00533323) (0.224 sec/batch)\n",
      "2017-04-05 22:27:19.085734: step 30040, loss = (G: 6.64537811, D: 0.05561275) (0.225 sec/batch)\n",
      "2017-04-05 22:27:23.577746: step 30060, loss = (G: 8.50497723, D: 0.02206010) (0.225 sec/batch)\n",
      "2017-04-05 22:27:28.062680: step 30080, loss = (G: 5.30703259, D: 0.07138503) (0.225 sec/batch)\n",
      "2017-04-05 22:27:32.552675: step 30100, loss = (G: 7.11445141, D: 0.05812906) (0.225 sec/batch)\n",
      "2017-04-05 22:27:37.186360: step 30120, loss = (G: 3.77424812, D: 0.15156567) (0.225 sec/batch)\n",
      "2017-04-05 22:27:41.808838: step 30140, loss = (G: 5.83992720, D: 0.04168633) (0.224 sec/batch)\n",
      "2017-04-05 22:27:46.291898: step 30160, loss = (G: 6.22603035, D: 0.03318371) (0.225 sec/batch)\n",
      "2017-04-05 22:27:50.778147: step 30180, loss = (G: 4.99736023, D: 0.13224554) (0.225 sec/batch)\n",
      "2017-04-05 22:27:55.266593: step 30200, loss = (G: 14.55083370, D: 0.17532872) (0.226 sec/batch)\n",
      "2017-04-05 22:27:59.928485: step 30220, loss = (G: 6.96199417, D: 0.01689640) (0.226 sec/batch)\n",
      "2017-04-05 22:28:04.413236: step 30240, loss = (G: 9.50999355, D: 0.01926774) (0.224 sec/batch)\n",
      "2017-04-05 22:28:08.904474: step 30260, loss = (G: 5.58581352, D: 0.06613518) (0.224 sec/batch)\n",
      "2017-04-05 22:28:13.402895: step 30280, loss = (G: 3.87238789, D: 0.13063769) (0.230 sec/batch)\n",
      "2017-04-05 22:28:17.942698: step 30300, loss = (G: 6.50758696, D: 0.09057198) (0.225 sec/batch)\n",
      "2017-04-05 22:28:22.563300: step 30320, loss = (G: 7.06624937, D: 0.02067981) (0.225 sec/batch)\n",
      "2017-04-05 22:28:27.049690: step 30340, loss = (G: 11.77292919, D: 0.00643458) (0.225 sec/batch)\n",
      "2017-04-05 22:28:31.534861: step 30360, loss = (G: 7.02648544, D: 0.17173189) (0.223 sec/batch)\n",
      "2017-04-05 22:28:36.059684: step 30380, loss = (G: 10.09817791, D: 0.12739822) (0.227 sec/batch)\n",
      "2017-04-05 22:28:40.545059: step 30400, loss = (G: 5.84766102, D: 0.07704408) (0.224 sec/batch)\n",
      "2017-04-05 22:28:45.202133: step 30420, loss = (G: 9.67037010, D: 0.00313353) (0.227 sec/batch)\n",
      "2017-04-05 22:28:49.686766: step 30440, loss = (G: 4.73194599, D: 0.08960985) (0.225 sec/batch)\n",
      "2017-04-05 22:28:54.172632: step 30460, loss = (G: 10.36011219, D: 0.76226294) (0.225 sec/batch)\n",
      "2017-04-05 22:28:58.658479: step 30480, loss = (G: 5.68890667, D: 0.02660898) (0.224 sec/batch)\n",
      "2017-04-05 22:29:03.145720: step 30500, loss = (G: 5.31778955, D: 0.04469805) (0.226 sec/batch)\n",
      "2017-04-05 22:29:07.770082: step 30520, loss = (G: 11.51086712, D: 0.25379717) (0.224 sec/batch)\n",
      "2017-04-05 22:29:12.313058: step 30540, loss = (G: 6.37824392, D: 0.04627118) (0.224 sec/batch)\n",
      "2017-04-05 22:29:16.796615: step 30560, loss = (G: 5.83343315, D: 0.03580616) (0.224 sec/batch)\n",
      "2017-04-05 22:29:21.286433: step 30580, loss = (G: 13.01660061, D: 0.00209133) (0.225 sec/batch)\n",
      "2017-04-05 22:29:25.803226: step 30600, loss = (G: 5.96787357, D: 0.02987755) (0.223 sec/batch)\n",
      "2017-04-05 22:29:30.424875: step 30620, loss = (G: 6.75689125, D: 0.01105468) (0.225 sec/batch)\n",
      "2017-04-05 22:29:34.910484: step 30640, loss = (G: 4.76172018, D: 0.07434225) (0.224 sec/batch)\n",
      "2017-04-05 22:29:39.403067: step 30660, loss = (G: 8.02506542, D: 0.01506965) (0.225 sec/batch)\n",
      "2017-04-05 22:29:43.901223: step 30680, loss = (G: 4.84083986, D: 0.12544224) (0.224 sec/batch)\n",
      "2017-04-05 22:29:48.392823: step 30700, loss = (G: 3.76755810, D: 0.20610930) (0.224 sec/batch)\n",
      "2017-04-05 22:29:53.018827: step 30720, loss = (G: 7.39616060, D: 0.33911675) (0.225 sec/batch)\n",
      "2017-04-05 22:29:57.514029: step 30740, loss = (G: 9.29600906, D: 0.02074401) (0.226 sec/batch)\n",
      "2017-04-05 22:30:02.005312: step 30760, loss = (G: 9.58829308, D: 0.01562167) (0.226 sec/batch)\n",
      "2017-04-05 22:30:06.487371: step 30780, loss = (G: 8.79348087, D: 0.01138722) (0.224 sec/batch)\n",
      "2017-04-05 22:30:10.970710: step 30800, loss = (G: 3.62050962, D: 0.23710658) (0.224 sec/batch)\n",
      "2017-04-05 22:30:15.600333: step 30820, loss = (G: 7.47543526, D: 0.02478741) (0.224 sec/batch)\n",
      "2017-04-05 22:30:20.088377: step 30840, loss = (G: 3.95510769, D: 0.09242143) (0.224 sec/batch)\n",
      "2017-04-05 22:30:24.582908: step 30860, loss = (G: 8.07135296, D: 0.01937419) (0.225 sec/batch)\n",
      "2017-04-05 22:30:29.072594: step 30880, loss = (G: 7.43243313, D: 0.10046974) (0.224 sec/batch)\n",
      "2017-04-05 22:30:33.594220: step 30900, loss = (G: 8.47744179, D: 0.06441981) (0.229 sec/batch)\n",
      "2017-04-05 22:30:38.219359: step 30920, loss = (G: 12.36406803, D: 0.02068584) (0.223 sec/batch)\n",
      "2017-04-05 22:30:42.718829: step 30940, loss = (G: 4.18521261, D: 0.25444934) (0.225 sec/batch)\n",
      "2017-04-05 22:30:47.218437: step 30960, loss = (G: 8.42919254, D: 0.04974581) (0.224 sec/batch)\n",
      "2017-04-05 22:30:51.712539: step 30980, loss = (G: 8.28889084, D: 0.02176536) (0.225 sec/batch)\n",
      "2017-04-05 22:30:56.197243: step 31000, loss = (G: 7.50892639, D: 0.00582391) (0.225 sec/batch)\n",
      "2017-04-05 22:31:00.829862: step 31020, loss = (G: 8.07792282, D: 0.08167404) (0.226 sec/batch)\n",
      "2017-04-05 22:31:05.315934: step 31040, loss = (G: 6.98080444, D: 0.01823293) (0.225 sec/batch)\n",
      "2017-04-05 22:31:09.795480: step 31060, loss = (G: 4.98364687, D: 0.05116113) (0.223 sec/batch)\n",
      "2017-04-05 22:31:14.338047: step 31080, loss = (G: 8.11817265, D: 0.00544832) (0.224 sec/batch)\n",
      "2017-04-05 22:31:18.821675: step 31100, loss = (G: 10.10569000, D: 0.11757942) (0.224 sec/batch)\n",
      "2017-04-05 22:31:23.442561: step 31120, loss = (G: 6.00824976, D: 0.63905221) (0.224 sec/batch)\n",
      "2017-04-05 22:31:27.923250: step 31140, loss = (G: 8.39044571, D: 0.19854391) (0.224 sec/batch)\n",
      "2017-04-05 22:31:32.418569: step 31160, loss = (G: 6.82479858, D: 0.01217614) (0.225 sec/batch)\n",
      "2017-04-05 22:31:36.905555: step 31180, loss = (G: 7.88003731, D: 0.01201666) (0.225 sec/batch)\n",
      "2017-04-05 22:31:41.383630: step 31200, loss = (G: 13.15671444, D: 0.07796529) (0.224 sec/batch)\n",
      "2017-04-05 22:31:46.046692: step 31220, loss = (G: 8.57829857, D: 0.01068729) (0.232 sec/batch)\n",
      "2017-04-05 22:31:50.551865: step 31240, loss = (G: 8.25964832, D: 0.02889045) (0.223 sec/batch)\n",
      "2017-04-05 22:31:55.036750: step 31260, loss = (G: 7.67858315, D: 0.00804451) (0.223 sec/batch)\n",
      "2017-04-05 22:31:59.530680: step 31280, loss = (G: 11.17488575, D: 0.01012786) (0.224 sec/batch)\n",
      "2017-04-05 22:32:04.197374: step 31300, loss = (G: 5.17057180, D: 0.06081705) (0.225 sec/batch)\n",
      "2017-04-05 22:32:08.832615: step 31320, loss = (G: 6.90739059, D: 0.01425865) (0.225 sec/batch)\n",
      "2017-04-05 22:32:13.365883: step 31340, loss = (G: 9.70227051, D: 0.02055933) (0.226 sec/batch)\n",
      "2017-04-05 22:32:17.848974: step 31360, loss = (G: 6.40706062, D: 0.01784877) (0.224 sec/batch)\n",
      "2017-04-05 22:32:22.334832: step 31380, loss = (G: 8.48755360, D: 0.08865215) (0.224 sec/batch)\n",
      "2017-04-05 22:32:26.830874: step 31400, loss = (G: 10.08592415, D: 0.00237874) (0.226 sec/batch)\n",
      "2017-04-05 22:32:31.451796: step 31420, loss = (G: 5.32222366, D: 0.06912571) (0.225 sec/batch)\n",
      "2017-04-05 22:32:35.944460: step 31440, loss = (G: 6.83695698, D: 0.02041529) (0.225 sec/batch)\n",
      "2017-04-05 22:32:40.433788: step 31460, loss = (G: 21.18097305, D: 0.02668831) (0.225 sec/batch)\n",
      "2017-04-05 22:32:44.933676: step 31480, loss = (G: 6.05164957, D: 0.02968393) (0.225 sec/batch)\n",
      "2017-04-05 22:32:49.424608: step 31500, loss = (G: 7.27250004, D: 0.02317852) (0.224 sec/batch)\n",
      "2017-04-05 22:32:54.051824: step 31520, loss = (G: 10.25400734, D: 0.00176326) (0.224 sec/batch)\n",
      "2017-04-05 22:32:58.547386: step 31540, loss = (G: 8.83039570, D: 0.00612772) (0.225 sec/batch)\n",
      "2017-04-05 22:33:03.037708: step 31560, loss = (G: 11.10033321, D: 0.00327019) (0.224 sec/batch)\n",
      "2017-04-05 22:33:07.516969: step 31580, loss = (G: 14.99758053, D: 0.00584649) (0.225 sec/batch)\n",
      "2017-04-05 22:33:11.999702: step 31600, loss = (G: 4.99898863, D: 0.07362290) (0.223 sec/batch)\n",
      "2017-04-05 22:33:16.670656: step 31620, loss = (G: 7.48939419, D: 0.01805889) (0.226 sec/batch)\n",
      "2017-04-05 22:33:21.158907: step 31640, loss = (G: 8.74410725, D: 0.00653217) (0.225 sec/batch)\n",
      "2017-04-05 22:33:25.642427: step 31660, loss = (G: 7.15097523, D: 0.02495074) (0.223 sec/batch)\n",
      "2017-04-05 22:33:30.132131: step 31680, loss = (G: 6.60448837, D: 0.02689909) (0.224 sec/batch)\n",
      "2017-04-05 22:33:34.627089: step 31700, loss = (G: 6.44016552, D: 0.09309393) (0.224 sec/batch)\n",
      "2017-04-05 22:33:39.255292: step 31720, loss = (G: 6.49630165, D: 0.06811324) (0.225 sec/batch)\n",
      "2017-04-05 22:33:43.745357: step 31740, loss = (G: 6.39498186, D: 0.39111394) (0.225 sec/batch)\n",
      "2017-04-05 22:33:48.239948: step 31760, loss = (G: 9.08907223, D: 0.05435270) (0.223 sec/batch)\n",
      "2017-04-05 22:33:52.737228: step 31780, loss = (G: 6.33772039, D: 0.02469284) (0.224 sec/batch)\n",
      "2017-04-05 22:33:57.238234: step 31800, loss = (G: 8.92126083, D: 0.00775773) (0.225 sec/batch)\n",
      "2017-04-05 22:34:01.878080: step 31820, loss = (G: 5.13907862, D: 0.18134746) (0.225 sec/batch)\n",
      "2017-04-05 22:34:06.368443: step 31840, loss = (G: 3.89537525, D: 0.14016052) (0.225 sec/batch)\n",
      "2017-04-05 22:34:10.857894: step 31860, loss = (G: 8.37969398, D: 0.09458344) (0.226 sec/batch)\n",
      "2017-04-05 22:34:15.361122: step 31880, loss = (G: 3.87867832, D: 0.20969003) (0.227 sec/batch)\n",
      "2017-04-05 22:34:19.849920: step 31900, loss = (G: 6.57962418, D: 0.02405447) (0.225 sec/batch)\n",
      "2017-04-05 22:34:24.477650: step 31920, loss = (G: 9.56992817, D: 0.03857607) (0.225 sec/batch)\n",
      "2017-04-05 22:34:28.978280: step 31940, loss = (G: 5.78110027, D: 0.02809628) (0.229 sec/batch)\n",
      "2017-04-05 22:34:33.469383: step 31960, loss = (G: 6.52035427, D: 0.01555285) (0.224 sec/batch)\n",
      "2017-04-05 22:34:37.967003: step 31980, loss = (G: 5.98462439, D: 0.02086078) (0.224 sec/batch)\n",
      "2017-04-05 22:34:42.455047: step 32000, loss = (G: 5.05910254, D: 0.04118610) (0.225 sec/batch)\n",
      "2017-04-05 22:34:47.103903: step 32020, loss = (G: 6.25388241, D: 0.03144305) (0.226 sec/batch)\n",
      "2017-04-05 22:34:51.587946: step 32040, loss = (G: 5.58091116, D: 0.04248065) (0.223 sec/batch)\n",
      "2017-04-05 22:34:56.070171: step 32060, loss = (G: 8.40461349, D: 0.01281017) (0.224 sec/batch)\n",
      "2017-04-05 22:35:00.562589: step 32080, loss = (G: 5.81616163, D: 0.01907323) (0.226 sec/batch)\n",
      "2017-04-05 22:35:05.060846: step 32100, loss = (G: 10.15298939, D: 0.29696864) (0.224 sec/batch)\n",
      "2017-04-05 22:35:09.700146: step 32120, loss = (G: 8.43739223, D: 0.07481764) (0.224 sec/batch)\n",
      "2017-04-05 22:35:14.221641: step 32140, loss = (G: 7.75777245, D: 0.06074059) (0.229 sec/batch)\n",
      "2017-04-05 22:35:18.735446: step 32160, loss = (G: 6.93651009, D: 0.01762052) (0.225 sec/batch)\n",
      "2017-04-05 22:35:23.220283: step 32180, loss = (G: 5.13527346, D: 0.05606197) (0.225 sec/batch)\n",
      "2017-04-05 22:35:27.743113: step 32200, loss = (G: 14.32803059, D: 0.98532844) (0.230 sec/batch)\n",
      "2017-04-05 22:35:32.378304: step 32220, loss = (G: 5.30641794, D: 0.05267918) (0.224 sec/batch)\n",
      "2017-04-05 22:35:36.868710: step 32240, loss = (G: 5.98596811, D: 0.03426315) (0.224 sec/batch)\n",
      "2017-04-05 22:35:41.350937: step 32260, loss = (G: 8.36738110, D: 0.02557857) (0.225 sec/batch)\n",
      "2017-04-05 22:35:45.849480: step 32280, loss = (G: 6.36614990, D: 0.01744175) (0.224 sec/batch)\n",
      "2017-04-05 22:35:50.339753: step 32300, loss = (G: 4.71483088, D: 0.09587014) (0.225 sec/batch)\n",
      "2017-04-05 22:35:55.006009: step 32320, loss = (G: 9.12286186, D: 0.00917209) (0.224 sec/batch)\n",
      "2017-04-05 22:35:59.493519: step 32340, loss = (G: 4.34234905, D: 0.21432593) (0.224 sec/batch)\n",
      "2017-04-05 22:36:03.994775: step 32360, loss = (G: 4.96002865, D: 0.35293987) (0.224 sec/batch)\n",
      "2017-04-05 22:36:08.491585: step 32380, loss = (G: 8.30391121, D: 0.03494225) (0.224 sec/batch)\n",
      "2017-04-05 22:36:12.971834: step 32400, loss = (G: 6.17812061, D: 0.04865940) (0.223 sec/batch)\n",
      "2017-04-05 22:36:17.617189: step 32420, loss = (G: 8.11437225, D: 0.29672527) (0.224 sec/batch)\n",
      "2017-04-05 22:36:22.103480: step 32440, loss = (G: 5.32743263, D: 0.05410601) (0.224 sec/batch)\n",
      "2017-04-05 22:36:26.735712: step 32460, loss = (G: 8.72040844, D: 0.02672385) (0.224 sec/batch)\n",
      "2017-04-05 22:36:31.217120: step 32480, loss = (G: 4.35952568, D: 0.15429297) (0.224 sec/batch)\n",
      "2017-04-05 22:36:35.700595: step 32500, loss = (G: 7.55340862, D: 0.00735294) (0.224 sec/batch)\n",
      "2017-04-05 22:36:40.333262: step 32520, loss = (G: 7.95167828, D: 0.01660543) (0.225 sec/batch)\n",
      "2017-04-05 22:36:44.840914: step 32540, loss = (G: 8.41858482, D: 0.01153655) (0.224 sec/batch)\n",
      "2017-04-05 22:36:49.333015: step 32560, loss = (G: 6.73196697, D: 0.02246940) (0.224 sec/batch)\n",
      "2017-04-05 22:36:53.823896: step 32580, loss = (G: 5.84211826, D: 0.04732694) (0.224 sec/batch)\n",
      "2017-04-05 22:36:58.321844: step 32600, loss = (G: 5.29928923, D: 0.04346789) (0.224 sec/batch)\n",
      "2017-04-05 22:37:02.965482: step 32620, loss = (G: 3.92418146, D: 0.20864415) (0.223 sec/batch)\n",
      "2017-04-05 22:37:07.477549: step 32640, loss = (G: 8.73554802, D: 0.04217473) (0.225 sec/batch)\n",
      "2017-04-05 22:37:11.967506: step 32660, loss = (G: 4.24141979, D: 0.17545716) (0.223 sec/batch)\n",
      "2017-04-05 22:37:16.460894: step 32680, loss = (G: 12.23258591, D: 0.00040282) (0.225 sec/batch)\n",
      "2017-04-05 22:37:20.945225: step 32700, loss = (G: 5.55134630, D: 0.02632145) (0.225 sec/batch)\n",
      "2017-04-05 22:37:25.579391: step 32720, loss = (G: 6.41663742, D: 0.02300772) (0.225 sec/batch)\n",
      "2017-04-05 22:37:30.077178: step 32740, loss = (G: 6.33602762, D: 0.03417499) (0.225 sec/batch)\n",
      "2017-04-05 22:37:34.559255: step 32760, loss = (G: 6.19508648, D: 0.04721439) (0.224 sec/batch)\n",
      "2017-04-05 22:37:39.041805: step 32780, loss = (G: 2.26663756, D: 1.39008284) (0.223 sec/batch)\n",
      "2017-04-05 22:37:43.541387: step 32800, loss = (G: 5.52610683, D: 0.04438782) (0.225 sec/batch)\n",
      "2017-04-05 22:37:48.169130: step 32820, loss = (G: 7.79799223, D: 0.03028060) (0.225 sec/batch)\n",
      "2017-04-05 22:37:52.651699: step 32840, loss = (G: 11.47183990, D: 0.97755581) (0.224 sec/batch)\n",
      "2017-04-05 22:37:57.154836: step 32860, loss = (G: 7.97599316, D: 0.01900880) (0.226 sec/batch)\n",
      "2017-04-05 22:38:01.651655: step 32880, loss = (G: 7.83719969, D: 0.09151071) (0.225 sec/batch)\n",
      "2017-04-05 22:38:06.135436: step 32900, loss = (G: 7.21222448, D: 0.09069368) (0.224 sec/batch)\n",
      "2017-04-05 22:38:10.758028: step 32920, loss = (G: 8.93823528, D: 0.02925049) (0.223 sec/batch)\n",
      "2017-04-05 22:38:15.301572: step 32940, loss = (G: 6.73182869, D: 0.07274668) (0.222 sec/batch)\n",
      "2017-04-05 22:38:19.785664: step 32960, loss = (G: 7.20140123, D: 0.03278096) (0.224 sec/batch)\n",
      "2017-04-05 22:38:24.282442: step 32980, loss = (G: 7.31377602, D: 0.02535688) (0.225 sec/batch)\n",
      "2017-04-05 22:38:28.774359: step 33000, loss = (G: 6.53766823, D: 0.01695844) (0.227 sec/batch)\n",
      "2017-04-05 22:38:33.445186: step 33020, loss = (G: 5.93509293, D: 0.04219636) (0.226 sec/batch)\n",
      "2017-04-05 22:38:37.935120: step 33040, loss = (G: 8.53194523, D: 0.35930607) (0.225 sec/batch)\n",
      "2017-04-05 22:38:42.426961: step 33060, loss = (G: 6.49563360, D: 0.03107747) (0.225 sec/batch)\n",
      "2017-04-05 22:38:46.918091: step 33080, loss = (G: 11.57276058, D: 0.00779625) (0.224 sec/batch)\n",
      "2017-04-05 22:38:51.412526: step 33100, loss = (G: 4.96285295, D: 0.05887816) (0.225 sec/batch)\n",
      "2017-04-05 22:38:56.080672: step 33120, loss = (G: 6.18037033, D: 0.02638257) (0.224 sec/batch)\n",
      "2017-04-05 22:39:00.562720: step 33140, loss = (G: 5.15771484, D: 0.05702473) (0.224 sec/batch)\n",
      "2017-04-05 22:39:05.047797: step 33160, loss = (G: 9.71568012, D: 0.02008877) (0.225 sec/batch)\n",
      "2017-04-05 22:39:09.532070: step 33180, loss = (G: 16.26443291, D: 0.30877024) (0.223 sec/batch)\n",
      "2017-04-05 22:39:14.028138: step 33200, loss = (G: 5.40297413, D: 0.03007414) (0.228 sec/batch)\n",
      "2017-04-05 22:39:18.662814: step 33220, loss = (G: 5.19668102, D: 0.05809835) (0.224 sec/batch)\n",
      "2017-04-05 22:39:23.145351: step 33240, loss = (G: 6.47412014, D: 0.10372050) (0.224 sec/batch)\n",
      "2017-04-05 22:39:27.645895: step 33260, loss = (G: 6.57918453, D: 0.04322906) (0.225 sec/batch)\n",
      "2017-04-05 22:39:32.127956: step 33280, loss = (G: 8.69258976, D: 0.05834644) (0.224 sec/batch)\n",
      "2017-04-05 22:39:36.628642: step 33300, loss = (G: 8.52687645, D: 0.03295738) (0.224 sec/batch)\n",
      "2017-04-05 22:39:41.265604: step 33320, loss = (G: 6.75671101, D: 0.01997577) (0.225 sec/batch)\n",
      "2017-04-05 22:39:45.778940: step 33340, loss = (G: 9.09444809, D: 0.03370043) (0.225 sec/batch)\n",
      "2017-04-05 22:39:50.269434: step 33360, loss = (G: 7.67282295, D: 0.02703123) (0.227 sec/batch)\n",
      "2017-04-05 22:39:54.757996: step 33380, loss = (G: 7.91291332, D: 0.00861553) (0.224 sec/batch)\n",
      "2017-04-05 22:39:59.245011: step 33400, loss = (G: 5.99864960, D: 0.02747278) (0.223 sec/batch)\n",
      "2017-04-05 22:40:03.871651: step 33420, loss = (G: 5.42755127, D: 0.12920800) (0.224 sec/batch)\n",
      "2017-04-05 22:40:08.366965: step 33440, loss = (G: 7.65508127, D: 0.01670552) (0.225 sec/batch)\n",
      "2017-04-05 22:40:12.860330: step 33460, loss = (G: 6.04986048, D: 0.06335799) (0.224 sec/batch)\n",
      "2017-04-05 22:40:17.347941: step 33480, loss = (G: 8.51578331, D: 0.03067433) (0.224 sec/batch)\n",
      "2017-04-05 22:40:21.836015: step 33500, loss = (G: 3.84687901, D: 0.15934399) (0.224 sec/batch)\n",
      "2017-04-05 22:40:26.467303: step 33520, loss = (G: 5.99942875, D: 0.01952695) (0.227 sec/batch)\n",
      "2017-04-05 22:40:30.963560: step 33540, loss = (G: 6.66362000, D: 0.08102803) (0.223 sec/batch)\n",
      "2017-04-05 22:40:35.451297: step 33560, loss = (G: 6.22327948, D: 0.02542958) (0.224 sec/batch)\n",
      "2017-04-05 22:40:39.959895: step 33580, loss = (G: 4.63433743, D: 0.15785404) (0.225 sec/batch)\n",
      "2017-04-05 22:40:44.448272: step 33600, loss = (G: 5.62057018, D: 0.06463859) (0.225 sec/batch)\n",
      "2017-04-05 22:40:49.219547: step 33620, loss = (G: 12.87006569, D: 0.00521807) (0.225 sec/batch)\n",
      "2017-04-05 22:40:53.704112: step 33640, loss = (G: 9.21373367, D: 0.08272831) (0.225 sec/batch)\n",
      "2017-04-05 22:40:58.239251: step 33660, loss = (G: 5.12105751, D: 0.04676386) (0.225 sec/batch)\n",
      "2017-04-05 22:41:02.719819: step 33680, loss = (G: 4.99092817, D: 0.05971071) (0.224 sec/batch)\n",
      "2017-04-05 22:41:07.215127: step 33700, loss = (G: 6.87001276, D: 0.02035427) (0.224 sec/batch)\n",
      "2017-04-05 22:41:11.840805: step 33720, loss = (G: 7.73543596, D: 0.00603195) (0.225 sec/batch)\n",
      "2017-04-05 22:41:16.358804: step 33740, loss = (G: 6.36711693, D: 0.06823626) (0.227 sec/batch)\n",
      "2017-04-05 22:41:20.840623: step 33760, loss = (G: 7.53712845, D: 0.03027118) (0.224 sec/batch)\n",
      "2017-04-05 22:41:25.321237: step 33780, loss = (G: 7.85127068, D: 0.10414393) (0.225 sec/batch)\n",
      "2017-04-05 22:41:29.861113: step 33800, loss = (G: 5.71441174, D: 0.05090129) (0.224 sec/batch)\n",
      "2017-04-05 22:41:34.535559: step 33820, loss = (G: 7.55559015, D: 0.06511047) (0.224 sec/batch)\n",
      "2017-04-05 22:41:39.011183: step 33840, loss = (G: 6.08333445, D: 0.02632146) (0.227 sec/batch)\n",
      "2017-04-05 22:41:43.495189: step 33860, loss = (G: 4.85266066, D: 0.07083376) (0.224 sec/batch)\n",
      "2017-04-05 22:41:47.998040: step 33880, loss = (G: 9.70398521, D: 0.02852046) (0.225 sec/batch)\n",
      "2017-04-05 22:41:52.484662: step 33900, loss = (G: 6.71426678, D: 0.01785714) (0.225 sec/batch)\n",
      "2017-04-05 22:41:57.123868: step 33920, loss = (G: 7.48663425, D: 0.09260464) (0.225 sec/batch)\n",
      "2017-04-05 22:42:01.618006: step 33940, loss = (G: 6.91228485, D: 0.03517378) (0.223 sec/batch)\n",
      "2017-04-05 22:42:06.104445: step 33960, loss = (G: 5.28334904, D: 0.18648061) (0.224 sec/batch)\n",
      "2017-04-05 22:42:10.594562: step 33980, loss = (G: 2.78860331, D: 0.97254270) (0.225 sec/batch)\n",
      "2017-04-05 22:42:15.086368: step 34000, loss = (G: 4.76025629, D: 0.05036544) (0.225 sec/batch)\n",
      "2017-04-05 22:42:19.733249: step 34020, loss = (G: 6.03738403, D: 0.10377272) (0.228 sec/batch)\n",
      "2017-04-05 22:42:24.235899: step 34040, loss = (G: 6.75810099, D: 0.02217040) (0.225 sec/batch)\n",
      "2017-04-05 22:42:28.729349: step 34060, loss = (G: 4.88871288, D: 0.08126751) (0.225 sec/batch)\n",
      "2017-04-05 22:42:33.220931: step 34080, loss = (G: 7.64249754, D: 0.03242896) (0.228 sec/batch)\n",
      "2017-04-05 22:42:37.707958: step 34100, loss = (G: 4.26177788, D: 0.09747953) (0.223 sec/batch)\n",
      "2017-04-05 22:42:42.360982: step 34120, loss = (G: 6.28774166, D: 0.01733685) (0.224 sec/batch)\n",
      "2017-04-05 22:42:46.855715: step 34140, loss = (G: 5.67167234, D: 0.03032259) (0.226 sec/batch)\n",
      "2017-04-05 22:42:51.353279: step 34160, loss = (G: 7.84607601, D: 0.14171188) (0.225 sec/batch)\n",
      "2017-04-05 22:42:55.840895: step 34180, loss = (G: 5.97028732, D: 0.01677542) (0.224 sec/batch)\n",
      "2017-04-05 22:43:00.336645: step 34200, loss = (G: 4.34822655, D: 0.08541350) (0.225 sec/batch)\n",
      "2017-04-05 22:43:04.982712: step 34220, loss = (G: 7.52793217, D: 0.00992237) (0.231 sec/batch)\n",
      "2017-04-05 22:43:09.497206: step 34240, loss = (G: 6.71819878, D: 0.01998335) (0.224 sec/batch)\n",
      "2017-04-05 22:43:14.001685: step 34260, loss = (G: 7.79617596, D: 0.01936875) (0.227 sec/batch)\n",
      "2017-04-05 22:43:18.491085: step 34280, loss = (G: 7.17178726, D: 0.01429334) (0.224 sec/batch)\n",
      "2017-04-05 22:43:22.980604: step 34300, loss = (G: 7.71974325, D: 0.46249527) (0.223 sec/batch)\n",
      "2017-04-05 22:43:27.610157: step 34320, loss = (G: 6.87257576, D: 0.03215425) (0.224 sec/batch)\n",
      "2017-04-05 22:43:32.112880: step 34340, loss = (G: 6.88544607, D: 0.01843752) (0.225 sec/batch)\n",
      "2017-04-05 22:43:36.597001: step 34360, loss = (G: 8.06050396, D: 0.04880545) (0.226 sec/batch)\n",
      "2017-04-05 22:43:41.093954: step 34380, loss = (G: 6.87255192, D: 0.03021488) (0.225 sec/batch)\n",
      "2017-04-05 22:43:45.581512: step 34400, loss = (G: 9.31001949, D: 0.03482449) (0.225 sec/batch)\n",
      "2017-04-05 22:43:50.225691: step 34420, loss = (G: 4.78185892, D: 0.11693684) (0.224 sec/batch)\n",
      "2017-04-05 22:43:54.714212: step 34440, loss = (G: 8.45908928, D: 0.08160680) (0.223 sec/batch)\n",
      "2017-04-05 22:43:59.205172: step 34460, loss = (G: 6.49491024, D: 0.05136995) (0.225 sec/batch)\n",
      "2017-04-05 22:44:03.693155: step 34480, loss = (G: 7.04607344, D: 0.02516290) (0.224 sec/batch)\n",
      "2017-04-05 22:44:08.178973: step 34500, loss = (G: 8.26962471, D: 0.01623344) (0.224 sec/batch)\n",
      "2017-04-05 22:44:12.805666: step 34520, loss = (G: 8.33209610, D: 0.00696281) (0.224 sec/batch)\n",
      "2017-04-05 22:44:17.321572: step 34540, loss = (G: 8.15467834, D: 0.00724677) (0.230 sec/batch)\n",
      "2017-04-05 22:44:21.837130: step 34560, loss = (G: 6.15298748, D: 0.02350050) (0.228 sec/batch)\n",
      "2017-04-05 22:44:26.368935: step 34580, loss = (G: 14.35802937, D: 0.07366459) (0.226 sec/batch)\n",
      "2017-04-05 22:44:30.858447: step 34600, loss = (G: 7.05761766, D: 0.02940676) (0.225 sec/batch)\n",
      "2017-04-05 22:44:35.485454: step 34620, loss = (G: 5.14062929, D: 0.12058619) (0.228 sec/batch)\n",
      "2017-04-05 22:44:39.996321: step 34640, loss = (G: 5.71405029, D: 0.04675785) (0.225 sec/batch)\n",
      "2017-04-05 22:44:44.477300: step 34660, loss = (G: 6.36369228, D: 0.01805477) (0.225 sec/batch)\n",
      "2017-04-05 22:44:48.955817: step 34680, loss = (G: 5.80478716, D: 0.03200622) (0.225 sec/batch)\n",
      "2017-04-05 22:44:53.447030: step 34700, loss = (G: 6.00664616, D: 0.02077699) (0.225 sec/batch)\n",
      "2017-04-05 22:44:58.074561: step 34720, loss = (G: 4.90029192, D: 0.05876569) (0.225 sec/batch)\n",
      "2017-04-05 22:45:02.562521: step 34740, loss = (G: 9.03583908, D: 0.07448567) (0.224 sec/batch)\n",
      "2017-04-05 22:45:07.057797: step 34760, loss = (G: 14.91903114, D: 0.03591319) (0.224 sec/batch)\n",
      "2017-04-05 22:45:11.573014: step 34780, loss = (G: 2.40243864, D: 0.75050294) (0.227 sec/batch)\n",
      "2017-04-05 22:45:16.069045: step 34800, loss = (G: 7.27582455, D: 0.21453266) (0.224 sec/batch)\n",
      "2017-04-05 22:45:20.703240: step 34820, loss = (G: 7.03042507, D: 0.12873499) (0.225 sec/batch)\n",
      "2017-04-05 22:45:25.201473: step 34840, loss = (G: 6.18395042, D: 0.02589031) (0.226 sec/batch)\n",
      "2017-04-05 22:45:29.691452: step 34860, loss = (G: 6.07062531, D: 0.09176325) (0.225 sec/batch)\n",
      "2017-04-05 22:45:34.188493: step 34880, loss = (G: 2.08906054, D: 1.40919614) (0.224 sec/batch)\n",
      "2017-04-05 22:45:38.673100: step 34900, loss = (G: 5.98008156, D: 0.13111089) (0.223 sec/batch)\n",
      "2017-04-05 22:45:43.305726: step 34920, loss = (G: 5.46882105, D: 0.05338220) (0.225 sec/batch)\n",
      "2017-04-05 22:45:47.794905: step 34940, loss = (G: 5.92978096, D: 0.16705653) (0.224 sec/batch)\n",
      "2017-04-05 22:45:52.289395: step 34960, loss = (G: 10.38469315, D: 0.02113442) (0.230 sec/batch)\n",
      "2017-04-05 22:45:56.823159: step 34980, loss = (G: 7.16821814, D: 0.02971483) (0.225 sec/batch)\n",
      "2017-04-05 22:46:01.310290: step 35000, loss = (G: 6.39354801, D: 0.02015264) (0.225 sec/batch)\n",
      "2017-04-05 22:46:05.951420: step 35020, loss = (G: 3.35983753, D: 0.78470826) (0.223 sec/batch)\n",
      "2017-04-05 22:46:10.433373: step 35040, loss = (G: 6.10385561, D: 0.02212887) (0.224 sec/batch)\n",
      "2017-04-05 22:46:14.915477: step 35060, loss = (G: 14.20351028, D: 0.18385722) (0.224 sec/batch)\n",
      "2017-04-05 22:46:19.422985: step 35080, loss = (G: 7.68447113, D: 0.02032159) (0.227 sec/batch)\n",
      "2017-04-05 22:46:23.910188: step 35100, loss = (G: 5.45320082, D: 0.06277306) (0.224 sec/batch)\n",
      "2017-04-05 22:46:28.535040: step 35120, loss = (G: 6.67242050, D: 0.17546670) (0.225 sec/batch)\n",
      "2017-04-05 22:46:33.028884: step 35140, loss = (G: 11.14747524, D: 0.24273261) (0.225 sec/batch)\n",
      "2017-04-05 22:46:37.513874: step 35160, loss = (G: 7.09530115, D: 0.04243628) (0.224 sec/batch)\n",
      "2017-04-05 22:46:42.002278: step 35180, loss = (G: 7.66164017, D: 0.00576655) (0.224 sec/batch)\n",
      "2017-04-05 22:46:46.508241: step 35200, loss = (G: 9.56114674, D: 0.00566429) (0.225 sec/batch)\n",
      "2017-04-05 22:46:51.145150: step 35220, loss = (G: 6.48928165, D: 0.02300203) (0.225 sec/batch)\n",
      "2017-04-05 22:46:55.641097: step 35240, loss = (G: 12.03606129, D: 0.02218265) (0.224 sec/batch)\n",
      "2017-04-05 22:47:00.126598: step 35260, loss = (G: 6.55467033, D: 0.21092905) (0.225 sec/batch)\n",
      "2017-04-05 22:47:04.606130: step 35280, loss = (G: 8.84221840, D: 0.12436344) (0.224 sec/batch)\n",
      "2017-04-05 22:47:09.102823: step 35300, loss = (G: 7.67852926, D: 0.07101095) (0.225 sec/batch)\n",
      "2017-04-05 22:47:13.741450: step 35320, loss = (G: 8.37344551, D: 0.00830694) (0.223 sec/batch)\n",
      "2017-04-05 22:47:18.227926: step 35340, loss = (G: 6.60075092, D: 0.02375264) (0.223 sec/batch)\n",
      "2017-04-05 22:47:22.721052: step 35360, loss = (G: 7.97680855, D: 0.00735593) (0.225 sec/batch)\n",
      "2017-04-05 22:47:27.210709: step 35380, loss = (G: 5.11436749, D: 0.05157597) (0.225 sec/batch)\n",
      "2017-04-05 22:47:31.708439: step 35400, loss = (G: 4.97939253, D: 0.04551669) (0.223 sec/batch)\n",
      "2017-04-05 22:47:36.339765: step 35420, loss = (G: 7.72330332, D: 0.00802106) (0.225 sec/batch)\n",
      "2017-04-05 22:47:40.830186: step 35440, loss = (G: 6.13198376, D: 0.01584277) (0.228 sec/batch)\n",
      "2017-04-05 22:47:45.332072: step 35460, loss = (G: 2.26156425, D: 0.95975411) (0.226 sec/batch)\n",
      "2017-04-05 22:47:49.824111: step 35480, loss = (G: 11.26642990, D: 0.81255102) (0.224 sec/batch)\n",
      "2017-04-05 22:47:54.313382: step 35500, loss = (G: 12.98672104, D: 0.08084010) (0.223 sec/batch)\n",
      "2017-04-05 22:47:58.940294: step 35520, loss = (G: 6.51811171, D: 0.02036526) (0.224 sec/batch)\n",
      "2017-04-05 22:48:03.431354: step 35540, loss = (G: 6.07074499, D: 0.17016885) (0.224 sec/batch)\n",
      "2017-04-05 22:48:07.911957: step 35560, loss = (G: 5.20031071, D: 0.04268626) (0.223 sec/batch)\n",
      "2017-04-05 22:48:12.430340: step 35580, loss = (G: 5.79607964, D: 0.04701247) (0.224 sec/batch)\n",
      "2017-04-05 22:48:16.944622: step 35600, loss = (G: 3.86025906, D: 0.14620268) (0.224 sec/batch)\n",
      "2017-04-05 22:48:21.568247: step 35620, loss = (G: 8.38126564, D: 0.00564385) (0.225 sec/batch)\n",
      "2017-04-05 22:48:26.064725: step 35640, loss = (G: 8.59967422, D: 0.00633854) (0.225 sec/batch)\n",
      "2017-04-05 22:48:30.559688: step 35660, loss = (G: 5.83650017, D: 0.02975123) (0.225 sec/batch)\n",
      "2017-04-05 22:48:35.093690: step 35680, loss = (G: 8.14284801, D: 0.00541152) (0.224 sec/batch)\n",
      "2017-04-05 22:48:39.586763: step 35700, loss = (G: 8.83444405, D: 0.00418025) (0.224 sec/batch)\n",
      "2017-04-05 22:48:44.211936: step 35720, loss = (G: 10.29153728, D: 0.01985635) (0.223 sec/batch)\n",
      "2017-04-05 22:48:48.729586: step 35740, loss = (G: 4.28312397, D: 0.13450959) (0.224 sec/batch)\n",
      "2017-04-05 22:48:53.202019: step 35760, loss = (G: 6.80253410, D: 0.02016904) (0.222 sec/batch)\n",
      "2017-04-05 22:48:57.690752: step 35780, loss = (G: 7.19893551, D: 0.62816674) (0.225 sec/batch)\n",
      "2017-04-05 22:49:02.178687: step 35800, loss = (G: 7.14928198, D: 0.02360505) (0.224 sec/batch)\n",
      "2017-04-05 22:49:06.805829: step 35820, loss = (G: 7.34768534, D: 0.20675136) (0.224 sec/batch)\n",
      "2017-04-05 22:49:11.287627: step 35840, loss = (G: 7.49046803, D: 0.01173139) (0.223 sec/batch)\n",
      "2017-04-05 22:49:15.811976: step 35860, loss = (G: 10.11167336, D: 0.00912254) (0.224 sec/batch)\n",
      "2017-04-05 22:49:20.290877: step 35880, loss = (G: 6.69122314, D: 0.01953412) (0.222 sec/batch)\n",
      "2017-04-05 22:49:24.772958: step 35900, loss = (G: 21.34855461, D: 0.11143443) (0.225 sec/batch)\n",
      "2017-04-05 22:49:29.392354: step 35920, loss = (G: 7.90299129, D: 0.02107170) (0.225 sec/batch)\n",
      "2017-04-05 22:49:34.046491: step 35940, loss = (G: 10.30860138, D: 0.07067642) (0.225 sec/batch)\n",
      "2017-04-05 22:49:38.527809: step 35960, loss = (G: 8.43347454, D: 0.02450884) (0.225 sec/batch)\n",
      "2017-04-05 22:49:43.011109: step 35980, loss = (G: 6.61919785, D: 0.02454806) (0.225 sec/batch)\n",
      "2017-04-05 22:49:47.496539: step 36000, loss = (G: 5.14256763, D: 0.13712707) (0.224 sec/batch)\n",
      "2017-04-05 22:49:52.120352: step 36020, loss = (G: 6.94230795, D: 0.03206483) (0.224 sec/batch)\n",
      "2017-04-05 22:49:56.606026: step 36040, loss = (G: 6.97811127, D: 0.14643830) (0.225 sec/batch)\n",
      "2017-04-05 22:50:01.090281: step 36060, loss = (G: 5.74864197, D: 0.07324041) (0.223 sec/batch)\n",
      "2017-04-05 22:50:05.577540: step 36080, loss = (G: 11.13882256, D: 0.00089904) (0.224 sec/batch)\n",
      "2017-04-05 22:50:10.097816: step 36100, loss = (G: 7.78781414, D: 0.00870203) (0.223 sec/batch)\n",
      "2017-04-05 22:50:14.725639: step 36120, loss = (G: 5.25815725, D: 0.05167368) (0.224 sec/batch)\n",
      "2017-04-05 22:50:19.210647: step 36140, loss = (G: 7.40528631, D: 0.00803645) (0.225 sec/batch)\n",
      "2017-04-05 22:50:23.694278: step 36160, loss = (G: 14.16623402, D: 0.00037803) (0.223 sec/batch)\n",
      "2017-04-05 22:50:28.186144: step 36180, loss = (G: 5.12128878, D: 0.06064846) (0.225 sec/batch)\n",
      "2017-04-05 22:50:32.681330: step 36200, loss = (G: 4.53624630, D: 0.85534567) (0.223 sec/batch)\n",
      "2017-04-05 22:50:37.310575: step 36220, loss = (G: 4.48235369, D: 0.09604974) (0.223 sec/batch)\n",
      "2017-04-05 22:50:41.810649: step 36240, loss = (G: 3.41457367, D: 0.28862712) (0.227 sec/batch)\n",
      "2017-04-05 22:50:46.291795: step 36260, loss = (G: 7.09198284, D: 0.01093416) (0.223 sec/batch)\n",
      "2017-04-05 22:50:50.788673: step 36280, loss = (G: 16.61307907, D: 0.23557431) (0.225 sec/batch)\n",
      "2017-04-05 22:50:55.269766: step 36300, loss = (G: 6.82153559, D: 0.01782975) (0.224 sec/batch)\n",
      "2017-04-05 22:50:59.943121: step 36320, loss = (G: 6.24930859, D: 0.06291331) (0.224 sec/batch)\n",
      "2017-04-05 22:51:04.427587: step 36340, loss = (G: 5.77781773, D: 0.04895768) (0.224 sec/batch)\n",
      "2017-04-05 22:51:08.912433: step 36360, loss = (G: 3.84363079, D: 0.37608457) (0.224 sec/batch)\n",
      "2017-04-05 22:51:13.399900: step 36380, loss = (G: 4.59112072, D: 0.06540038) (0.224 sec/batch)\n",
      "2017-04-05 22:51:17.889957: step 36400, loss = (G: 6.79070139, D: 0.04658581) (0.224 sec/batch)\n",
      "2017-04-05 22:51:22.507564: step 36420, loss = (G: 2.12079287, D: 0.86555499) (0.225 sec/batch)\n",
      "2017-04-05 22:51:26.994453: step 36440, loss = (G: 4.80806446, D: 0.08955198) (0.225 sec/batch)\n",
      "2017-04-05 22:51:31.489118: step 36460, loss = (G: 6.50725174, D: 0.02580910) (0.223 sec/batch)\n",
      "2017-04-05 22:51:35.998325: step 36480, loss = (G: 6.32120132, D: 0.03380744) (0.227 sec/batch)\n",
      "2017-04-05 22:51:40.491072: step 36500, loss = (G: 5.05655003, D: 0.12751558) (0.225 sec/batch)\n",
      "2017-04-05 22:51:45.119625: step 36520, loss = (G: 7.02412987, D: 0.03181753) (0.225 sec/batch)\n",
      "2017-04-05 22:51:49.601957: step 36540, loss = (G: 5.36791086, D: 0.03746421) (0.225 sec/batch)\n",
      "2017-04-05 22:51:54.089121: step 36560, loss = (G: 6.51236963, D: 0.02189636) (0.225 sec/batch)\n",
      "2017-04-05 22:51:58.567688: step 36580, loss = (G: 7.48420525, D: 0.00654410) (0.223 sec/batch)\n",
      "2017-04-05 22:52:03.062675: step 36600, loss = (G: 8.97051144, D: 0.01004470) (0.224 sec/batch)\n",
      "2017-04-05 22:52:07.708980: step 36620, loss = (G: 8.48896122, D: 0.01559541) (0.225 sec/batch)\n",
      "2017-04-05 22:52:12.222952: step 36640, loss = (G: 5.29211903, D: 0.05640333) (0.225 sec/batch)\n",
      "2017-04-05 22:52:16.710450: step 36660, loss = (G: 6.61968136, D: 0.07440855) (0.225 sec/batch)\n",
      "2017-04-05 22:52:21.196352: step 36680, loss = (G: 14.42931366, D: 0.05233272) (0.223 sec/batch)\n",
      "2017-04-05 22:52:25.694977: step 36700, loss = (G: 3.13568187, D: 0.44084045) (0.224 sec/batch)\n",
      "2017-04-05 22:52:30.318772: step 36720, loss = (G: 8.09716225, D: 0.02203924) (0.223 sec/batch)\n",
      "2017-04-05 22:52:34.814842: step 36740, loss = (G: 6.92206001, D: 0.02331378) (0.225 sec/batch)\n",
      "2017-04-05 22:52:39.302380: step 36760, loss = (G: 6.16730881, D: 0.01557453) (0.224 sec/batch)\n",
      "2017-04-05 22:52:43.784172: step 36780, loss = (G: 5.57281971, D: 0.03469430) (0.224 sec/batch)\n",
      "2017-04-05 22:52:48.275865: step 36800, loss = (G: 7.18271828, D: 0.12337468) (0.229 sec/batch)\n",
      "2017-04-05 22:52:52.907550: step 36820, loss = (G: 11.03300667, D: 0.00159077) (0.225 sec/batch)\n",
      "2017-04-05 22:52:57.394977: step 36840, loss = (G: 4.07836342, D: 0.13280618) (0.224 sec/batch)\n",
      "2017-04-05 22:53:01.876355: step 36860, loss = (G: 13.23649311, D: 0.11427478) (0.223 sec/batch)\n",
      "2017-04-05 22:53:06.367669: step 36880, loss = (G: 7.81094933, D: 0.01999974) (0.224 sec/batch)\n",
      "2017-04-05 22:53:10.859785: step 36900, loss = (G: 6.05140781, D: 0.10728112) (0.227 sec/batch)\n",
      "2017-04-05 22:53:15.498545: step 36920, loss = (G: 4.54207134, D: 0.10114894) (0.224 sec/batch)\n",
      "2017-04-05 22:53:19.987810: step 36940, loss = (G: 14.76577568, D: 0.28261831) (0.225 sec/batch)\n",
      "2017-04-05 22:53:24.474501: step 36960, loss = (G: 9.66409111, D: 0.52819127) (0.224 sec/batch)\n",
      "2017-04-05 22:53:28.956262: step 36980, loss = (G: 15.47183704, D: 0.25692976) (0.224 sec/batch)\n",
      "2017-04-05 22:53:33.438883: step 37000, loss = (G: 7.59716368, D: 0.01861683) (0.225 sec/batch)\n",
      "2017-04-05 22:53:38.088680: step 37020, loss = (G: 11.95380211, D: 0.01136564) (0.224 sec/batch)\n",
      "2017-04-05 22:53:42.614557: step 37040, loss = (G: 13.43099213, D: 0.06732276) (0.226 sec/batch)\n",
      "2017-04-05 22:53:47.101945: step 37060, loss = (G: 7.02785158, D: 0.01137700) (0.226 sec/batch)\n",
      "2017-04-05 22:53:51.584605: step 37080, loss = (G: 9.29830551, D: 0.01259192) (0.225 sec/batch)\n",
      "2017-04-05 22:53:56.248719: step 37100, loss = (G: 6.09876060, D: 0.05817055) (0.225 sec/batch)\n",
      "2017-04-05 22:54:00.860761: step 37120, loss = (G: 7.14462090, D: 0.03733771) (0.224 sec/batch)\n",
      "2017-04-05 22:54:05.350754: step 37140, loss = (G: 8.51020432, D: 0.00296580) (0.225 sec/batch)\n",
      "2017-04-05 22:54:09.840947: step 37160, loss = (G: 6.35209179, D: 0.08660448) (0.224 sec/batch)\n",
      "2017-04-05 22:54:14.329334: step 37180, loss = (G: 7.91895199, D: 0.00528927) (0.226 sec/batch)\n",
      "2017-04-05 22:54:18.830058: step 37200, loss = (G: 5.79367495, D: 0.05015240) (0.224 sec/batch)\n",
      "2017-04-05 22:54:23.455289: step 37220, loss = (G: 6.59485674, D: 0.02350466) (0.225 sec/batch)\n",
      "2017-04-05 22:54:27.938436: step 37240, loss = (G: 6.65479898, D: 0.12209967) (0.225 sec/batch)\n",
      "2017-04-05 22:54:32.419349: step 37260, loss = (G: 10.07322407, D: 0.00204911) (0.224 sec/batch)\n",
      "2017-04-05 22:54:36.921999: step 37280, loss = (G: 6.09801483, D: 0.11505008) (0.224 sec/batch)\n",
      "2017-04-05 22:54:41.414224: step 37300, loss = (G: 5.27588701, D: 0.05584239) (0.228 sec/batch)\n",
      "2017-04-05 22:54:46.049702: step 37320, loss = (G: 4.52431393, D: 0.11637326) (0.223 sec/batch)\n",
      "2017-04-05 22:54:50.536614: step 37340, loss = (G: 10.52155972, D: 0.00541572) (0.224 sec/batch)\n",
      "2017-04-05 22:54:55.021647: step 37360, loss = (G: 3.98823452, D: 0.22423479) (0.225 sec/batch)\n",
      "2017-04-05 22:54:59.515941: step 37380, loss = (G: 5.62897682, D: 0.02626757) (0.225 sec/batch)\n",
      "2017-04-05 22:55:04.007527: step 37400, loss = (G: 8.26099396, D: 0.02336204) (0.224 sec/batch)\n",
      "2017-04-05 22:55:08.633182: step 37420, loss = (G: 8.40967846, D: 0.08658510) (0.226 sec/batch)\n",
      "2017-04-05 22:55:13.118473: step 37440, loss = (G: 9.26199341, D: 0.00690268) (0.224 sec/batch)\n",
      "2017-04-05 22:55:17.605017: step 37460, loss = (G: 20.54603958, D: 0.54509687) (0.224 sec/batch)\n",
      "2017-04-05 22:55:22.099153: step 37480, loss = (G: 9.11889172, D: 0.01349014) (0.224 sec/batch)\n",
      "2017-04-05 22:55:26.591114: step 37500, loss = (G: 5.72120428, D: 0.03928156) (0.226 sec/batch)\n",
      "2017-04-05 22:55:31.226498: step 37520, loss = (G: 3.61249828, D: 0.23569524) (0.230 sec/batch)\n",
      "2017-04-05 22:55:35.731861: step 37540, loss = (G: 6.21288538, D: 0.04404862) (0.224 sec/batch)\n",
      "2017-04-05 22:55:40.214053: step 37560, loss = (G: 4.99836922, D: 0.04367207) (0.226 sec/batch)\n",
      "2017-04-05 22:55:44.706916: step 37580, loss = (G: 5.48292112, D: 0.02889137) (0.225 sec/batch)\n",
      "2017-04-05 22:55:49.193512: step 37600, loss = (G: 4.98241663, D: 0.05112034) (0.225 sec/batch)\n",
      "2017-04-05 22:55:53.811566: step 37620, loss = (G: 11.09091282, D: 0.11772098) (0.223 sec/batch)\n",
      "2017-04-05 22:55:58.298678: step 37640, loss = (G: 5.15946150, D: 0.08087096) (0.225 sec/batch)\n",
      "2017-04-05 22:56:02.783995: step 37660, loss = (G: 8.00129032, D: 0.00589617) (0.226 sec/batch)\n",
      "2017-04-05 22:56:07.270663: step 37680, loss = (G: 15.34059620, D: 0.31103441) (0.226 sec/batch)\n",
      "2017-04-05 22:56:11.764904: step 37700, loss = (G: 6.57547617, D: 0.03019072) (0.225 sec/batch)\n",
      "2017-04-05 22:56:16.396023: step 37720, loss = (G: 7.69418621, D: 0.01427667) (0.225 sec/batch)\n",
      "2017-04-05 22:56:20.895738: step 37740, loss = (G: 4.87331200, D: 0.05941376) (0.225 sec/batch)\n",
      "2017-04-05 22:56:25.382870: step 37760, loss = (G: 3.69229293, D: 0.14932922) (0.224 sec/batch)\n",
      "2017-04-05 22:56:29.873747: step 37780, loss = (G: 11.04672909, D: 0.03485133) (0.224 sec/batch)\n",
      "2017-04-05 22:56:34.365532: step 37800, loss = (G: 6.09801197, D: 0.08435418) (0.226 sec/batch)\n",
      "2017-04-05 22:56:39.033334: step 37820, loss = (G: 13.09816647, D: 0.11375594) (0.226 sec/batch)\n",
      "2017-04-05 22:56:43.521677: step 37840, loss = (G: 5.20734310, D: 0.06064712) (0.225 sec/batch)\n",
      "2017-04-05 22:56:48.002930: step 37860, loss = (G: 7.66719818, D: 0.01920788) (0.224 sec/batch)\n",
      "2017-04-05 22:56:52.493739: step 37880, loss = (G: 6.49249935, D: 0.04335972) (0.225 sec/batch)\n",
      "2017-04-05 22:56:56.971476: step 37900, loss = (G: 8.13626575, D: 0.00753862) (0.224 sec/batch)\n",
      "2017-04-05 22:57:01.590216: step 37920, loss = (G: 5.94598341, D: 0.36066335) (0.224 sec/batch)\n",
      "2017-04-05 22:57:06.087280: step 37940, loss = (G: 8.02074242, D: 0.05032658) (0.225 sec/batch)\n",
      "2017-04-05 22:57:10.570986: step 37960, loss = (G: 4.72044754, D: 0.09596400) (0.223 sec/batch)\n",
      "2017-04-05 22:57:15.067982: step 37980, loss = (G: 7.27002239, D: 0.00937390) (0.225 sec/batch)\n",
      "2017-04-05 22:57:19.554060: step 38000, loss = (G: 7.18600082, D: 0.02813222) (0.224 sec/batch)\n",
      "2017-04-05 22:57:24.201982: step 38020, loss = (G: 8.86281586, D: 0.05218701) (0.225 sec/batch)\n",
      "2017-04-05 22:57:28.688640: step 38040, loss = (G: 6.21271610, D: 0.02131951) (0.228 sec/batch)\n",
      "2017-04-05 22:57:33.172926: step 38060, loss = (G: 5.42671442, D: 0.04436632) (0.224 sec/batch)\n",
      "2017-04-05 22:57:37.649719: step 38080, loss = (G: 4.03413343, D: 0.10404147) (0.225 sec/batch)\n",
      "2017-04-05 22:57:42.122738: step 38100, loss = (G: 7.06701756, D: 0.01474435) (0.223 sec/batch)\n",
      "2017-04-05 22:57:46.751295: step 38120, loss = (G: 8.32765198, D: 0.00489922) (0.224 sec/batch)\n",
      "2017-04-05 22:57:51.244964: step 38140, loss = (G: 8.02376652, D: 0.27523467) (0.227 sec/batch)\n",
      "2017-04-05 22:57:55.766260: step 38160, loss = (G: 8.22700882, D: 0.05127925) (0.227 sec/batch)\n",
      "2017-04-05 22:58:00.253545: step 38180, loss = (G: 11.95004845, D: 0.07600035) (0.224 sec/batch)\n",
      "2017-04-05 22:58:04.739851: step 38200, loss = (G: 18.29394913, D: 0.42508337) (0.224 sec/batch)\n",
      "2017-04-05 22:58:09.392758: step 38220, loss = (G: 16.72391129, D: 0.92579865) (0.225 sec/batch)\n",
      "2017-04-05 22:58:13.883559: step 38240, loss = (G: 12.23942852, D: 0.26468581) (0.226 sec/batch)\n",
      "2017-04-05 22:58:18.525861: step 38260, loss = (G: 5.34940243, D: 0.27174309) (0.224 sec/batch)\n",
      "2017-04-05 22:58:23.007966: step 38280, loss = (G: 6.19986629, D: 0.05296464) (0.224 sec/batch)\n",
      "2017-04-05 22:58:27.485635: step 38300, loss = (G: 7.52532387, D: 0.00906688) (0.223 sec/batch)\n",
      "2017-04-05 22:58:32.118089: step 38320, loss = (G: 4.60970306, D: 0.07218660) (0.224 sec/batch)\n",
      "2017-04-05 22:58:36.600785: step 38340, loss = (G: 9.42897320, D: 0.00396339) (0.224 sec/batch)\n",
      "2017-04-05 22:58:41.091030: step 38360, loss = (G: 4.51652956, D: 0.05653259) (0.223 sec/batch)\n",
      "2017-04-05 22:58:45.575653: step 38380, loss = (G: 5.93879938, D: 0.11436051) (0.224 sec/batch)\n",
      "2017-04-05 22:58:50.058031: step 38400, loss = (G: 8.17305851, D: 0.03221528) (0.224 sec/batch)\n",
      "2017-04-05 22:58:54.681607: step 38420, loss = (G: 10.04944229, D: 0.00952313) (0.224 sec/batch)\n",
      "2017-04-05 22:58:59.162311: step 38440, loss = (G: 5.53176403, D: 0.06658640) (0.224 sec/batch)\n",
      "2017-04-05 22:59:03.659689: step 38460, loss = (G: 7.62610626, D: 0.00764983) (0.229 sec/batch)\n",
      "2017-04-05 22:59:08.181290: step 38480, loss = (G: 8.99054909, D: 0.19894272) (0.224 sec/batch)\n",
      "2017-04-05 22:59:12.663351: step 38500, loss = (G: 7.06736326, D: 0.06100734) (0.224 sec/batch)\n",
      "2017-04-05 22:59:17.295642: step 38520, loss = (G: 6.82714415, D: 0.05567481) (0.223 sec/batch)\n",
      "2017-04-05 22:59:21.790552: step 38540, loss = (G: 5.36593866, D: 0.04882126) (0.223 sec/batch)\n",
      "2017-04-05 22:59:26.286125: step 38560, loss = (G: 8.94292259, D: 0.00373708) (0.224 sec/batch)\n",
      "2017-04-05 22:59:30.775112: step 38580, loss = (G: 7.67938948, D: 0.04422597) (0.223 sec/batch)\n",
      "2017-04-05 22:59:35.264258: step 38600, loss = (G: 8.51916695, D: 0.07071090) (0.226 sec/batch)\n",
      "2017-04-05 22:59:39.892234: step 38620, loss = (G: 10.30518818, D: 0.04344884) (0.225 sec/batch)\n",
      "2017-04-05 22:59:44.384618: step 38640, loss = (G: 7.95343733, D: 0.01258976) (0.225 sec/batch)\n",
      "2017-04-05 22:59:48.868393: step 38660, loss = (G: 7.29663229, D: 0.35813096) (0.224 sec/batch)\n",
      "2017-04-05 22:59:53.353387: step 38680, loss = (G: 3.84949064, D: 0.31216514) (0.224 sec/batch)\n",
      "2017-04-05 22:59:57.838520: step 38700, loss = (G: 7.93547726, D: 0.09003115) (0.224 sec/batch)\n",
      "2017-04-05 23:00:02.463924: step 38720, loss = (G: 3.86989737, D: 0.22256534) (0.224 sec/batch)\n",
      "2017-04-05 23:00:06.942225: step 38740, loss = (G: 3.88802767, D: 0.63973486) (0.223 sec/batch)\n",
      "2017-04-05 23:00:11.439686: step 38760, loss = (G: 6.11963272, D: 0.01191627) (0.224 sec/batch)\n",
      "2017-04-05 23:00:15.926725: step 38780, loss = (G: 4.65667915, D: 0.07118864) (0.225 sec/batch)\n",
      "2017-04-05 23:00:20.411668: step 38800, loss = (G: 6.75225353, D: 0.01383330) (0.224 sec/batch)\n",
      "2017-04-05 23:00:25.036751: step 38820, loss = (G: 5.79843330, D: 0.06720309) (0.224 sec/batch)\n",
      "2017-04-05 23:00:29.552827: step 38840, loss = (G: 4.95976210, D: 0.07939672) (0.224 sec/batch)\n",
      "2017-04-05 23:00:34.043157: step 38860, loss = (G: 20.78819466, D: 0.88877505) (0.224 sec/batch)\n",
      "2017-04-05 23:00:38.524899: step 38880, loss = (G: 7.18432999, D: 0.04100842) (0.224 sec/batch)\n",
      "2017-04-05 23:00:43.004209: step 38900, loss = (G: 6.12445593, D: 0.10631363) (0.223 sec/batch)\n",
      "2017-04-05 23:00:47.638111: step 38920, loss = (G: 5.76521587, D: 0.06125862) (0.223 sec/batch)\n",
      "2017-04-05 23:00:52.134489: step 38940, loss = (G: 10.03228283, D: 0.83563423) (0.224 sec/batch)\n",
      "2017-04-05 23:00:56.615436: step 38960, loss = (G: 6.98009491, D: 0.01538711) (0.224 sec/batch)\n",
      "2017-04-05 23:01:01.115185: step 38980, loss = (G: 3.49442434, D: 0.13642822) (0.223 sec/batch)\n",
      "2017-04-05 23:01:05.606350: step 39000, loss = (G: 8.70817947, D: 0.00215582) (0.224 sec/batch)\n",
      "2017-04-05 23:01:10.247544: step 39020, loss = (G: 10.27712250, D: 0.00365630) (0.224 sec/batch)\n",
      "2017-04-05 23:01:14.735519: step 39040, loss = (G: 3.24502611, D: 0.78436410) (0.224 sec/batch)\n",
      "2017-04-05 23:01:19.211049: step 39060, loss = (G: 6.23044920, D: 0.09007817) (0.224 sec/batch)\n",
      "2017-04-05 23:01:23.717017: step 39080, loss = (G: 19.37292671, D: 1.60354972) (0.225 sec/batch)\n",
      "2017-04-05 23:01:28.216759: step 39100, loss = (G: 5.66315889, D: 0.03054944) (0.225 sec/batch)\n",
      "2017-04-05 23:01:32.856665: step 39120, loss = (G: 5.59841728, D: 0.02954844) (0.225 sec/batch)\n",
      "2017-04-05 23:01:37.335877: step 39140, loss = (G: 4.29934359, D: 0.16120826) (0.223 sec/batch)\n",
      "2017-04-05 23:01:41.857195: step 39160, loss = (G: 7.42874146, D: 0.00951018) (0.224 sec/batch)\n",
      "2017-04-05 23:01:46.348413: step 39180, loss = (G: 6.32127380, D: 0.03431757) (0.225 sec/batch)\n",
      "2017-04-05 23:01:50.832764: step 39200, loss = (G: 6.87908936, D: 0.01911583) (0.229 sec/batch)\n",
      "2017-04-05 23:01:55.461801: step 39220, loss = (G: 7.23531914, D: 0.06256224) (0.224 sec/batch)\n",
      "2017-04-05 23:01:59.953379: step 39240, loss = (G: 8.86240196, D: 0.01996034) (0.224 sec/batch)\n",
      "2017-04-05 23:02:04.437659: step 39260, loss = (G: 3.30503798, D: 0.40359327) (0.230 sec/batch)\n",
      "2017-04-05 23:02:08.943438: step 39280, loss = (G: 10.71122742, D: 0.00150009) (0.224 sec/batch)\n",
      "2017-04-05 23:02:13.443469: step 39300, loss = (G: 6.46324825, D: 0.02255935) (0.224 sec/batch)\n",
      "2017-04-05 23:02:18.075349: step 39320, loss = (G: 5.03763294, D: 0.06785996) (0.224 sec/batch)\n",
      "2017-04-05 23:02:22.566621: step 39340, loss = (G: 8.08050346, D: 0.00750957) (0.225 sec/batch)\n",
      "2017-04-05 23:02:27.051197: step 39360, loss = (G: 10.03445053, D: 2.03756881) (0.227 sec/batch)\n",
      "2017-04-05 23:02:31.532752: step 39380, loss = (G: 6.73192739, D: 0.05405176) (0.224 sec/batch)\n",
      "2017-04-05 23:02:36.009807: step 39400, loss = (G: 8.19596577, D: 0.02648020) (0.224 sec/batch)\n",
      "2017-04-05 23:02:40.774249: step 39420, loss = (G: 7.96889162, D: 0.10865127) (0.223 sec/batch)\n",
      "2017-04-05 23:02:45.264847: step 39440, loss = (G: 6.59328270, D: 1.17941368) (0.224 sec/batch)\n",
      "2017-04-05 23:02:49.742414: step 39460, loss = (G: 6.76328802, D: 0.02306850) (0.224 sec/batch)\n",
      "2017-04-05 23:02:54.254344: step 39480, loss = (G: 4.99159861, D: 0.18418340) (0.226 sec/batch)\n",
      "2017-04-05 23:02:58.741190: step 39500, loss = (G: 6.80764914, D: 0.04945986) (0.224 sec/batch)\n",
      "2017-04-05 23:03:03.365056: step 39520, loss = (G: 5.37524176, D: 0.06018456) (0.223 sec/batch)\n",
      "2017-04-05 23:03:07.856282: step 39540, loss = (G: 7.40703297, D: 0.02031013) (0.225 sec/batch)\n",
      "2017-04-05 23:03:12.343798: step 39560, loss = (G: 8.55306816, D: 0.28289327) (0.226 sec/batch)\n",
      "2017-04-05 23:03:16.831838: step 39580, loss = (G: 6.88902044, D: 0.03922787) (0.224 sec/batch)\n",
      "2017-04-05 23:03:21.328015: step 39600, loss = (G: 9.02360344, D: 0.01347450) (0.225 sec/batch)\n",
      "2017-04-05 23:03:25.948605: step 39620, loss = (G: 3.53148031, D: 0.20712589) (0.224 sec/batch)\n",
      "2017-04-05 23:03:30.443617: step 39640, loss = (G: 6.38481808, D: 0.13755754) (0.224 sec/batch)\n",
      "2017-04-05 23:03:34.931851: step 39660, loss = (G: 6.47120333, D: 0.01771506) (0.225 sec/batch)\n",
      "2017-04-05 23:03:39.407873: step 39680, loss = (G: 8.86580753, D: 0.00837582) (0.224 sec/batch)\n",
      "2017-04-05 23:03:43.888467: step 39700, loss = (G: 7.56030893, D: 0.03194907) (0.223 sec/batch)\n",
      "2017-04-05 23:03:48.524482: step 39720, loss = (G: 5.60295391, D: 0.05948212) (0.223 sec/batch)\n",
      "2017-04-05 23:03:53.004025: step 39740, loss = (G: 8.19785500, D: 0.01141047) (0.226 sec/batch)\n",
      "2017-04-05 23:03:57.516096: step 39760, loss = (G: 5.57901049, D: 0.03461061) (0.225 sec/batch)\n",
      "2017-04-05 23:04:02.006584: step 39780, loss = (G: 6.11529636, D: 0.01973018) (0.224 sec/batch)\n",
      "2017-04-05 23:04:06.506209: step 39800, loss = (G: 5.58146429, D: 0.02717910) (0.225 sec/batch)\n",
      "2017-04-05 23:04:11.129122: step 39820, loss = (G: 8.52521610, D: 0.01566101) (0.225 sec/batch)\n",
      "2017-04-05 23:04:15.609655: step 39840, loss = (G: 6.34829521, D: 0.02992423) (0.224 sec/batch)\n",
      "2017-04-05 23:04:20.105472: step 39860, loss = (G: 7.86586189, D: 0.08778325) (0.225 sec/batch)\n",
      "2017-04-05 23:04:24.589996: step 39880, loss = (G: 7.03312969, D: 0.02285158) (0.225 sec/batch)\n",
      "2017-04-05 23:04:29.068724: step 39900, loss = (G: 9.87382412, D: 0.02134342) (0.224 sec/batch)\n",
      "2017-04-05 23:04:33.702251: step 39920, loss = (G: 9.01520348, D: 0.00358569) (0.225 sec/batch)\n",
      "2017-04-05 23:04:38.236218: step 39940, loss = (G: 2.67248249, D: 0.53823853) (0.223 sec/batch)\n",
      "2017-04-05 23:04:42.715296: step 39960, loss = (G: 7.49804211, D: 0.07154746) (0.224 sec/batch)\n",
      "2017-04-05 23:04:47.206042: step 39980, loss = (G: 5.67528582, D: 0.23132202) (0.225 sec/batch)\n",
      "2017-04-05 23:04:51.695073: step 40000, loss = (G: 7.06390762, D: 0.03557922) (0.223 sec/batch)\n",
      "2017-04-05 23:04:56.315030: step 40020, loss = (G: 7.32197142, D: 0.00747497) (0.223 sec/batch)\n",
      "2017-04-05 23:05:00.797589: step 40040, loss = (G: 6.79045582, D: 0.07960452) (0.224 sec/batch)\n",
      "2017-04-05 23:05:05.288881: step 40060, loss = (G: 9.31250858, D: 0.00507028) (0.224 sec/batch)\n",
      "2017-04-05 23:05:09.771489: step 40080, loss = (G: 8.04151726, D: 0.02796968) (0.225 sec/batch)\n",
      "2017-04-05 23:05:14.261657: step 40100, loss = (G: 8.93007088, D: 0.01503297) (0.224 sec/batch)\n",
      "2017-04-05 23:05:18.897646: step 40120, loss = (G: 4.52006388, D: 0.08480070) (0.227 sec/batch)\n",
      "2017-04-05 23:05:23.386156: step 40140, loss = (G: 7.57130384, D: 0.01582627) (0.224 sec/batch)\n",
      "2017-04-05 23:05:27.867061: step 40160, loss = (G: 7.62462187, D: 0.00429197) (0.224 sec/batch)\n",
      "2017-04-05 23:05:32.360076: step 40180, loss = (G: 7.89191055, D: 0.02554401) (0.224 sec/batch)\n",
      "2017-04-05 23:05:36.846891: step 40200, loss = (G: 8.51848888, D: 0.11823048) (0.225 sec/batch)\n",
      "2017-04-05 23:05:41.478232: step 40220, loss = (G: 7.52860975, D: 0.06739888) (0.225 sec/batch)\n",
      "2017-04-05 23:05:45.961366: step 40240, loss = (G: 8.78802872, D: 0.00445623) (0.225 sec/batch)\n",
      "2017-04-05 23:05:50.450817: step 40260, loss = (G: 10.48990631, D: 0.04290655) (0.225 sec/batch)\n",
      "2017-04-05 23:05:54.937805: step 40280, loss = (G: 5.07162380, D: 0.09034318) (0.225 sec/batch)\n",
      "2017-04-05 23:05:59.440234: step 40300, loss = (G: 7.10731792, D: 0.01338372) (0.226 sec/batch)\n",
      "2017-04-05 23:06:04.067679: step 40320, loss = (G: 7.56730986, D: 0.06072022) (0.224 sec/batch)\n",
      "2017-04-05 23:06:08.550058: step 40340, loss = (G: 9.09863377, D: 0.02590575) (0.224 sec/batch)\n",
      "2017-04-05 23:06:13.041806: step 40360, loss = (G: 7.56163645, D: 0.09708342) (0.224 sec/batch)\n",
      "2017-04-05 23:06:17.547416: step 40380, loss = (G: 9.59160709, D: 0.01455796) (0.228 sec/batch)\n",
      "2017-04-05 23:06:22.051084: step 40400, loss = (G: 5.14752579, D: 0.04832304) (0.227 sec/batch)\n",
      "2017-04-05 23:06:26.682371: step 40420, loss = (G: 4.45724678, D: 0.18670079) (0.223 sec/batch)\n",
      "2017-04-05 23:06:31.173194: step 40440, loss = (G: 5.74676704, D: 0.01839966) (0.226 sec/batch)\n",
      "2017-04-05 23:06:35.661110: step 40460, loss = (G: 4.74456978, D: 0.08243025) (0.224 sec/batch)\n",
      "2017-04-05 23:06:40.152501: step 40480, loss = (G: 6.77031708, D: 0.13497359) (0.225 sec/batch)\n",
      "2017-04-05 23:06:44.657500: step 40500, loss = (G: 5.60616875, D: 0.02958321) (0.224 sec/batch)\n",
      "2017-04-05 23:06:49.283954: step 40520, loss = (G: 6.71112156, D: 0.03077153) (0.225 sec/batch)\n",
      "2017-04-05 23:06:53.776928: step 40540, loss = (G: 7.05220032, D: 0.27673194) (0.223 sec/batch)\n",
      "2017-04-05 23:06:58.260347: step 40560, loss = (G: 6.89409781, D: 0.02047659) (0.224 sec/batch)\n",
      "2017-04-05 23:07:02.916734: step 40580, loss = (G: 5.33498955, D: 0.02966834) (0.231 sec/batch)\n",
      "2017-04-05 23:07:07.422676: step 40600, loss = (G: 9.22738075, D: 0.04986240) (0.224 sec/batch)\n",
      "2017-04-05 23:07:12.057560: step 40620, loss = (G: 7.89604759, D: 0.00482168) (0.222 sec/batch)\n",
      "2017-04-05 23:07:16.541348: step 40640, loss = (G: 11.45407677, D: 0.03859092) (0.223 sec/batch)\n",
      "2017-04-05 23:07:21.029194: step 40660, loss = (G: 8.49855614, D: 0.01577119) (0.224 sec/batch)\n",
      "2017-04-05 23:07:25.513942: step 40680, loss = (G: 5.61930275, D: 0.05725993) (0.223 sec/batch)\n",
      "2017-04-05 23:07:30.036447: step 40700, loss = (G: 7.31237602, D: 0.26082367) (0.223 sec/batch)\n",
      "2017-04-05 23:07:34.654692: step 40720, loss = (G: 8.78977776, D: 0.02075250) (0.225 sec/batch)\n",
      "2017-04-05 23:07:39.138792: step 40740, loss = (G: 6.42210531, D: 0.01371014) (0.224 sec/batch)\n",
      "2017-04-05 23:07:43.622612: step 40760, loss = (G: 8.70457458, D: 0.07957052) (0.224 sec/batch)\n",
      "2017-04-05 23:07:48.101414: step 40780, loss = (G: 9.85052013, D: 0.00557086) (0.223 sec/batch)\n",
      "2017-04-05 23:07:52.589959: step 40800, loss = (G: 6.01144266, D: 0.03990966) (0.224 sec/batch)\n",
      "2017-04-05 23:07:57.228343: step 40820, loss = (G: 2.18196154, D: 2.25118136) (0.226 sec/batch)\n",
      "2017-04-05 23:08:01.721493: step 40840, loss = (G: 8.04895210, D: 0.04160679) (0.223 sec/batch)\n",
      "2017-04-05 23:08:06.203924: step 40860, loss = (G: 10.29283810, D: 0.01113756) (0.224 sec/batch)\n",
      "2017-04-05 23:08:10.680738: step 40880, loss = (G: 7.07431936, D: 0.01128806) (0.224 sec/batch)\n",
      "2017-04-05 23:08:15.159938: step 40900, loss = (G: 4.59565449, D: 0.05494069) (0.225 sec/batch)\n",
      "2017-04-05 23:08:19.788763: step 40920, loss = (G: 10.31546307, D: 0.00986887) (0.225 sec/batch)\n",
      "2017-04-05 23:08:24.283644: step 40940, loss = (G: 8.94244003, D: 0.51637983) (0.224 sec/batch)\n",
      "2017-04-05 23:08:28.786032: step 40960, loss = (G: 9.30356121, D: 0.00383648) (0.224 sec/batch)\n",
      "2017-04-05 23:08:33.267644: step 40980, loss = (G: 8.08292675, D: 0.19657516) (0.226 sec/batch)\n",
      "2017-04-05 23:08:37.757034: step 41000, loss = (G: 7.84253836, D: 0.00518581) (0.222 sec/batch)\n",
      "2017-04-05 23:08:42.383088: step 41020, loss = (G: 4.91301394, D: 0.09741955) (0.230 sec/batch)\n",
      "2017-04-05 23:08:46.892178: step 41040, loss = (G: 7.97194815, D: 0.18121128) (0.224 sec/batch)\n",
      "2017-04-05 23:08:51.372743: step 41060, loss = (G: 21.21627808, D: 1.36269510) (0.224 sec/batch)\n",
      "2017-04-05 23:08:55.856571: step 41080, loss = (G: 6.08631611, D: 0.03300871) (0.224 sec/batch)\n",
      "2017-04-05 23:09:00.343381: step 41100, loss = (G: 7.76247454, D: 0.07670760) (0.223 sec/batch)\n",
      "2017-04-05 23:09:04.971522: step 41120, loss = (G: 10.21469402, D: 0.00082025) (0.227 sec/batch)\n",
      "2017-04-05 23:09:09.452574: step 41140, loss = (G: 8.46552658, D: 0.01171970) (0.224 sec/batch)\n",
      "2017-04-05 23:09:13.953915: step 41160, loss = (G: 6.13392591, D: 0.01786301) (0.225 sec/batch)\n",
      "2017-04-05 23:09:18.436483: step 41180, loss = (G: 8.93424797, D: 0.04922142) (0.224 sec/batch)\n",
      "2017-04-05 23:09:22.917833: step 41200, loss = (G: 4.20119476, D: 0.12482965) (0.222 sec/batch)\n",
      "2017-04-05 23:09:27.546666: step 41220, loss = (G: 6.67349625, D: 0.01615053) (0.225 sec/batch)\n",
      "2017-04-05 23:09:32.033312: step 41240, loss = (G: 4.79945755, D: 0.06387760) (0.225 sec/batch)\n",
      "2017-04-05 23:09:36.510357: step 41260, loss = (G: 7.58100224, D: 0.02846009) (0.224 sec/batch)\n",
      "2017-04-05 23:09:40.988225: step 41280, loss = (G: 6.99121666, D: 0.01149331) (0.224 sec/batch)\n",
      "2017-04-05 23:09:45.487479: step 41300, loss = (G: 4.46597147, D: 0.07633720) (0.223 sec/batch)\n",
      "2017-04-05 23:09:50.116786: step 41320, loss = (G: 8.36560059, D: 0.13747782) (0.224 sec/batch)\n",
      "2017-04-05 23:09:54.603815: step 41340, loss = (G: 5.55134583, D: 0.04620229) (0.224 sec/batch)\n",
      "2017-04-05 23:09:59.089163: step 41360, loss = (G: 4.54737520, D: 0.06403538) (0.225 sec/batch)\n",
      "2017-04-05 23:10:03.583996: step 41380, loss = (G: 6.76237392, D: 0.61459959) (0.225 sec/batch)\n",
      "2017-04-05 23:10:08.072965: step 41400, loss = (G: 4.75819731, D: 0.08879687) (0.223 sec/batch)\n",
      "2017-04-05 23:10:12.722424: step 41420, loss = (G: 9.72867870, D: 0.21705727) (0.230 sec/batch)\n",
      "2017-04-05 23:10:17.240206: step 41440, loss = (G: 10.82640648, D: 0.14216506) (0.223 sec/batch)\n",
      "2017-04-05 23:10:21.729380: step 41460, loss = (G: 4.11747360, D: 0.16272017) (0.230 sec/batch)\n",
      "2017-04-05 23:10:26.252820: step 41480, loss = (G: 13.34446812, D: 0.02090899) (0.225 sec/batch)\n",
      "2017-04-05 23:10:30.734015: step 41500, loss = (G: 6.66808844, D: 0.00911004) (0.223 sec/batch)\n",
      "2017-04-05 23:10:35.354903: step 41520, loss = (G: 9.50776863, D: 0.04672727) (0.225 sec/batch)\n",
      "2017-04-05 23:10:39.839234: step 41540, loss = (G: 2.90880299, D: 0.44512239) (0.227 sec/batch)\n",
      "2017-04-05 23:10:44.324377: step 41560, loss = (G: 4.01039362, D: 0.12511721) (0.224 sec/batch)\n",
      "2017-04-05 23:10:48.802158: step 41580, loss = (G: 6.74676037, D: 0.06458010) (0.223 sec/batch)\n",
      "2017-04-05 23:10:53.283020: step 41600, loss = (G: 2.62892437, D: 0.43827227) (0.224 sec/batch)\n",
      "2017-04-05 23:10:57.914644: step 41620, loss = (G: 7.75493336, D: 0.01646972) (0.225 sec/batch)\n",
      "2017-04-05 23:11:02.445278: step 41640, loss = (G: 4.79655504, D: 0.07406684) (0.230 sec/batch)\n",
      "2017-04-05 23:11:06.933883: step 41660, loss = (G: 5.49638748, D: 0.03099548) (0.225 sec/batch)\n",
      "2017-04-05 23:11:11.420107: step 41680, loss = (G: 4.81210995, D: 0.10262958) (0.224 sec/batch)\n",
      "2017-04-05 23:11:15.903128: step 41700, loss = (G: 4.81767654, D: 0.06138024) (0.223 sec/batch)\n",
      "2017-04-05 23:11:20.544064: step 41720, loss = (G: 5.48554802, D: 0.05724989) (0.225 sec/batch)\n",
      "2017-04-05 23:11:25.165706: step 41740, loss = (G: 8.16739082, D: 0.03500239) (0.223 sec/batch)\n",
      "2017-04-05 23:11:29.655618: step 41760, loss = (G: 5.75375891, D: 0.05683532) (0.224 sec/batch)\n",
      "2017-04-05 23:11:34.135391: step 41780, loss = (G: 8.59438896, D: 0.06950353) (0.224 sec/batch)\n",
      "2017-04-05 23:11:38.620943: step 41800, loss = (G: 7.56842327, D: 0.02112139) (0.223 sec/batch)\n",
      "2017-04-05 23:11:43.239082: step 41820, loss = (G: 6.54914761, D: 0.02145283) (0.224 sec/batch)\n",
      "2017-04-05 23:11:47.729638: step 41840, loss = (G: 7.09191942, D: 0.06144217) (0.224 sec/batch)\n",
      "2017-04-05 23:11:52.209706: step 41860, loss = (G: 9.21524811, D: 0.01256356) (0.224 sec/batch)\n",
      "2017-04-05 23:11:56.694420: step 41880, loss = (G: 6.36270523, D: 0.10377187) (0.225 sec/batch)\n",
      "2017-04-05 23:12:01.187739: step 41900, loss = (G: 6.67070007, D: 0.14533380) (0.224 sec/batch)\n",
      "2017-04-05 23:12:05.806806: step 41920, loss = (G: 6.08703804, D: 0.02611986) (0.223 sec/batch)\n",
      "2017-04-05 23:12:10.293540: step 41940, loss = (G: 5.41213512, D: 0.04930545) (0.224 sec/batch)\n",
      "2017-04-05 23:12:14.795722: step 41960, loss = (G: 5.77783394, D: 0.05405479) (0.224 sec/batch)\n",
      "2017-04-05 23:12:19.277279: step 41980, loss = (G: 6.08768702, D: 0.01314221) (0.224 sec/batch)\n",
      "2017-04-05 23:12:23.813343: step 42000, loss = (G: 5.45292282, D: 0.03278397) (0.224 sec/batch)\n",
      "2017-04-05 23:12:28.435552: step 42020, loss = (G: 5.45930958, D: 0.03357193) (0.224 sec/batch)\n",
      "2017-04-05 23:12:32.911197: step 42040, loss = (G: 5.62121677, D: 0.06204196) (0.224 sec/batch)\n",
      "2017-04-05 23:12:37.393154: step 42060, loss = (G: 5.28659391, D: 0.04578112) (0.223 sec/batch)\n",
      "2017-04-05 23:12:41.880399: step 42080, loss = (G: 2.92107749, D: 0.65476811) (0.225 sec/batch)\n",
      "2017-04-05 23:12:46.368263: step 42100, loss = (G: 7.77318430, D: 0.01033873) (0.225 sec/batch)\n",
      "2017-04-05 23:12:51.001108: step 42120, loss = (G: 5.95601606, D: 0.02360764) (0.224 sec/batch)\n",
      "2017-04-05 23:12:55.484180: step 42140, loss = (G: 6.98300982, D: 0.01959938) (0.224 sec/batch)\n",
      "2017-04-05 23:12:59.978657: step 42160, loss = (G: 5.81235743, D: 0.04512238) (0.224 sec/batch)\n",
      "2017-04-05 23:13:04.479798: step 42180, loss = (G: 7.51289272, D: 0.04308694) (0.224 sec/batch)\n",
      "2017-04-05 23:13:08.972825: step 42200, loss = (G: 6.97957611, D: 0.02601617) (0.226 sec/batch)\n",
      "2017-04-05 23:13:13.608358: step 42220, loss = (G: 7.51346207, D: 0.07677682) (0.224 sec/batch)\n",
      "2017-04-05 23:13:18.093788: step 42240, loss = (G: 7.41363668, D: 0.00746137) (0.224 sec/batch)\n",
      "2017-04-05 23:13:22.592836: step 42260, loss = (G: 7.24941826, D: 0.02281003) (0.225 sec/batch)\n",
      "2017-04-05 23:13:27.083936: step 42280, loss = (G: 11.54811478, D: 0.32796365) (0.225 sec/batch)\n",
      "2017-04-05 23:13:31.571686: step 42300, loss = (G: 5.46101570, D: 0.06357954) (0.224 sec/batch)\n",
      "2017-04-05 23:13:36.191947: step 42320, loss = (G: 3.27202344, D: 0.59373218) (0.224 sec/batch)\n",
      "2017-04-05 23:13:40.669615: step 42340, loss = (G: 4.61445856, D: 0.09368303) (0.224 sec/batch)\n",
      "2017-04-05 23:13:45.153004: step 42360, loss = (G: 8.46384621, D: 0.01933960) (0.223 sec/batch)\n",
      "2017-04-05 23:13:49.635108: step 42380, loss = (G: 6.65502691, D: 0.02814190) (0.225 sec/batch)\n",
      "2017-04-05 23:13:54.132702: step 42400, loss = (G: 6.47433662, D: 0.01251689) (0.224 sec/batch)\n",
      "2017-04-05 23:13:58.755579: step 42420, loss = (G: 6.52331018, D: 0.06171436) (0.224 sec/batch)\n",
      "2017-04-05 23:14:03.260928: step 42440, loss = (G: 7.61774158, D: 0.02538710) (0.224 sec/batch)\n",
      "2017-04-05 23:14:07.762263: step 42460, loss = (G: 6.76179886, D: 0.01757311) (0.226 sec/batch)\n",
      "2017-04-05 23:14:12.254293: step 42480, loss = (G: 5.66123676, D: 0.03790518) (0.224 sec/batch)\n",
      "2017-04-05 23:14:16.730870: step 42500, loss = (G: 6.58026075, D: 0.02311222) (0.222 sec/batch)\n",
      "2017-04-05 23:14:21.371398: step 42520, loss = (G: 5.55818939, D: 0.03249936) (0.224 sec/batch)\n",
      "2017-04-05 23:14:25.851980: step 42540, loss = (G: 6.66984367, D: 0.07429422) (0.224 sec/batch)\n",
      "2017-04-05 23:14:30.342323: step 42560, loss = (G: 7.10696983, D: 0.01685330) (0.224 sec/batch)\n",
      "2017-04-05 23:14:34.844343: step 42580, loss = (G: 4.51832342, D: 0.09101446) (0.225 sec/batch)\n",
      "2017-04-05 23:14:39.327431: step 42600, loss = (G: 7.38463974, D: 0.00787887) (0.225 sec/batch)\n",
      "2017-04-05 23:14:43.952679: step 42620, loss = (G: 13.91939831, D: 0.03146395) (0.224 sec/batch)\n",
      "2017-04-05 23:14:48.445528: step 42640, loss = (G: 7.93837929, D: 0.01065256) (0.224 sec/batch)\n",
      "2017-04-05 23:14:52.932397: step 42660, loss = (G: 5.82546377, D: 0.05419267) (0.224 sec/batch)\n",
      "2017-04-05 23:14:57.420350: step 42680, loss = (G: 10.41456318, D: 0.00135536) (0.224 sec/batch)\n",
      "2017-04-05 23:15:01.904887: step 42700, loss = (G: 7.06012487, D: 0.01762372) (0.225 sec/batch)\n",
      "2017-04-05 23:15:06.541025: step 42720, loss = (G: 5.24994183, D: 0.03591105) (0.225 sec/batch)\n",
      "2017-04-05 23:15:11.036581: step 42740, loss = (G: 4.64127111, D: 0.07474765) (0.223 sec/batch)\n",
      "2017-04-05 23:15:15.524079: step 42760, loss = (G: 10.15643024, D: 0.10996840) (0.225 sec/batch)\n",
      "2017-04-05 23:15:20.024120: step 42780, loss = (G: 3.57615876, D: 0.33924717) (0.225 sec/batch)\n",
      "2017-04-05 23:15:24.512610: step 42800, loss = (G: 7.14707947, D: 0.02275794) (0.223 sec/batch)\n",
      "2017-04-05 23:15:29.142890: step 42820, loss = (G: 3.89065862, D: 0.25406712) (0.225 sec/batch)\n",
      "2017-04-05 23:15:33.633056: step 42840, loss = (G: 5.34662580, D: 0.03873628) (0.225 sec/batch)\n",
      "2017-04-05 23:15:38.121584: step 42860, loss = (G: 2.07800961, D: 1.99003911) (0.225 sec/batch)\n",
      "2017-04-05 23:15:42.610529: step 42880, loss = (G: 5.35696554, D: 0.03389096) (0.224 sec/batch)\n",
      "2017-04-05 23:15:47.252357: step 42900, loss = (G: 13.33194637, D: 0.06072298) (0.225 sec/batch)\n",
      "2017-04-05 23:15:51.907621: step 42920, loss = (G: 7.81539917, D: 0.00373189) (0.231 sec/batch)\n",
      "2017-04-05 23:15:56.414871: step 42940, loss = (G: 6.40856075, D: 0.02148936) (0.224 sec/batch)\n",
      "2017-04-05 23:16:00.898139: step 42960, loss = (G: 10.47761917, D: 0.04604740) (0.226 sec/batch)\n",
      "2017-04-05 23:16:05.394474: step 42980, loss = (G: 2.95232821, D: 0.28434205) (0.224 sec/batch)\n",
      "2017-04-05 23:16:09.887052: step 43000, loss = (G: 9.13140202, D: 0.01842819) (0.224 sec/batch)\n",
      "2017-04-05 23:16:14.510287: step 43020, loss = (G: 10.13565159, D: 0.02953267) (0.225 sec/batch)\n",
      "2017-04-05 23:16:18.986983: step 43040, loss = (G: 5.20606947, D: 0.03875649) (0.224 sec/batch)\n",
      "2017-04-05 23:16:23.465604: step 43060, loss = (G: 4.97085857, D: 0.32724360) (0.224 sec/batch)\n",
      "2017-04-05 23:16:27.957319: step 43080, loss = (G: 5.58410931, D: 0.05310725) (0.225 sec/batch)\n",
      "2017-04-05 23:16:32.448140: step 43100, loss = (G: 6.13417530, D: 0.42900440) (0.225 sec/batch)\n",
      "2017-04-05 23:16:37.076888: step 43120, loss = (G: 16.20074272, D: 1.07784736) (0.224 sec/batch)\n",
      "2017-04-05 23:16:41.577847: step 43140, loss = (G: 5.12730980, D: 0.15126380) (0.231 sec/batch)\n",
      "2017-04-05 23:16:46.093080: step 43160, loss = (G: 4.82194424, D: 0.06305303) (0.223 sec/batch)\n",
      "2017-04-05 23:16:50.632384: step 43180, loss = (G: 8.80514908, D: 0.02873018) (0.228 sec/batch)\n",
      "2017-04-05 23:16:55.112999: step 43200, loss = (G: 6.98846245, D: 0.00753587) (0.230 sec/batch)\n",
      "2017-04-05 23:16:59.771187: step 43220, loss = (G: 9.30039310, D: 0.00197789) (0.223 sec/batch)\n",
      "2017-04-05 23:17:04.260443: step 43240, loss = (G: 8.36032295, D: 0.02349922) (0.225 sec/batch)\n",
      "2017-04-05 23:17:08.742675: step 43260, loss = (G: 6.79120493, D: 0.05228269) (0.227 sec/batch)\n",
      "2017-04-05 23:17:13.224888: step 43280, loss = (G: 5.97639418, D: 0.03157154) (0.223 sec/batch)\n",
      "2017-04-05 23:17:17.753129: step 43300, loss = (G: 5.98515272, D: 0.04130345) (0.224 sec/batch)\n",
      "2017-04-05 23:17:22.374864: step 43320, loss = (G: 6.79167128, D: 0.01333864) (0.224 sec/batch)\n",
      "2017-04-05 23:17:26.854860: step 43340, loss = (G: 8.97147942, D: 0.00407760) (0.224 sec/batch)\n",
      "2017-04-05 23:17:31.349347: step 43360, loss = (G: 8.26677322, D: 0.01923667) (0.225 sec/batch)\n",
      "2017-04-05 23:17:35.839398: step 43380, loss = (G: 9.91976452, D: 0.00966667) (0.225 sec/batch)\n",
      "2017-04-05 23:17:40.324881: step 43400, loss = (G: 7.91352034, D: 0.06030845) (0.224 sec/batch)\n",
      "2017-04-05 23:17:44.955711: step 43420, loss = (G: 7.29307938, D: 0.00869478) (0.224 sec/batch)\n",
      "2017-04-05 23:17:49.495357: step 43440, loss = (G: 6.55738068, D: 0.10836610) (0.225 sec/batch)\n",
      "2017-04-05 23:17:53.998824: step 43460, loss = (G: 10.37339973, D: 0.12661602) (0.225 sec/batch)\n",
      "2017-04-05 23:17:58.480931: step 43480, loss = (G: 4.38736773, D: 0.11211629) (0.226 sec/batch)\n",
      "2017-04-05 23:18:02.971455: step 43500, loss = (G: 4.98515797, D: 0.05277908) (0.224 sec/batch)\n",
      "2017-04-05 23:18:07.617544: step 43520, loss = (G: 7.14805794, D: 0.00684185) (0.224 sec/batch)\n",
      "2017-04-05 23:18:12.106789: step 43540, loss = (G: 6.81425905, D: 0.01717287) (0.224 sec/batch)\n",
      "2017-04-05 23:18:16.592594: step 43560, loss = (G: 6.90124416, D: 0.08323567) (0.224 sec/batch)\n",
      "2017-04-05 23:18:21.095453: step 43580, loss = (G: 9.30895615, D: 0.00338501) (0.223 sec/batch)\n",
      "2017-04-05 23:18:25.579002: step 43600, loss = (G: 6.13614321, D: 0.03148271) (0.225 sec/batch)\n",
      "2017-04-05 23:18:30.203722: step 43620, loss = (G: 8.92030144, D: 0.01816978) (0.225 sec/batch)\n",
      "2017-04-05 23:18:34.691745: step 43640, loss = (G: 8.43730640, D: 0.14942715) (0.225 sec/batch)\n",
      "2017-04-05 23:18:39.187819: step 43660, loss = (G: 7.52898073, D: 0.02514810) (0.224 sec/batch)\n",
      "2017-04-05 23:18:43.674271: step 43680, loss = (G: 5.43807983, D: 0.04203340) (0.224 sec/batch)\n",
      "2017-04-05 23:18:48.169698: step 43700, loss = (G: 14.10232544, D: 0.00973278) (0.225 sec/batch)\n",
      "2017-04-05 23:18:52.792010: step 43720, loss = (G: 6.54941463, D: 0.04180066) (0.225 sec/batch)\n",
      "2017-04-05 23:18:57.282065: step 43740, loss = (G: 5.40950346, D: 0.53319234) (0.228 sec/batch)\n",
      "2017-04-05 23:19:01.772944: step 43760, loss = (G: 8.42504120, D: 0.53783005) (0.224 sec/batch)\n",
      "2017-04-05 23:19:06.309527: step 43780, loss = (G: 5.49093437, D: 0.08640295) (0.224 sec/batch)\n",
      "2017-04-05 23:19:10.839284: step 43800, loss = (G: 6.16308117, D: 0.04905584) (0.224 sec/batch)\n",
      "2017-04-05 23:19:15.481266: step 43820, loss = (G: 7.15735865, D: 0.00884178) (0.223 sec/batch)\n",
      "2017-04-05 23:19:19.957731: step 43840, loss = (G: 10.09814167, D: 0.00515503) (0.225 sec/batch)\n",
      "2017-04-05 23:19:24.431352: step 43860, loss = (G: 6.77518654, D: 0.01862007) (0.223 sec/batch)\n",
      "2017-04-05 23:19:28.925446: step 43880, loss = (G: 6.81799316, D: 0.01366058) (0.231 sec/batch)\n",
      "2017-04-05 23:19:33.454867: step 43900, loss = (G: 8.09418869, D: 0.00702806) (0.224 sec/batch)\n",
      "2017-04-05 23:19:38.113225: step 43920, loss = (G: 5.50871277, D: 0.03471116) (0.227 sec/batch)\n",
      "2017-04-05 23:19:42.608739: step 43940, loss = (G: 9.46040916, D: 0.01264561) (0.224 sec/batch)\n",
      "2017-04-05 23:19:47.090778: step 43960, loss = (G: 6.13881683, D: 0.01814839) (0.225 sec/batch)\n",
      "2017-04-05 23:19:51.575957: step 43980, loss = (G: 13.13536453, D: 0.00373080) (0.225 sec/batch)\n",
      "2017-04-05 23:19:56.066573: step 44000, loss = (G: 6.62088108, D: 0.05694687) (0.224 sec/batch)\n",
      "2017-04-05 23:20:00.710967: step 44020, loss = (G: 4.93220139, D: 0.32770523) (0.230 sec/batch)\n",
      "2017-04-05 23:20:05.202131: step 44040, loss = (G: 2.23976326, D: 1.88526309) (0.224 sec/batch)\n",
      "2017-04-05 23:20:09.812133: step 44060, loss = (G: 6.58522749, D: 0.06442969) (0.222 sec/batch)\n",
      "2017-04-05 23:20:14.298432: step 44080, loss = (G: 14.11305237, D: 0.08665391) (0.223 sec/batch)\n",
      "2017-04-05 23:20:18.785917: step 44100, loss = (G: 6.66805458, D: 0.04310531) (0.226 sec/batch)\n",
      "2017-04-05 23:20:23.422186: step 44120, loss = (G: 7.80471849, D: 0.01726835) (0.224 sec/batch)\n",
      "2017-04-05 23:20:27.911935: step 44140, loss = (G: 6.65107632, D: 0.13188954) (0.223 sec/batch)\n",
      "2017-04-05 23:20:32.414926: step 44160, loss = (G: 9.83227062, D: 0.72014171) (0.225 sec/batch)\n",
      "2017-04-05 23:20:36.895098: step 44180, loss = (G: 6.10802555, D: 0.02731054) (0.224 sec/batch)\n",
      "2017-04-05 23:20:41.392359: step 44200, loss = (G: 5.83166027, D: 0.35208035) (0.223 sec/batch)\n",
      "2017-04-05 23:20:46.032291: step 44220, loss = (G: 8.72368145, D: 0.04645061) (0.224 sec/batch)\n",
      "2017-04-05 23:20:50.514828: step 44240, loss = (G: 8.53976154, D: 0.05603060) (0.224 sec/batch)\n",
      "2017-04-05 23:20:55.008140: step 44260, loss = (G: 7.46027899, D: 0.00799131) (0.224 sec/batch)\n",
      "2017-04-05 23:20:59.493024: step 44280, loss = (G: 4.18590832, D: 0.12481266) (0.225 sec/batch)\n",
      "2017-04-05 23:21:03.992162: step 44300, loss = (G: 3.36489534, D: 0.26705849) (0.225 sec/batch)\n",
      "2017-04-05 23:21:08.620207: step 44320, loss = (G: 5.91954899, D: 0.14961147) (0.225 sec/batch)\n",
      "2017-04-05 23:21:13.108205: step 44340, loss = (G: 6.05960798, D: 0.04042651) (0.224 sec/batch)\n",
      "2017-04-05 23:21:17.596552: step 44360, loss = (G: 15.71044350, D: 1.18882418) (0.226 sec/batch)\n",
      "2017-04-05 23:21:22.091074: step 44380, loss = (G: 5.43381548, D: 0.25925273) (0.225 sec/batch)\n",
      "2017-04-05 23:21:26.619417: step 44400, loss = (G: 12.56666183, D: 0.03322288) (0.226 sec/batch)\n",
      "2017-04-05 23:21:31.243671: step 44420, loss = (G: 7.90662289, D: 0.04504171) (0.223 sec/batch)\n",
      "2017-04-05 23:21:35.726476: step 44440, loss = (G: 6.28503942, D: 0.04262423) (0.223 sec/batch)\n",
      "2017-04-05 23:21:40.267187: step 44460, loss = (G: 3.35220337, D: 0.33185631) (0.225 sec/batch)\n",
      "2017-04-05 23:21:44.740399: step 44480, loss = (G: 5.31358957, D: 0.06027748) (0.223 sec/batch)\n",
      "2017-04-05 23:21:49.220428: step 44500, loss = (G: 7.15120697, D: 0.01284590) (0.226 sec/batch)\n",
      "2017-04-05 23:21:53.913145: step 44520, loss = (G: 12.53056526, D: 0.03923530) (0.228 sec/batch)\n",
      "2017-04-05 23:21:58.384319: step 44540, loss = (G: 6.61238813, D: 0.01411540) (0.224 sec/batch)\n",
      "2017-04-05 23:22:02.869353: step 44560, loss = (G: 10.07030106, D: 0.00429578) (0.224 sec/batch)\n",
      "2017-04-05 23:22:07.344803: step 44580, loss = (G: 5.80625010, D: 0.10227823) (0.224 sec/batch)\n",
      "2017-04-05 23:22:11.874553: step 44600, loss = (G: 6.81820488, D: 0.01888797) (0.224 sec/batch)\n",
      "2017-04-05 23:22:16.494647: step 44620, loss = (G: 8.51006126, D: 0.45793366) (0.226 sec/batch)\n",
      "2017-04-05 23:22:20.978790: step 44640, loss = (G: 9.74316788, D: 0.02875610) (0.225 sec/batch)\n",
      "2017-04-05 23:22:25.465138: step 44660, loss = (G: 5.08781910, D: 0.04565493) (0.225 sec/batch)\n",
      "2017-04-05 23:22:29.942530: step 44680, loss = (G: 6.42229843, D: 0.01973172) (0.223 sec/batch)\n",
      "2017-04-05 23:22:34.476771: step 44700, loss = (G: 7.03778315, D: 0.02009494) (0.225 sec/batch)\n",
      "2017-04-05 23:22:39.098923: step 44720, loss = (G: 4.82670975, D: 0.10914613) (0.224 sec/batch)\n",
      "2017-04-05 23:22:43.578964: step 44740, loss = (G: 7.99543571, D: 0.01121332) (0.224 sec/batch)\n",
      "2017-04-05 23:22:48.071607: step 44760, loss = (G: 8.59170914, D: 0.03697835) (0.225 sec/batch)\n",
      "2017-04-05 23:22:52.561013: step 44780, loss = (G: 6.17530203, D: 0.01945635) (0.224 sec/batch)\n",
      "2017-04-05 23:22:57.060663: step 44800, loss = (G: 7.39463902, D: 0.01332512) (0.225 sec/batch)\n",
      "2017-04-05 23:23:01.674856: step 44820, loss = (G: 7.19700956, D: 0.00715332) (0.223 sec/batch)\n",
      "2017-04-05 23:23:06.181933: step 44840, loss = (G: 8.43092251, D: 0.01954664) (0.224 sec/batch)\n",
      "2017-04-05 23:23:10.672349: step 44860, loss = (G: 5.01544714, D: 0.09626856) (0.223 sec/batch)\n",
      "2017-04-05 23:23:15.162147: step 44880, loss = (G: 8.26230717, D: 0.01173899) (0.224 sec/batch)\n",
      "2017-04-05 23:23:19.659843: step 44900, loss = (G: 7.68756580, D: 0.05924973) (0.224 sec/batch)\n",
      "2017-04-05 23:23:24.279485: step 44920, loss = (G: 3.69976616, D: 0.13653883) (0.224 sec/batch)\n",
      "2017-04-05 23:23:28.780931: step 44940, loss = (G: 3.83884573, D: 0.16930611) (0.224 sec/batch)\n",
      "2017-04-05 23:23:33.279108: step 44960, loss = (G: 8.21798325, D: 0.12707132) (0.224 sec/batch)\n",
      "2017-04-05 23:23:37.765118: step 44980, loss = (G: 10.27130127, D: 0.00401070) (0.224 sec/batch)\n",
      "2017-04-05 23:23:42.260028: step 45000, loss = (G: 8.96905422, D: 0.00321310) (0.225 sec/batch)\n",
      "2017-04-05 23:23:46.884203: step 45020, loss = (G: 4.78079510, D: 0.06987800) (0.224 sec/batch)\n",
      "2017-04-05 23:23:51.364840: step 45040, loss = (G: 7.28428555, D: 0.01067620) (0.224 sec/batch)\n",
      "2017-04-05 23:23:55.860792: step 45060, loss = (G: 5.84036112, D: 0.02180715) (0.225 sec/batch)\n",
      "2017-04-05 23:24:00.348849: step 45080, loss = (G: 7.08586121, D: 0.02897318) (0.224 sec/batch)\n",
      "2017-04-05 23:24:04.833210: step 45100, loss = (G: 6.86594009, D: 0.01705884) (0.225 sec/batch)\n",
      "2017-04-05 23:24:09.458272: step 45120, loss = (G: 5.87516975, D: 0.09063599) (0.225 sec/batch)\n",
      "2017-04-05 23:24:13.945881: step 45140, loss = (G: 6.45688200, D: 0.01328501) (0.224 sec/batch)\n",
      "2017-04-05 23:24:18.430339: step 45160, loss = (G: 4.07869768, D: 0.23975860) (0.224 sec/batch)\n",
      "2017-04-05 23:24:22.931994: step 45180, loss = (G: 8.57114029, D: 0.21142429) (0.224 sec/batch)\n",
      "2017-04-05 23:24:27.469653: step 45200, loss = (G: 4.94766617, D: 0.08414602) (0.224 sec/batch)\n",
      "2017-04-05 23:24:32.225483: step 45220, loss = (G: 4.35877609, D: 0.23894191) (0.225 sec/batch)\n",
      "2017-04-05 23:24:36.718561: step 45240, loss = (G: 3.76470566, D: 0.19959825) (0.224 sec/batch)\n",
      "2017-04-05 23:24:41.201643: step 45260, loss = (G: 7.95266533, D: 0.00454616) (0.226 sec/batch)\n",
      "2017-04-05 23:24:45.695066: step 45280, loss = (G: 5.06206274, D: 0.08993337) (0.225 sec/batch)\n",
      "2017-04-05 23:24:50.194634: step 45300, loss = (G: 5.17523432, D: 0.04096929) (0.225 sec/batch)\n",
      "2017-04-05 23:24:54.818765: step 45320, loss = (G: 6.43880796, D: 0.01964212) (0.226 sec/batch)\n",
      "2017-04-05 23:24:59.308680: step 45340, loss = (G: 7.08845663, D: 0.00884646) (0.225 sec/batch)\n",
      "2017-04-05 23:25:03.792028: step 45360, loss = (G: 4.90313911, D: 0.07936433) (0.224 sec/batch)\n",
      "2017-04-05 23:25:08.288291: step 45380, loss = (G: 7.81103945, D: 0.00453084) (0.231 sec/batch)\n",
      "2017-04-05 23:25:12.812243: step 45400, loss = (G: 8.70073986, D: 0.01339477) (0.225 sec/batch)\n",
      "2017-04-05 23:25:17.438380: step 45420, loss = (G: 4.64041710, D: 0.08301388) (0.225 sec/batch)\n",
      "2017-04-05 23:25:21.935526: step 45440, loss = (G: 6.78589058, D: 0.01764429) (0.225 sec/batch)\n",
      "2017-04-05 23:25:26.422196: step 45460, loss = (G: 6.46603394, D: 0.02459016) (0.223 sec/batch)\n",
      "2017-04-05 23:25:30.923742: step 45480, loss = (G: 6.33875465, D: 0.01508441) (0.225 sec/batch)\n",
      "2017-04-05 23:25:35.410467: step 45500, loss = (G: 5.66164064, D: 0.03206239) (0.224 sec/batch)\n",
      "2017-04-05 23:25:40.043278: step 45520, loss = (G: 9.86994743, D: 0.05006567) (0.230 sec/batch)\n",
      "2017-04-05 23:25:44.538585: step 45540, loss = (G: 5.89937353, D: 0.02533490) (0.224 sec/batch)\n",
      "2017-04-05 23:25:49.016869: step 45560, loss = (G: 3.86580443, D: 0.17988017) (0.224 sec/batch)\n",
      "2017-04-05 23:25:53.528472: step 45580, loss = (G: 8.09108067, D: 0.02302317) (0.224 sec/batch)\n",
      "2017-04-05 23:25:58.009291: step 45600, loss = (G: 7.43866158, D: 0.13704339) (0.223 sec/batch)\n",
      "2017-04-05 23:26:02.658644: step 45620, loss = (G: 6.20788622, D: 0.03106124) (0.223 sec/batch)\n",
      "2017-04-05 23:26:07.152642: step 45640, loss = (G: 10.65074921, D: 0.04731427) (0.224 sec/batch)\n",
      "2017-04-05 23:26:11.651396: step 45660, loss = (G: 7.33494759, D: 0.01010182) (0.227 sec/batch)\n",
      "2017-04-05 23:26:16.146730: step 45680, loss = (G: 7.17849588, D: 0.09034587) (0.225 sec/batch)\n",
      "2017-04-05 23:26:20.635680: step 45700, loss = (G: 4.41662836, D: 0.14137064) (0.225 sec/batch)\n",
      "2017-04-05 23:26:25.278009: step 45720, loss = (G: 8.56659698, D: 0.01532754) (0.229 sec/batch)\n",
      "2017-04-05 23:26:29.768159: step 45740, loss = (G: 5.87894964, D: 0.06023143) (0.224 sec/batch)\n",
      "2017-04-05 23:26:34.313514: step 45760, loss = (G: 12.17340279, D: 0.06629610) (0.226 sec/batch)\n",
      "2017-04-05 23:26:38.786815: step 45780, loss = (G: 12.59820175, D: 0.12118074) (0.224 sec/batch)\n",
      "2017-04-05 23:26:43.264890: step 45800, loss = (G: 5.58683491, D: 0.05940314) (0.224 sec/batch)\n",
      "2017-04-05 23:26:47.896354: step 45820, loss = (G: 9.78451157, D: 0.02364452) (0.226 sec/batch)\n",
      "2017-04-05 23:26:52.441296: step 45840, loss = (G: 6.85169125, D: 0.01444056) (0.224 sec/batch)\n",
      "2017-04-05 23:26:56.918532: step 45860, loss = (G: 7.92021990, D: 0.04775318) (0.224 sec/batch)\n",
      "2017-04-05 23:27:01.412330: step 45880, loss = (G: 6.51269913, D: 0.02959944) (0.224 sec/batch)\n",
      "2017-04-05 23:27:05.933118: step 45900, loss = (G: 9.81948376, D: 0.01350617) (0.230 sec/batch)\n",
      "2017-04-05 23:27:10.553356: step 45920, loss = (G: 12.64813137, D: 0.05378238) (0.223 sec/batch)\n",
      "2017-04-05 23:27:15.039135: step 45940, loss = (G: 8.78260803, D: 0.00850604) (0.225 sec/batch)\n",
      "2017-04-05 23:27:19.528871: step 45960, loss = (G: 6.89910173, D: 0.01040323) (0.224 sec/batch)\n",
      "2017-04-05 23:27:24.029961: step 45980, loss = (G: 6.99297380, D: 0.07952949) (0.225 sec/batch)\n",
      "2017-04-05 23:27:28.518565: step 46000, loss = (G: 6.12798691, D: 0.07932734) (0.224 sec/batch)\n",
      "2017-04-05 23:27:33.146690: step 46020, loss = (G: 6.37497044, D: 0.02986519) (0.224 sec/batch)\n",
      "2017-04-05 23:27:37.635940: step 46040, loss = (G: 6.00116491, D: 0.02141344) (0.225 sec/batch)\n",
      "2017-04-05 23:27:42.123719: step 46060, loss = (G: 5.75669670, D: 0.04428260) (0.223 sec/batch)\n",
      "2017-04-05 23:27:46.613891: step 46080, loss = (G: 6.45599651, D: 0.01063403) (0.225 sec/batch)\n",
      "2017-04-05 23:27:51.111908: step 46100, loss = (G: 5.22643328, D: 0.02548685) (0.231 sec/batch)\n",
      "2017-04-05 23:27:55.765715: step 46120, loss = (G: 6.93675232, D: 0.04164471) (0.223 sec/batch)\n",
      "2017-04-05 23:28:00.265432: step 46140, loss = (G: 9.44440365, D: 0.10834117) (0.224 sec/batch)\n",
      "2017-04-05 23:28:04.755586: step 46160, loss = (G: 12.72038555, D: 0.96053427) (0.225 sec/batch)\n",
      "2017-04-05 23:28:09.240040: step 46180, loss = (G: 6.44531679, D: 0.01694587) (0.224 sec/batch)\n",
      "2017-04-05 23:28:13.742246: step 46200, loss = (G: 6.59664345, D: 0.01284923) (0.224 sec/batch)\n",
      "2017-04-05 23:28:18.366942: step 46220, loss = (G: 5.72611713, D: 0.02593897) (0.223 sec/batch)\n",
      "2017-04-05 23:28:22.861680: step 46240, loss = (G: 8.92311096, D: 0.00371941) (0.225 sec/batch)\n",
      "2017-04-05 23:28:27.351581: step 46260, loss = (G: 6.48285055, D: 0.07137442) (0.230 sec/batch)\n",
      "2017-04-05 23:28:31.882432: step 46280, loss = (G: 6.92461061, D: 0.01181053) (0.224 sec/batch)\n",
      "2017-04-05 23:28:36.372611: step 46300, loss = (G: 4.70773172, D: 0.14648317) (0.224 sec/batch)\n",
      "2017-04-05 23:28:41.008055: step 46320, loss = (G: 5.16199255, D: 0.03929590) (0.226 sec/batch)\n",
      "2017-04-05 23:28:45.495622: step 46340, loss = (G: 12.98121643, D: 0.11090446) (0.224 sec/batch)\n",
      "2017-04-05 23:28:49.992335: step 46360, loss = (G: 7.01567221, D: 0.01748158) (0.227 sec/batch)\n",
      "2017-04-05 23:28:54.570768: step 46380, loss = (G: 16.61709404, D: 0.25665438) (0.223 sec/batch)\n",
      "2017-04-05 23:28:59.063594: step 46400, loss = (G: 4.58146191, D: 0.08080415) (0.226 sec/batch)\n",
      "2017-04-05 23:29:03.721562: step 46420, loss = (G: 7.61721706, D: 0.08296458) (0.229 sec/batch)\n",
      "2017-04-05 23:29:08.218569: step 46440, loss = (G: 7.06995201, D: 0.03818210) (0.224 sec/batch)\n",
      "2017-04-05 23:29:12.695255: step 46460, loss = (G: 5.25725317, D: 0.07676636) (0.224 sec/batch)\n",
      "2017-04-05 23:29:17.186695: step 46480, loss = (G: 6.59938622, D: 0.04630749) (0.224 sec/batch)\n",
      "2017-04-05 23:29:21.680800: step 46500, loss = (G: 8.39867592, D: 0.20907423) (0.225 sec/batch)\n",
      "2017-04-05 23:29:26.308285: step 46520, loss = (G: 7.90290546, D: 0.00436898) (0.224 sec/batch)\n",
      "2017-04-05 23:29:30.856173: step 46540, loss = (G: 6.04318476, D: 0.02165404) (0.230 sec/batch)\n",
      "2017-04-05 23:29:35.363582: step 46560, loss = (G: 7.81786299, D: 0.00940666) (0.224 sec/batch)\n",
      "2017-04-05 23:29:39.852156: step 46580, loss = (G: 5.82912874, D: 0.14577377) (0.225 sec/batch)\n",
      "2017-04-05 23:29:44.342233: step 46600, loss = (G: 12.18806553, D: 0.08488496) (0.224 sec/batch)\n",
      "2017-04-05 23:29:49.006309: step 46620, loss = (G: 11.94377708, D: 0.17797391) (0.223 sec/batch)\n",
      "2017-04-05 23:29:53.480741: step 46640, loss = (G: 6.90795612, D: 0.01991185) (0.223 sec/batch)\n",
      "2017-04-05 23:29:57.966097: step 46660, loss = (G: 13.71244431, D: 0.00029731) (0.224 sec/batch)\n",
      "2017-04-05 23:30:02.493354: step 46680, loss = (G: 7.53213549, D: 0.07716487) (0.227 sec/batch)\n",
      "2017-04-05 23:30:06.976825: step 46700, loss = (G: 9.53751373, D: 0.02337625) (0.223 sec/batch)\n",
      "2017-04-05 23:30:11.595432: step 46720, loss = (G: 5.76001930, D: 0.05804861) (0.224 sec/batch)\n",
      "2017-04-05 23:30:16.076279: step 46740, loss = (G: 4.51518393, D: 0.26547697) (0.225 sec/batch)\n",
      "2017-04-05 23:30:20.564815: step 46760, loss = (G: 6.36277151, D: 0.02799876) (0.222 sec/batch)\n",
      "2017-04-05 23:30:25.055885: step 46780, loss = (G: 7.40061951, D: 0.01269161) (0.224 sec/batch)\n",
      "2017-04-05 23:30:29.550004: step 46800, loss = (G: 4.72838306, D: 0.25173581) (0.224 sec/batch)\n",
      "2017-04-05 23:30:34.168527: step 46820, loss = (G: 5.78422022, D: 0.01950223) (0.224 sec/batch)\n",
      "2017-04-05 23:30:38.650434: step 46840, loss = (G: 7.35605526, D: 0.00899360) (0.225 sec/batch)\n",
      "2017-04-05 23:30:43.175000: step 46860, loss = (G: 16.37047195, D: 0.62765080) (0.225 sec/batch)\n",
      "2017-04-05 23:30:47.656555: step 46880, loss = (G: 8.49076271, D: 0.00635913) (0.225 sec/batch)\n",
      "2017-04-05 23:30:52.142390: step 46900, loss = (G: 11.25144482, D: 0.00677213) (0.231 sec/batch)\n",
      "2017-04-05 23:30:56.813416: step 46920, loss = (G: 6.43431377, D: 0.04084545) (0.224 sec/batch)\n",
      "2017-04-05 23:31:01.299453: step 46940, loss = (G: 5.63016653, D: 0.02224180) (0.225 sec/batch)\n",
      "2017-04-05 23:31:05.792993: step 46960, loss = (G: 7.82730389, D: 0.00906246) (0.224 sec/batch)\n",
      "2017-04-05 23:31:10.294706: step 46980, loss = (G: 15.63287926, D: 0.00064892) (0.223 sec/batch)\n",
      "2017-04-05 23:31:14.770214: step 47000, loss = (G: 7.63521147, D: 0.00400094) (0.224 sec/batch)\n",
      "2017-04-05 23:31:19.404987: step 47020, loss = (G: 9.38929081, D: 0.01678780) (0.223 sec/batch)\n",
      "2017-04-05 23:31:23.899616: step 47040, loss = (G: 6.04551601, D: 0.01678855) (0.224 sec/batch)\n",
      "2017-04-05 23:31:28.429141: step 47060, loss = (G: 8.34650040, D: 0.03009282) (0.224 sec/batch)\n",
      "2017-04-05 23:31:32.907256: step 47080, loss = (G: 7.22969723, D: 0.13887051) (0.223 sec/batch)\n",
      "2017-04-05 23:31:37.405206: step 47100, loss = (G: 9.36670303, D: 0.01657762) (0.224 sec/batch)\n",
      "2017-04-05 23:31:42.033222: step 47120, loss = (G: 4.89435816, D: 0.08223870) (0.225 sec/batch)\n",
      "2017-04-05 23:31:46.521457: step 47140, loss = (G: 6.82746840, D: 0.01227293) (0.225 sec/batch)\n",
      "2017-04-05 23:31:51.002565: step 47160, loss = (G: 11.84659386, D: 0.00670408) (0.224 sec/batch)\n",
      "2017-04-05 23:31:55.494364: step 47180, loss = (G: 6.78302193, D: 0.01962266) (0.225 sec/batch)\n",
      "2017-04-05 23:31:59.982286: step 47200, loss = (G: 4.83559608, D: 0.04790120) (0.224 sec/batch)\n",
      "2017-04-05 23:32:04.635115: step 47220, loss = (G: 7.17760468, D: 0.02567281) (0.224 sec/batch)\n",
      "2017-04-05 23:32:09.123935: step 47240, loss = (G: 3.93618608, D: 0.13619561) (0.223 sec/batch)\n",
      "2017-04-05 23:32:13.602943: step 47260, loss = (G: 5.74765396, D: 0.02074926) (0.225 sec/batch)\n",
      "2017-04-05 23:32:18.094292: step 47280, loss = (G: 11.15492439, D: 0.00427863) (0.224 sec/batch)\n",
      "2017-04-05 23:32:22.583005: step 47300, loss = (G: 6.35525799, D: 0.03172505) (0.226 sec/batch)\n",
      "2017-04-05 23:32:27.215180: step 47320, loss = (G: 5.87007523, D: 0.18158568) (0.226 sec/batch)\n",
      "2017-04-05 23:32:31.702369: step 47340, loss = (G: 4.63308620, D: 0.06770270) (0.224 sec/batch)\n",
      "2017-04-05 23:32:36.207544: step 47360, loss = (G: 9.99085999, D: 0.20037194) (0.230 sec/batch)\n",
      "2017-04-05 23:32:40.725625: step 47380, loss = (G: 7.02853680, D: 0.01134517) (0.223 sec/batch)\n",
      "2017-04-05 23:32:45.214497: step 47400, loss = (G: 6.62323236, D: 0.01781258) (0.226 sec/batch)\n",
      "2017-04-05 23:32:49.855318: step 47420, loss = (G: 10.23580551, D: 0.06449559) (0.224 sec/batch)\n",
      "2017-04-05 23:32:54.340731: step 47440, loss = (G: 7.52969456, D: 0.04689453) (0.224 sec/batch)\n",
      "2017-04-05 23:32:58.827889: step 47460, loss = (G: 14.40664673, D: 0.11656569) (0.225 sec/batch)\n",
      "2017-04-05 23:33:03.315911: step 47480, loss = (G: 5.34038401, D: 0.10403711) (0.224 sec/batch)\n",
      "2017-04-05 23:33:07.800348: step 47500, loss = (G: 11.18946075, D: 0.00059039) (0.224 sec/batch)\n",
      "2017-04-05 23:33:12.420994: step 47520, loss = (G: 8.62950420, D: 0.00849630) (0.224 sec/batch)\n",
      "2017-04-05 23:33:17.038778: step 47540, loss = (G: 6.69070530, D: 0.01355333) (0.227 sec/batch)\n",
      "2017-04-05 23:33:21.533564: step 47560, loss = (G: 7.93114567, D: 0.02099296) (0.224 sec/batch)\n",
      "2017-04-05 23:33:26.018516: step 47580, loss = (G: 8.47095299, D: 0.01623098) (0.224 sec/batch)\n",
      "2017-04-05 23:33:30.523832: step 47600, loss = (G: 5.64903593, D: 0.03686932) (0.224 sec/batch)\n",
      "2017-04-05 23:33:35.157795: step 47620, loss = (G: 7.69628239, D: 0.01886911) (0.224 sec/batch)\n",
      "2017-04-05 23:33:39.644414: step 47640, loss = (G: 10.81907272, D: 0.01990130) (0.224 sec/batch)\n",
      "2017-04-05 23:33:44.133982: step 47660, loss = (G: 9.16888905, D: 0.00338686) (0.225 sec/batch)\n",
      "2017-04-05 23:33:48.629217: step 47680, loss = (G: 2.99649811, D: 0.32716140) (0.224 sec/batch)\n",
      "2017-04-05 23:33:53.115973: step 47700, loss = (G: 4.74328995, D: 0.07302772) (0.224 sec/batch)\n",
      "2017-04-05 23:33:57.739718: step 47720, loss = (G: 5.46277618, D: 0.05242617) (0.224 sec/batch)\n",
      "2017-04-05 23:34:02.231085: step 47740, loss = (G: 6.63308430, D: 0.01179401) (0.226 sec/batch)\n",
      "2017-04-05 23:34:06.716791: step 47760, loss = (G: 6.99549961, D: 0.07469582) (0.224 sec/batch)\n",
      "2017-04-05 23:34:11.251791: step 47780, loss = (G: 4.46616364, D: 0.08114484) (0.225 sec/batch)\n",
      "2017-04-05 23:34:15.730232: step 47800, loss = (G: 9.38193226, D: 0.00292829) (0.223 sec/batch)\n",
      "2017-04-05 23:34:20.347836: step 47820, loss = (G: 9.70844555, D: 0.14032952) (0.224 sec/batch)\n",
      "2017-04-05 23:34:24.843525: step 47840, loss = (G: 4.60667944, D: 0.14563784) (0.229 sec/batch)\n",
      "2017-04-05 23:34:29.329074: step 47860, loss = (G: 8.67094517, D: 0.14251590) (0.224 sec/batch)\n",
      "2017-04-05 23:34:33.834930: step 47880, loss = (G: 9.75667000, D: 0.01083318) (0.224 sec/batch)\n",
      "2017-04-05 23:34:38.313933: step 47900, loss = (G: 12.31385040, D: 0.00181009) (0.227 sec/batch)\n",
      "2017-04-05 23:34:42.952031: step 47920, loss = (G: 1.06993210, D: 4.57703257) (0.224 sec/batch)\n",
      "2017-04-05 23:34:47.442616: step 47940, loss = (G: 6.80776548, D: 0.13464756) (0.225 sec/batch)\n",
      "2017-04-05 23:34:51.935075: step 47960, loss = (G: 6.04691887, D: 0.05574138) (0.225 sec/batch)\n",
      "2017-04-05 23:34:56.421780: step 47980, loss = (G: 8.40116024, D: 0.01859991) (0.225 sec/batch)\n",
      "2017-04-05 23:35:00.979923: step 48000, loss = (G: 6.68510818, D: 0.01630202) (0.223 sec/batch)\n",
      "2017-04-05 23:35:05.612171: step 48020, loss = (G: 5.83657837, D: 0.01847158) (0.223 sec/batch)\n",
      "2017-04-05 23:35:10.106627: step 48040, loss = (G: 5.83355713, D: 0.26939279) (0.225 sec/batch)\n",
      "2017-04-05 23:35:14.600434: step 48060, loss = (G: 10.53741264, D: 0.02545986) (0.226 sec/batch)\n",
      "2017-04-05 23:35:19.083382: step 48080, loss = (G: 6.46860600, D: 0.04702728) (0.224 sec/batch)\n",
      "2017-04-05 23:35:23.569991: step 48100, loss = (G: 5.15233612, D: 0.09273054) (0.224 sec/batch)\n",
      "2017-04-05 23:35:28.194703: step 48120, loss = (G: 8.64906025, D: 0.00383784) (0.224 sec/batch)\n",
      "2017-04-05 23:35:32.703244: step 48140, loss = (G: 6.94641161, D: 0.01391492) (0.224 sec/batch)\n",
      "2017-04-05 23:35:37.182796: step 48160, loss = (G: 12.18602562, D: 0.02021725) (0.224 sec/batch)\n",
      "2017-04-05 23:35:41.683008: step 48180, loss = (G: 5.34585333, D: 0.06838947) (0.230 sec/batch)\n",
      "2017-04-05 23:35:46.208169: step 48200, loss = (G: 17.76376534, D: 0.08563890) (0.224 sec/batch)\n",
      "2017-04-05 23:35:50.835589: step 48220, loss = (G: 9.11104584, D: 0.00183885) (0.224 sec/batch)\n",
      "2017-04-05 23:35:55.332824: step 48240, loss = (G: 7.50083637, D: 0.02399178) (0.224 sec/batch)\n",
      "2017-04-05 23:35:59.827122: step 48260, loss = (G: 8.14867496, D: 0.02422212) (0.225 sec/batch)\n",
      "2017-04-05 23:36:04.342158: step 48280, loss = (G: 4.72831964, D: 0.07678721) (0.229 sec/batch)\n",
      "2017-04-05 23:36:08.834751: step 48300, loss = (G: 11.39709949, D: 0.00562364) (0.224 sec/batch)\n",
      "2017-04-05 23:36:13.467334: step 48320, loss = (G: 7.05454731, D: 0.01823539) (0.224 sec/batch)\n",
      "2017-04-05 23:36:17.950066: step 48340, loss = (G: 4.45990849, D: 0.16017914) (0.224 sec/batch)\n",
      "2017-04-05 23:36:22.470679: step 48360, loss = (G: 10.04850578, D: 0.00247653) (0.225 sec/batch)\n",
      "2017-04-05 23:36:26.951558: step 48380, loss = (G: 5.42053604, D: 0.16302893) (0.224 sec/batch)\n",
      "2017-04-05 23:36:31.432329: step 48400, loss = (G: 4.80373430, D: 0.06940977) (0.225 sec/batch)\n",
      "2017-04-05 23:36:36.074276: step 48420, loss = (G: 6.99841690, D: 0.01818839) (0.225 sec/batch)\n",
      "2017-04-05 23:36:40.559473: step 48440, loss = (G: 4.68319702, D: 0.07049177) (0.225 sec/batch)\n",
      "2017-04-05 23:36:45.070982: step 48460, loss = (G: 7.30342674, D: 0.01383509) (0.230 sec/batch)\n",
      "2017-04-05 23:36:49.594142: step 48480, loss = (G: 6.45057678, D: 0.02281173) (0.225 sec/batch)\n",
      "2017-04-05 23:36:54.068244: step 48500, loss = (G: 6.33573389, D: 0.03249266) (0.224 sec/batch)\n",
      "2017-04-05 23:36:58.693217: step 48520, loss = (G: 8.60984516, D: 0.04298718) (0.223 sec/batch)\n",
      "2017-04-05 23:37:03.228648: step 48540, loss = (G: 2.98531818, D: 0.41975501) (0.226 sec/batch)\n",
      "2017-04-05 23:37:07.708917: step 48560, loss = (G: 5.89837265, D: 0.03209577) (0.223 sec/batch)\n",
      "2017-04-05 23:37:12.207143: step 48580, loss = (G: 8.69290638, D: 0.02660487) (0.227 sec/batch)\n",
      "2017-04-05 23:37:16.701098: step 48600, loss = (G: 5.59243584, D: 0.02371179) (0.224 sec/batch)\n",
      "2017-04-05 23:37:21.332616: step 48620, loss = (G: 7.69574165, D: 0.01098917) (0.225 sec/batch)\n",
      "2017-04-05 23:37:25.822567: step 48640, loss = (G: 7.79172087, D: 0.01015751) (0.225 sec/batch)\n",
      "2017-04-05 23:37:30.333046: step 48660, loss = (G: 2.73581123, D: 0.53208274) (0.225 sec/batch)\n",
      "2017-04-05 23:37:34.817495: step 48680, loss = (G: 12.51592064, D: 0.00027112) (0.225 sec/batch)\n",
      "2017-04-05 23:37:39.485086: step 48700, loss = (G: 6.33296013, D: 0.01675144) (0.225 sec/batch)\n",
      "2017-04-05 23:37:44.130706: step 48720, loss = (G: 3.00618052, D: 0.41685176) (0.225 sec/batch)\n",
      "2017-04-05 23:37:48.637805: step 48740, loss = (G: 2.41476703, D: 0.73530763) (0.225 sec/batch)\n",
      "2017-04-05 23:37:53.127516: step 48760, loss = (G: 6.25480556, D: 0.02078365) (0.223 sec/batch)\n",
      "2017-04-05 23:37:57.616868: step 48780, loss = (G: 1.56914532, D: 3.06658268) (0.225 sec/batch)\n",
      "2017-04-05 23:38:02.156540: step 48800, loss = (G: 6.19041061, D: 0.01937170) (0.225 sec/batch)\n",
      "2017-04-05 23:38:06.778371: step 48820, loss = (G: 8.97529602, D: 0.02076314) (0.223 sec/batch)\n",
      "2017-04-05 23:38:11.255547: step 48840, loss = (G: 11.89928246, D: 0.20868956) (0.225 sec/batch)\n",
      "2017-04-05 23:38:15.738872: step 48860, loss = (G: 6.07843018, D: 0.08422025) (0.224 sec/batch)\n",
      "2017-04-05 23:38:20.244780: step 48880, loss = (G: 37.08090591, D: 0.46196753) (0.224 sec/batch)\n",
      "2017-04-05 23:38:24.734322: step 48900, loss = (G: 8.36816692, D: 0.01032798) (0.224 sec/batch)\n",
      "2017-04-05 23:38:29.356898: step 48920, loss = (G: 6.43177795, D: 0.01509693) (0.224 sec/batch)\n",
      "2017-04-05 23:38:33.843093: step 48940, loss = (G: 8.76379490, D: 0.21831204) (0.225 sec/batch)\n",
      "2017-04-05 23:38:38.334571: step 48960, loss = (G: 7.68955994, D: 0.01094811) (0.225 sec/batch)\n",
      "2017-04-05 23:38:42.827263: step 48980, loss = (G: 8.12086296, D: 0.02405498) (0.225 sec/batch)\n",
      "2017-04-05 23:38:47.321197: step 49000, loss = (G: 8.51449299, D: 0.04407251) (0.224 sec/batch)\n",
      "2017-04-05 23:38:51.961732: step 49020, loss = (G: 6.20470619, D: 0.05000413) (0.225 sec/batch)\n",
      "2017-04-05 23:38:56.452777: step 49040, loss = (G: 6.72399426, D: 0.02396865) (0.225 sec/batch)\n",
      "2017-04-05 23:39:00.992474: step 49060, loss = (G: 12.76803493, D: 0.00949918) (0.228 sec/batch)\n",
      "2017-04-05 23:39:05.483377: step 49080, loss = (G: 7.86703730, D: 0.02224180) (0.224 sec/batch)\n",
      "2017-04-05 23:39:09.965126: step 49100, loss = (G: 7.57968569, D: 1.17936969) (0.224 sec/batch)\n",
      "2017-04-05 23:39:14.590044: step 49120, loss = (G: 14.82801247, D: 0.00020224) (0.226 sec/batch)\n",
      "2017-04-05 23:39:19.084623: step 49140, loss = (G: 5.73708248, D: 0.09006656) (0.223 sec/batch)\n",
      "2017-04-05 23:39:23.595169: step 49160, loss = (G: 7.29591274, D: 0.01007692) (0.224 sec/batch)\n",
      "2017-04-05 23:39:28.087302: step 49180, loss = (G: 5.65173817, D: 0.04144437) (0.224 sec/batch)\n",
      "2017-04-05 23:39:32.581604: step 49200, loss = (G: 7.99350023, D: 0.09937810) (0.224 sec/batch)\n",
      "2017-04-05 23:39:37.213444: step 49220, loss = (G: 5.01443577, D: 0.07308744) (0.224 sec/batch)\n",
      "2017-04-05 23:39:41.702402: step 49240, loss = (G: 5.75882530, D: 0.46900958) (0.225 sec/batch)\n",
      "2017-04-05 23:39:46.196730: step 49260, loss = (G: 9.82543564, D: 0.03516031) (0.224 sec/batch)\n",
      "2017-04-05 23:39:50.732720: step 49280, loss = (G: 9.40812588, D: 0.00606678) (0.225 sec/batch)\n",
      "2017-04-05 23:39:55.220859: step 49300, loss = (G: 12.20795727, D: 0.01536550) (0.226 sec/batch)\n",
      "2017-04-05 23:39:59.842644: step 49320, loss = (G: 7.59761667, D: 0.00761443) (0.223 sec/batch)\n",
      "2017-04-05 23:40:04.337625: step 49340, loss = (G: 11.77739716, D: 0.00356388) (0.226 sec/batch)\n",
      "2017-04-05 23:40:08.834663: step 49360, loss = (G: 7.73376989, D: 0.02347009) (0.225 sec/batch)\n",
      "2017-04-05 23:40:13.348297: step 49380, loss = (G: 8.10504627, D: 0.00435472) (0.231 sec/batch)\n",
      "2017-04-05 23:40:17.859230: step 49400, loss = (G: 11.47619152, D: 0.02572981) (0.224 sec/batch)\n",
      "2017-04-05 23:40:22.482671: step 49420, loss = (G: 5.16500473, D: 0.07486847) (0.224 sec/batch)\n",
      "2017-04-05 23:40:26.968043: step 49440, loss = (G: 4.94755268, D: 0.10422909) (0.225 sec/batch)\n",
      "2017-04-05 23:40:31.455783: step 49460, loss = (G: 5.69527912, D: 0.03484095) (0.224 sec/batch)\n",
      "2017-04-05 23:40:35.939227: step 49480, loss = (G: 5.97783279, D: 0.02096763) (0.224 sec/batch)\n",
      "2017-04-05 23:40:40.424956: step 49500, loss = (G: 6.66341877, D: 0.01336880) (0.224 sec/batch)\n",
      "2017-04-05 23:40:45.052606: step 49520, loss = (G: 4.27035093, D: 0.12000267) (0.225 sec/batch)\n",
      "2017-04-05 23:40:49.539590: step 49540, loss = (G: 7.94191647, D: 0.00728332) (0.225 sec/batch)\n",
      "2017-04-05 23:40:54.040284: step 49560, loss = (G: 5.04763937, D: 0.06363483) (0.224 sec/batch)\n",
      "2017-04-05 23:40:58.532877: step 49580, loss = (G: 6.69818115, D: 0.00815758) (0.223 sec/batch)\n",
      "2017-04-05 23:41:03.041315: step 49600, loss = (G: 7.03472233, D: 0.01557407) (0.224 sec/batch)\n",
      "2017-04-05 23:41:07.671530: step 49620, loss = (G: 8.82813644, D: 0.11966218) (0.224 sec/batch)\n",
      "2017-04-05 23:41:12.159895: step 49640, loss = (G: 9.44761753, D: 0.01993743) (0.224 sec/batch)\n",
      "2017-04-05 23:41:16.653609: step 49660, loss = (G: 5.51199102, D: 0.07987453) (0.224 sec/batch)\n",
      "2017-04-05 23:41:21.154679: step 49680, loss = (G: 9.72641945, D: 0.06399604) (0.224 sec/batch)\n",
      "2017-04-05 23:41:25.695011: step 49700, loss = (G: 10.07175159, D: 0.00488030) (0.229 sec/batch)\n",
      "2017-04-05 23:41:30.341195: step 49720, loss = (G: 6.18020153, D: 0.02341383) (0.224 sec/batch)\n",
      "2017-04-05 23:41:34.836384: step 49740, loss = (G: 10.28658104, D: 0.00181263) (0.225 sec/batch)\n",
      "2017-04-05 23:41:39.336830: step 49760, loss = (G: 8.11586761, D: 0.00395475) (0.225 sec/batch)\n",
      "2017-04-05 23:41:43.825349: step 49780, loss = (G: 8.29947376, D: 0.05937021) (0.224 sec/batch)\n",
      "2017-04-05 23:41:48.309108: step 49800, loss = (G: 9.47885799, D: 0.01380884) (0.224 sec/batch)\n",
      "2017-04-05 23:41:52.945820: step 49820, loss = (G: 7.33785677, D: 0.07967017) (0.224 sec/batch)\n",
      "2017-04-05 23:41:57.452562: step 49840, loss = (G: 7.92172718, D: 0.02080108) (0.226 sec/batch)\n",
      "2017-04-05 23:42:02.098019: step 49860, loss = (G: 5.43641281, D: 0.02728891) (0.230 sec/batch)\n",
      "2017-04-05 23:42:06.596647: step 49880, loss = (G: 8.08270550, D: 0.00313063) (0.225 sec/batch)\n",
      "2017-04-05 23:42:11.089105: step 49900, loss = (G: 3.37526608, D: 0.38641435) (0.228 sec/batch)\n",
      "2017-04-05 23:42:15.731847: step 49920, loss = (G: 7.57393217, D: 0.23696530) (0.225 sec/batch)\n",
      "2017-04-05 23:42:20.220195: step 49940, loss = (G: 5.64920616, D: 0.02891896) (0.224 sec/batch)\n",
      "2017-04-05 23:42:24.715891: step 49960, loss = (G: 10.32761002, D: 0.02147731) (0.223 sec/batch)\n",
      "2017-04-05 23:42:29.244361: step 49980, loss = (G: 11.65895081, D: 0.16631538) (0.225 sec/batch)\n",
      "2017-04-05 23:42:33.734232: step 50000, loss = (G: 8.96073151, D: 0.00445440) (0.225 sec/batch)\n",
      "2017-04-05 23:42:38.359396: step 50020, loss = (G: 10.23037624, D: 0.03549414) (0.225 sec/batch)\n",
      "2017-04-05 23:42:42.855681: step 50040, loss = (G: 5.46004438, D: 0.02829598) (0.225 sec/batch)\n",
      "2017-04-05 23:42:47.346228: step 50060, loss = (G: 8.81886959, D: 0.00162400) (0.225 sec/batch)\n",
      "2017-04-05 23:42:51.860677: step 50080, loss = (G: 6.00935221, D: 0.01439384) (0.226 sec/batch)\n",
      "2017-04-05 23:42:56.355997: step 50100, loss = (G: 10.67932320, D: 0.00228783) (0.223 sec/batch)\n",
      "2017-04-05 23:43:00.982174: step 50120, loss = (G: 10.47930717, D: 0.00325813) (0.225 sec/batch)\n",
      "2017-04-05 23:43:05.472321: step 50140, loss = (G: 6.81786442, D: 0.01897457) (0.226 sec/batch)\n",
      "2017-04-05 23:43:09.962316: step 50160, loss = (G: 8.91126728, D: 0.00507332) (0.224 sec/batch)\n",
      "2017-04-05 23:43:14.466730: step 50180, loss = (G: 6.14404297, D: 0.34383249) (0.223 sec/batch)\n",
      "2017-04-05 23:43:18.966702: step 50200, loss = (G: 6.70854568, D: 0.01552794) (0.225 sec/batch)\n",
      "2017-04-05 23:43:23.595359: step 50220, loss = (G: 7.11742926, D: 0.01332615) (0.225 sec/batch)\n",
      "2017-04-05 23:43:28.098302: step 50240, loss = (G: 7.61603642, D: 0.09175257) (0.224 sec/batch)\n",
      "2017-04-05 23:43:32.583971: step 50260, loss = (G: 7.10623026, D: 0.01276696) (0.224 sec/batch)\n",
      "2017-04-05 23:43:37.069210: step 50280, loss = (G: 7.79694271, D: 0.00360842) (0.225 sec/batch)\n",
      "2017-04-05 23:43:41.570341: step 50300, loss = (G: 7.04244423, D: 0.00886730) (0.224 sec/batch)\n",
      "2017-04-05 23:43:46.199651: step 50320, loss = (G: 6.30276203, D: 0.01938211) (0.225 sec/batch)\n",
      "2017-04-05 23:43:50.695922: step 50340, loss = (G: 6.37575579, D: 0.01369290) (0.225 sec/batch)\n",
      "2017-04-05 23:43:55.188464: step 50360, loss = (G: 5.99930668, D: 0.07775231) (0.225 sec/batch)\n",
      "2017-04-05 23:43:59.675834: step 50380, loss = (G: 8.88169098, D: 0.00392465) (0.224 sec/batch)\n",
      "2017-04-05 23:44:04.168999: step 50400, loss = (G: 6.77368355, D: 0.04142724) (0.225 sec/batch)\n",
      "2017-04-05 23:44:08.807019: step 50420, loss = (G: 8.28597355, D: 0.03929887) (0.224 sec/batch)\n",
      "2017-04-05 23:44:13.313453: step 50440, loss = (G: 4.59608126, D: 0.08967875) (0.223 sec/batch)\n",
      "2017-04-05 23:44:17.815386: step 50460, loss = (G: 7.10113859, D: 0.01368284) (0.224 sec/batch)\n",
      "2017-04-05 23:44:22.313100: step 50480, loss = (G: 10.59576321, D: 0.01356802) (0.224 sec/batch)\n",
      "2017-04-05 23:44:26.802335: step 50500, loss = (G: 7.72751093, D: 0.01350110) (0.224 sec/batch)\n",
      "2017-04-05 23:44:31.447040: step 50520, loss = (G: 8.91750908, D: 0.00392915) (0.224 sec/batch)\n",
      "2017-04-05 23:44:35.944999: step 50540, loss = (G: 7.64970016, D: 0.01088229) (0.224 sec/batch)\n",
      "2017-04-05 23:44:40.440775: step 50560, loss = (G: 7.36187220, D: 0.03954491) (0.226 sec/batch)\n",
      "2017-04-05 23:44:44.945429: step 50580, loss = (G: 8.12935352, D: 0.00758761) (0.224 sec/batch)\n",
      "2017-04-05 23:44:49.450874: step 50600, loss = (G: 5.15856743, D: 0.10235517) (0.224 sec/batch)\n",
      "2017-04-05 23:44:54.077267: step 50620, loss = (G: 5.78018713, D: 0.01478219) (0.225 sec/batch)\n",
      "2017-04-05 23:44:58.577825: step 50640, loss = (G: 6.63053131, D: 0.08766294) (0.224 sec/batch)\n",
      "2017-04-05 23:45:03.065365: step 50660, loss = (G: 6.94491005, D: 0.01471545) (0.224 sec/batch)\n",
      "2017-04-05 23:45:07.554622: step 50680, loss = (G: 4.56359673, D: 0.12492915) (0.225 sec/batch)\n",
      "2017-04-05 23:45:12.046902: step 50700, loss = (G: 6.84748697, D: 0.01300278) (0.224 sec/batch)\n",
      "2017-04-05 23:45:16.690798: step 50720, loss = (G: 7.86213207, D: 0.00889608) (0.225 sec/batch)\n",
      "2017-04-05 23:45:21.187755: step 50740, loss = (G: 10.39188194, D: 0.00364717) (0.224 sec/batch)\n",
      "2017-04-05 23:45:25.736446: step 50760, loss = (G: 7.74160862, D: 0.00712053) (0.224 sec/batch)\n",
      "2017-04-05 23:45:30.218957: step 50780, loss = (G: 19.17568398, D: 0.00670108) (0.224 sec/batch)\n",
      "2017-04-05 23:45:34.707558: step 50800, loss = (G: 16.93156242, D: 0.59404266) (0.225 sec/batch)\n",
      "2017-04-05 23:45:39.344046: step 50820, loss = (G: 11.89734173, D: 0.02917615) (0.224 sec/batch)\n",
      "2017-04-05 23:45:43.839976: step 50840, loss = (G: 5.99394608, D: 0.04943354) (0.225 sec/batch)\n",
      "2017-04-05 23:45:48.351616: step 50860, loss = (G: 14.99866867, D: 0.00601995) (0.231 sec/batch)\n",
      "2017-04-05 23:45:52.845735: step 50880, loss = (G: 5.20841026, D: 0.04468463) (0.223 sec/batch)\n",
      "2017-04-05 23:45:57.341947: step 50900, loss = (G: 5.98887205, D: 0.07253467) (0.224 sec/batch)\n",
      "2017-04-05 23:46:01.966244: step 50920, loss = (G: 4.61428595, D: 0.18011531) (0.225 sec/batch)\n",
      "2017-04-05 23:46:06.448816: step 50940, loss = (G: 6.57650232, D: 0.02287334) (0.224 sec/batch)\n",
      "2017-04-05 23:46:10.977704: step 50960, loss = (G: 11.84599876, D: 0.79954433) (0.225 sec/batch)\n",
      "2017-04-05 23:46:15.465018: step 50980, loss = (G: 5.67593431, D: 0.02131829) (0.225 sec/batch)\n",
      "2017-04-05 23:46:19.955262: step 51000, loss = (G: 5.41786242, D: 0.02289912) (0.225 sec/batch)\n",
      "2017-04-05 23:46:24.712188: step 51020, loss = (G: 7.25014544, D: 0.09645197) (0.227 sec/batch)\n",
      "2017-04-05 23:46:29.191829: step 51040, loss = (G: 6.75249147, D: 0.01584436) (0.226 sec/batch)\n",
      "2017-04-05 23:46:33.699178: step 51060, loss = (G: 6.70788145, D: 0.01453650) (0.230 sec/batch)\n",
      "2017-04-05 23:46:38.212112: step 51080, loss = (G: 6.20837831, D: 0.01922483) (0.224 sec/batch)\n",
      "2017-04-05 23:46:42.703958: step 51100, loss = (G: 4.59267855, D: 0.09010221) (0.224 sec/batch)\n",
      "2017-04-05 23:46:47.344649: step 51120, loss = (G: 5.77265120, D: 0.01626560) (0.225 sec/batch)\n",
      "2017-04-05 23:46:51.834921: step 51140, loss = (G: 7.19845581, D: 0.00812270) (0.223 sec/batch)\n",
      "2017-04-05 23:46:56.328706: step 51160, loss = (G: 8.26091003, D: 0.00800738) (0.224 sec/batch)\n",
      "2017-04-05 23:47:00.822572: step 51180, loss = (G: 8.07485771, D: 0.02712032) (0.224 sec/batch)\n",
      "2017-04-05 23:47:05.314221: step 51200, loss = (G: 13.32997608, D: 0.00016694) (0.224 sec/batch)\n",
      "2017-04-05 23:47:09.949764: step 51220, loss = (G: 6.76241875, D: 0.01789597) (0.227 sec/batch)\n",
      "2017-04-05 23:47:14.490045: step 51240, loss = (G: 6.86408949, D: 0.04046980) (0.228 sec/batch)\n",
      "2017-04-05 23:47:18.975039: step 51260, loss = (G: 8.30047798, D: 0.04502647) (0.225 sec/batch)\n",
      "2017-04-05 23:47:23.486489: step 51280, loss = (G: 6.87040091, D: 0.00928848) (0.230 sec/batch)\n",
      "2017-04-05 23:47:27.996713: step 51300, loss = (G: 8.61641979, D: 0.00290488) (0.223 sec/batch)\n",
      "2017-04-05 23:47:32.654391: step 51320, loss = (G: 8.85931969, D: 0.00611958) (0.225 sec/batch)\n",
      "2017-04-05 23:47:37.145049: step 51340, loss = (G: 6.64400864, D: 0.01180450) (0.225 sec/batch)\n",
      "2017-04-05 23:47:41.628800: step 51360, loss = (G: 8.90245438, D: 0.00185531) (0.225 sec/batch)\n",
      "2017-04-05 23:47:46.139387: step 51380, loss = (G: 9.57527542, D: 0.00324003) (0.225 sec/batch)\n",
      "2017-04-05 23:47:50.624945: step 51400, loss = (G: 5.74734783, D: 0.02995808) (0.224 sec/batch)\n",
      "2017-04-05 23:47:55.261607: step 51420, loss = (G: 6.04997492, D: 0.03155530) (0.225 sec/batch)\n",
      "2017-04-05 23:47:59.753111: step 51440, loss = (G: 3.96727562, D: 0.09374095) (0.225 sec/batch)\n",
      "2017-04-05 23:48:04.241380: step 51460, loss = (G: 7.60866261, D: 0.00673679) (0.224 sec/batch)\n",
      "2017-04-05 23:48:08.728662: step 51480, loss = (G: 5.66116095, D: 0.08737548) (0.224 sec/batch)\n",
      "2017-04-05 23:48:13.252840: step 51500, loss = (G: 6.17332458, D: 0.01977802) (0.224 sec/batch)\n",
      "2017-04-05 23:48:17.896563: step 51520, loss = (G: 7.38788748, D: 0.00985299) (0.223 sec/batch)\n",
      "2017-04-05 23:48:22.389813: step 51540, loss = (G: 9.73633003, D: 0.01755962) (0.224 sec/batch)\n",
      "2017-04-05 23:48:26.883323: step 51560, loss = (G: 10.24656868, D: 0.00521625) (0.224 sec/batch)\n",
      "2017-04-05 23:48:31.391790: step 51580, loss = (G: 6.13798857, D: 0.03173800) (0.225 sec/batch)\n",
      "2017-04-05 23:48:35.886569: step 51600, loss = (G: 7.00668478, D: 0.01018264) (0.225 sec/batch)\n",
      "2017-04-05 23:48:40.523565: step 51620, loss = (G: 7.92880249, D: 0.00972640) (0.224 sec/batch)\n",
      "2017-04-05 23:48:45.025535: step 51640, loss = (G: 7.50430298, D: 0.00906142) (0.227 sec/batch)\n",
      "2017-04-05 23:48:49.529483: step 51660, loss = (G: 10.84052658, D: 0.00214007) (0.230 sec/batch)\n",
      "2017-04-05 23:48:54.024688: step 51680, loss = (G: 4.88902617, D: 0.03352861) (0.226 sec/batch)\n",
      "2017-04-05 23:48:58.529207: step 51700, loss = (G: 9.84445858, D: 0.00168575) (0.224 sec/batch)\n",
      "2017-04-05 23:49:03.160361: step 51720, loss = (G: 9.00545883, D: 0.00213006) (0.225 sec/batch)\n",
      "2017-04-05 23:49:07.664391: step 51740, loss = (G: 7.35996628, D: 0.02172934) (0.224 sec/batch)\n",
      "2017-04-05 23:49:12.149334: step 51760, loss = (G: 5.67192793, D: 0.04539393) (0.224 sec/batch)\n",
      "2017-04-05 23:49:16.650662: step 51780, loss = (G: 8.50493145, D: 0.00257579) (0.224 sec/batch)\n",
      "2017-04-05 23:49:21.140957: step 51800, loss = (G: 6.84310341, D: 0.01600027) (0.226 sec/batch)\n",
      "2017-04-05 23:49:25.779748: step 51820, loss = (G: 5.18168449, D: 0.21555918) (0.224 sec/batch)\n",
      "2017-04-05 23:49:30.273552: step 51840, loss = (G: 7.45381355, D: 0.01975405) (0.224 sec/batch)\n",
      "2017-04-05 23:49:34.781537: step 51860, loss = (G: 7.32542324, D: 0.00673508) (0.225 sec/batch)\n",
      "2017-04-05 23:49:39.285434: step 51880, loss = (G: 14.04625702, D: 0.00540143) (0.224 sec/batch)\n",
      "2017-04-05 23:49:43.781837: step 51900, loss = (G: 3.55622387, D: 0.17851447) (0.224 sec/batch)\n",
      "2017-04-05 23:49:48.414783: step 51920, loss = (G: 4.58054590, D: 0.06451833) (0.224 sec/batch)\n",
      "2017-04-05 23:49:52.915887: step 51940, loss = (G: 6.01855755, D: 0.01244383) (0.224 sec/batch)\n",
      "2017-04-05 23:49:57.409640: step 51960, loss = (G: 7.59831810, D: 0.00635814) (0.224 sec/batch)\n",
      "2017-04-05 23:50:01.918204: step 51980, loss = (G: 7.71866846, D: 0.00323971) (0.225 sec/batch)\n",
      "2017-04-05 23:50:06.410319: step 52000, loss = (G: 6.36352491, D: 0.03106715) (0.223 sec/batch)\n",
      "2017-04-05 23:50:11.043125: step 52020, loss = (G: 5.99046755, D: 0.02065692) (0.225 sec/batch)\n",
      "2017-04-05 23:50:15.538787: step 52040, loss = (G: 8.40631294, D: 0.00355205) (0.224 sec/batch)\n",
      "2017-04-05 23:50:20.026260: step 52060, loss = (G: 6.02283430, D: 0.04851258) (0.226 sec/batch)\n",
      "2017-04-05 23:50:24.527128: step 52080, loss = (G: 7.12844801, D: 0.01363822) (0.224 sec/batch)\n",
      "2017-04-05 23:50:29.073261: step 52100, loss = (G: 5.74048662, D: 0.02981961) (0.225 sec/batch)\n",
      "2017-04-05 23:50:33.704765: step 52120, loss = (G: 6.45835829, D: 0.01200805) (0.225 sec/batch)\n",
      "2017-04-05 23:50:38.185928: step 52140, loss = (G: 3.47624493, D: 0.52555066) (0.223 sec/batch)\n",
      "2017-04-05 23:50:42.668924: step 52160, loss = (G: 8.80105400, D: 0.02799163) (0.224 sec/batch)\n",
      "2017-04-05 23:50:47.315130: step 52180, loss = (G: 5.71104717, D: 0.04802930) (0.230 sec/batch)\n",
      "2017-04-05 23:50:51.818945: step 52200, loss = (G: 7.47829437, D: 0.02277449) (0.224 sec/batch)\n",
      "2017-04-05 23:50:56.454788: step 52220, loss = (G: 5.56100512, D: 0.03043033) (0.229 sec/batch)\n",
      "2017-04-05 23:51:00.958321: step 52240, loss = (G: 11.17729664, D: 0.05843413) (0.225 sec/batch)\n",
      "2017-04-05 23:51:05.451063: step 52260, loss = (G: 8.00627327, D: 0.00633868) (0.224 sec/batch)\n",
      "2017-04-05 23:51:09.945205: step 52280, loss = (G: 7.88237190, D: 0.00665274) (0.223 sec/batch)\n",
      "2017-04-05 23:51:14.435905: step 52300, loss = (G: 8.85703087, D: 0.00822819) (0.225 sec/batch)\n",
      "2017-04-05 23:51:19.107358: step 52320, loss = (G: 11.66867065, D: 0.02286869) (0.229 sec/batch)\n",
      "2017-04-05 23:51:23.605617: step 52340, loss = (G: 8.88665104, D: 0.00927191) (0.224 sec/batch)\n",
      "2017-04-05 23:51:28.096775: step 52360, loss = (G: 9.55404282, D: 0.02211531) (0.225 sec/batch)\n",
      "2017-04-05 23:51:32.626678: step 52380, loss = (G: 3.47179794, D: 0.69307911) (0.227 sec/batch)\n",
      "2017-04-05 23:51:37.111762: step 52400, loss = (G: 8.29618263, D: 0.01991200) (0.224 sec/batch)\n",
      "2017-04-05 23:51:41.788821: step 52420, loss = (G: 3.70818377, D: 0.12956773) (0.225 sec/batch)\n",
      "2017-04-05 23:51:46.265801: step 52440, loss = (G: 6.03730297, D: 0.02210989) (0.225 sec/batch)\n",
      "2017-04-05 23:51:50.758366: step 52460, loss = (G: 8.03676796, D: 0.00487955) (0.224 sec/batch)\n",
      "2017-04-05 23:51:55.273254: step 52480, loss = (G: 6.93186378, D: 0.12280919) (0.226 sec/batch)\n",
      "2017-04-05 23:51:59.757858: step 52500, loss = (G: 10.42557907, D: 0.00094972) (0.224 sec/batch)\n",
      "2017-04-05 23:52:04.401276: step 52520, loss = (G: 14.79937649, D: 0.02345018) (0.225 sec/batch)\n",
      "2017-04-05 23:52:08.894134: step 52540, loss = (G: 8.06897926, D: 0.00416819) (0.225 sec/batch)\n",
      "2017-04-05 23:52:13.385580: step 52560, loss = (G: 7.21438503, D: 0.01141717) (0.224 sec/batch)\n",
      "2017-04-05 23:52:17.898205: step 52580, loss = (G: 7.41355848, D: 0.00531069) (0.224 sec/batch)\n",
      "2017-04-05 23:52:22.385435: step 52600, loss = (G: 6.02105331, D: 0.04258483) (0.223 sec/batch)\n",
      "2017-04-05 23:52:27.006495: step 52620, loss = (G: 6.04739428, D: 0.31466752) (0.225 sec/batch)\n",
      "2017-04-05 23:52:31.498998: step 52640, loss = (G: 3.76561165, D: 0.14733855) (0.225 sec/batch)\n",
      "2017-04-05 23:52:35.999572: step 52660, loss = (G: 8.45240688, D: 0.02490565) (0.225 sec/batch)\n",
      "2017-04-05 23:52:40.496044: step 52680, loss = (G: 6.63467360, D: 0.00917690) (0.225 sec/batch)\n",
      "2017-04-05 23:52:45.003181: step 52700, loss = (G: 8.11589336, D: 0.00646144) (0.225 sec/batch)\n",
      "2017-04-05 23:52:49.632462: step 52720, loss = (G: 5.57870960, D: 0.03696814) (0.223 sec/batch)\n",
      "2017-04-05 23:52:54.166500: step 52740, loss = (G: 4.71879292, D: 0.04585883) (0.224 sec/batch)\n",
      "2017-04-05 23:52:58.666306: step 52760, loss = (G: 7.22123718, D: 0.04013200) (0.223 sec/batch)\n",
      "2017-04-05 23:53:03.175082: step 52780, loss = (G: 5.75119162, D: 0.26903990) (0.223 sec/batch)\n",
      "2017-04-05 23:53:07.655294: step 52800, loss = (G: 5.29056358, D: 0.06711179) (0.225 sec/batch)\n",
      "2017-04-05 23:53:12.286049: step 52820, loss = (G: 7.41137218, D: 0.01606636) (0.224 sec/batch)\n",
      "2017-04-05 23:53:16.778207: step 52840, loss = (G: 8.62121773, D: 0.00477744) (0.226 sec/batch)\n",
      "2017-04-05 23:53:21.282112: step 52860, loss = (G: 5.90954351, D: 0.03224095) (0.230 sec/batch)\n",
      "2017-04-05 23:53:25.786901: step 52880, loss = (G: 5.46277905, D: 0.04259751) (0.226 sec/batch)\n",
      "2017-04-05 23:53:30.281932: step 52900, loss = (G: 3.94789314, D: 0.27291286) (0.223 sec/batch)\n",
      "2017-04-05 23:53:34.910787: step 52920, loss = (G: 8.80985451, D: 0.09066849) (0.225 sec/batch)\n",
      "2017-04-05 23:53:39.413458: step 52940, loss = (G: 7.51472187, D: 0.00466588) (0.225 sec/batch)\n",
      "2017-04-05 23:53:43.926305: step 52960, loss = (G: 7.95324755, D: 0.00333571) (0.225 sec/batch)\n",
      "2017-04-05 23:53:48.419225: step 52980, loss = (G: 5.98878956, D: 0.02064668) (0.226 sec/batch)\n",
      "2017-04-05 23:53:52.919404: step 53000, loss = (G: 7.70091486, D: 0.05427046) (0.225 sec/batch)\n",
      "2017-04-05 23:53:57.577594: step 53020, loss = (G: 10.99181175, D: 0.01321245) (0.225 sec/batch)\n",
      "2017-04-05 23:54:02.075310: step 53040, loss = (G: 6.30741310, D: 0.01688097) (0.226 sec/batch)\n",
      "2017-04-05 23:54:06.568440: step 53060, loss = (G: 12.44017792, D: 0.00538494) (0.226 sec/batch)\n",
      "2017-04-05 23:54:11.054438: step 53080, loss = (G: 5.62592793, D: 0.02690761) (0.225 sec/batch)\n",
      "2017-04-05 23:54:15.552652: step 53100, loss = (G: 1.82853401, D: 2.99407339) (0.224 sec/batch)\n",
      "2017-04-05 23:54:20.187149: step 53120, loss = (G: 4.25778532, D: 0.10364874) (0.224 sec/batch)\n",
      "2017-04-05 23:54:24.677278: step 53140, loss = (G: 4.31462383, D: 0.23052743) (0.224 sec/batch)\n",
      "2017-04-05 23:54:29.167610: step 53160, loss = (G: 12.41681957, D: 0.03068121) (0.224 sec/batch)\n",
      "2017-04-05 23:54:33.656242: step 53180, loss = (G: 8.17905235, D: 1.91312182) (0.225 sec/batch)\n",
      "2017-04-05 23:54:38.146093: step 53200, loss = (G: 7.83064842, D: 0.01675267) (0.225 sec/batch)\n",
      "2017-04-05 23:54:42.783015: step 53220, loss = (G: 3.55405045, D: 0.17832713) (0.226 sec/batch)\n",
      "2017-04-05 23:54:47.274457: step 53240, loss = (G: 8.74868965, D: 0.00483512) (0.225 sec/batch)\n",
      "2017-04-05 23:54:51.778268: step 53260, loss = (G: 5.63110161, D: 0.05668174) (0.230 sec/batch)\n",
      "2017-04-05 23:54:56.277022: step 53280, loss = (G: 4.98180008, D: 0.08511066) (0.225 sec/batch)\n",
      "2017-04-05 23:55:00.779394: step 53300, loss = (G: 7.80687809, D: 0.01545465) (0.223 sec/batch)\n",
      "2017-04-05 23:55:05.421330: step 53320, loss = (G: 10.12060833, D: 0.01496497) (0.224 sec/batch)\n",
      "2017-04-05 23:55:09.939973: step 53340, loss = (G: 7.68495750, D: 0.34497070) (0.223 sec/batch)\n",
      "2017-04-05 23:55:14.439156: step 53360, loss = (G: 9.28164482, D: 0.10062186) (0.225 sec/batch)\n",
      "2017-04-05 23:55:18.932747: step 53380, loss = (G: 5.09859943, D: 0.03665241) (0.225 sec/batch)\n",
      "2017-04-05 23:55:23.420990: step 53400, loss = (G: 7.42108965, D: 0.00783499) (0.224 sec/batch)\n",
      "2017-04-05 23:55:28.043921: step 53420, loss = (G: 6.36815023, D: 0.03126383) (0.224 sec/batch)\n",
      "2017-04-05 23:55:32.545890: step 53440, loss = (G: 8.35037518, D: 0.03959649) (0.225 sec/batch)\n",
      "2017-04-05 23:55:37.043235: step 53460, loss = (G: 7.89993429, D: 0.02142888) (0.224 sec/batch)\n",
      "2017-04-05 23:55:41.555864: step 53480, loss = (G: 6.87917614, D: 0.12285040) (0.225 sec/batch)\n",
      "2017-04-05 23:55:46.045434: step 53500, loss = (G: 16.89362144, D: 0.06570131) (0.225 sec/batch)\n",
      "2017-04-05 23:55:50.678232: step 53520, loss = (G: 5.26219702, D: 0.09945237) (0.225 sec/batch)\n",
      "2017-04-05 23:55:55.197960: step 53540, loss = (G: 6.76758766, D: 0.01785984) (0.225 sec/batch)\n",
      "2017-04-05 23:55:59.690183: step 53560, loss = (G: 7.79907894, D: 0.01003618) (0.225 sec/batch)\n",
      "2017-04-05 23:56:04.176059: step 53580, loss = (G: 5.04631376, D: 0.05239796) (0.224 sec/batch)\n",
      "2017-04-05 23:56:08.672699: step 53600, loss = (G: 6.15027523, D: 0.06145489) (0.225 sec/batch)\n",
      "2017-04-05 23:56:13.305244: step 53620, loss = (G: 6.90354633, D: 0.04885192) (0.223 sec/batch)\n",
      "2017-04-05 23:56:17.812795: step 53640, loss = (G: 3.56391048, D: 0.18318690) (0.225 sec/batch)\n",
      "2017-04-05 23:56:22.303278: step 53660, loss = (G: 6.75885105, D: 0.03532888) (0.225 sec/batch)\n",
      "2017-04-05 23:56:26.823131: step 53680, loss = (G: 2.14140534, D: 1.13166296) (0.225 sec/batch)\n",
      "2017-04-05 23:56:31.313889: step 53700, loss = (G: 5.15127468, D: 0.03253492) (0.227 sec/batch)\n",
      "2017-04-05 23:56:35.955748: step 53720, loss = (G: 4.17874718, D: 0.08859942) (0.224 sec/batch)\n",
      "2017-04-05 23:56:40.441461: step 53740, loss = (G: 7.12377453, D: 0.00494241) (0.224 sec/batch)\n",
      "2017-04-05 23:56:44.930677: step 53760, loss = (G: 5.55728102, D: 0.03990940) (0.224 sec/batch)\n",
      "2017-04-05 23:56:49.422300: step 53780, loss = (G: 9.16351604, D: 0.01369134) (0.225 sec/batch)\n",
      "2017-04-05 23:56:53.913250: step 53800, loss = (G: 6.07569218, D: 0.06613693) (0.225 sec/batch)\n",
      "2017-04-05 23:56:58.550576: step 53820, loss = (G: 5.79904175, D: 0.02389220) (0.225 sec/batch)\n",
      "2017-04-05 23:57:03.040588: step 53840, loss = (G: 5.94638157, D: 0.03334223) (0.223 sec/batch)\n",
      "2017-04-05 23:57:07.537456: step 53860, loss = (G: 7.96984291, D: 0.02037174) (0.225 sec/batch)\n",
      "2017-04-05 23:57:12.032968: step 53880, loss = (G: 7.92144203, D: 0.01578916) (0.225 sec/batch)\n",
      "2017-04-05 23:57:16.581790: step 53900, loss = (G: 8.88042450, D: 0.10598184) (0.226 sec/batch)\n",
      "2017-04-05 23:57:21.205007: step 53920, loss = (G: 5.93130970, D: 0.02454387) (0.224 sec/batch)\n",
      "2017-04-05 23:57:25.688164: step 53940, loss = (G: 11.91939735, D: 0.00016680) (0.224 sec/batch)\n",
      "2017-04-05 23:57:30.172919: step 53960, loss = (G: 11.87597752, D: 0.19472879) (0.224 sec/batch)\n",
      "2017-04-05 23:57:34.665511: step 53980, loss = (G: 5.32901096, D: 0.08417530) (0.224 sec/batch)\n",
      "2017-04-05 23:57:39.173817: step 54000, loss = (G: 7.01987934, D: 0.00883850) (0.228 sec/batch)\n",
      "2017-04-05 23:57:43.800710: step 54020, loss = (G: 2.90852976, D: 0.24405378) (0.224 sec/batch)\n",
      "2017-04-05 23:57:48.311100: step 54040, loss = (G: 7.24907684, D: 0.01316184) (0.225 sec/batch)\n",
      "2017-04-05 23:57:52.809806: step 54060, loss = (G: 9.11775684, D: 0.00574162) (0.225 sec/batch)\n",
      "2017-04-05 23:57:57.307433: step 54080, loss = (G: 4.99660635, D: 0.04177869) (0.224 sec/batch)\n",
      "2017-04-05 23:58:01.817077: step 54100, loss = (G: 5.27224731, D: 0.05717511) (0.230 sec/batch)\n",
      "2017-04-05 23:58:06.498714: step 54120, loss = (G: 15.22086906, D: 0.42766184) (0.223 sec/batch)\n",
      "2017-04-05 23:58:10.979013: step 54140, loss = (G: 8.04927254, D: 0.01765867) (0.223 sec/batch)\n",
      "2017-04-05 23:58:15.474471: step 54160, loss = (G: 9.69156170, D: 0.01045171) (0.226 sec/batch)\n",
      "2017-04-05 23:58:19.968935: step 54180, loss = (G: 9.69680214, D: 0.01949260) (0.224 sec/batch)\n",
      "2017-04-05 23:58:24.462177: step 54200, loss = (G: 10.89094639, D: 0.20981491) (0.223 sec/batch)\n",
      "2017-04-05 23:58:29.090803: step 54220, loss = (G: 8.68289375, D: 0.02410783) (0.225 sec/batch)\n",
      "2017-04-05 23:58:33.588316: step 54240, loss = (G: 8.28314590, D: 0.00842549) (0.224 sec/batch)\n",
      "2017-04-05 23:58:38.091366: step 54260, loss = (G: 6.35866451, D: 0.02726287) (0.225 sec/batch)\n",
      "2017-04-05 23:58:42.582149: step 54280, loss = (G: 6.62921238, D: 0.04164564) (0.225 sec/batch)\n",
      "2017-04-05 23:58:47.086828: step 54300, loss = (G: 20.88013840, D: 0.02708166) (0.224 sec/batch)\n",
      "2017-04-05 23:58:51.739493: step 54320, loss = (G: 5.87083483, D: 0.03354406) (0.225 sec/batch)\n",
      "2017-04-05 23:58:56.224669: step 54340, loss = (G: 8.83152771, D: 0.00454195) (0.225 sec/batch)\n",
      "2017-04-05 23:59:00.719850: step 54360, loss = (G: 5.99055195, D: 0.12283535) (0.224 sec/batch)\n",
      "2017-04-05 23:59:05.216833: step 54380, loss = (G: 6.80616140, D: 0.01307320) (0.225 sec/batch)\n",
      "2017-04-05 23:59:09.748437: step 54400, loss = (G: 8.50749683, D: 0.01419261) (0.231 sec/batch)\n",
      "2017-04-05 23:59:14.395819: step 54420, loss = (G: 6.51793385, D: 0.02081266) (0.227 sec/batch)\n",
      "2017-04-05 23:59:18.886566: step 54440, loss = (G: 9.62191677, D: 0.03681897) (0.224 sec/batch)\n",
      "2017-04-05 23:59:23.383953: step 54460, loss = (G: 6.86990166, D: 0.01013025) (0.224 sec/batch)\n",
      "2017-04-05 23:59:27.872451: step 54480, loss = (G: 5.62033796, D: 0.02190238) (0.225 sec/batch)\n",
      "2017-04-05 23:59:32.398062: step 54500, loss = (G: 17.36444283, D: 0.54064339) (0.224 sec/batch)\n",
      "2017-04-05 23:59:37.040777: step 54520, loss = (G: 10.34538174, D: 0.00081811) (0.225 sec/batch)\n",
      "2017-04-05 23:59:41.547640: step 54540, loss = (G: 6.46208429, D: 0.05020646) (0.230 sec/batch)\n",
      "2017-04-05 23:59:46.066759: step 54560, loss = (G: 4.76513290, D: 0.06987938) (0.224 sec/batch)\n",
      "2017-04-05 23:59:50.585306: step 54580, loss = (G: 5.16975021, D: 0.04427937) (0.224 sec/batch)\n",
      "2017-04-05 23:59:55.071498: step 54600, loss = (G: 21.01775742, D: 0.48359248) (0.224 sec/batch)\n",
      "2017-04-05 23:59:59.697898: step 54620, loss = (G: 7.89959049, D: 0.01521460) (0.225 sec/batch)\n",
      "2017-04-06 00:00:04.194042: step 54640, loss = (G: 6.56847382, D: 0.21250018) (0.227 sec/batch)\n",
      "2017-04-06 00:00:08.701525: step 54660, loss = (G: 2.31285143, D: 1.53188229) (0.224 sec/batch)\n",
      "2017-04-06 00:00:13.195864: step 54680, loss = (G: 5.90064001, D: 0.03589450) (0.224 sec/batch)\n",
      "2017-04-06 00:00:17.737353: step 54700, loss = (G: 8.59278202, D: 0.01902356) (0.224 sec/batch)\n",
      "2017-04-06 00:00:22.380577: step 54720, loss = (G: 6.19783497, D: 0.02899935) (0.225 sec/batch)\n",
      "2017-04-06 00:00:26.876170: step 54740, loss = (G: 3.97315264, D: 0.11528776) (0.226 sec/batch)\n",
      "2017-04-06 00:00:31.364496: step 54760, loss = (G: 10.91507149, D: 0.32351172) (0.225 sec/batch)\n",
      "2017-04-06 00:00:35.855880: step 54780, loss = (G: 6.84405994, D: 0.03174561) (0.224 sec/batch)\n",
      "2017-04-06 00:00:40.351841: step 54800, loss = (G: 3.48260164, D: 0.17268726) (0.227 sec/batch)\n",
      "2017-04-06 00:00:45.001355: step 54820, loss = (G: 6.12702370, D: 0.02329346) (0.231 sec/batch)\n",
      "2017-04-06 00:00:49.502759: step 54840, loss = (G: 5.67065287, D: 0.02514495) (0.225 sec/batch)\n",
      "2017-04-06 00:00:53.994685: step 54860, loss = (G: 7.72776222, D: 0.00525193) (0.225 sec/batch)\n",
      "2017-04-06 00:00:58.490915: step 54880, loss = (G: 5.21650362, D: 0.03944461) (0.224 sec/batch)\n",
      "2017-04-06 00:01:03.006741: step 54900, loss = (G: 11.30198097, D: 0.03697467) (0.223 sec/batch)\n",
      "2017-04-06 00:01:07.674070: step 54920, loss = (G: 7.91411781, D: 0.01826846) (0.225 sec/batch)\n",
      "2017-04-06 00:01:12.157120: step 54940, loss = (G: 5.08392859, D: 0.08855058) (0.224 sec/batch)\n",
      "2017-04-06 00:01:16.649370: step 54960, loss = (G: 5.52314377, D: 0.10309741) (0.223 sec/batch)\n",
      "2017-04-06 00:01:21.129790: step 54980, loss = (G: 6.30053520, D: 0.02721287) (0.223 sec/batch)\n",
      "2017-04-06 00:01:25.635886: step 55000, loss = (G: 7.40893841, D: 0.01746272) (0.224 sec/batch)\n",
      "2017-04-06 00:01:30.269534: step 55020, loss = (G: 5.00434399, D: 0.03747580) (0.225 sec/batch)\n",
      "2017-04-06 00:01:34.758772: step 55040, loss = (G: 11.73774147, D: 0.03515788) (0.224 sec/batch)\n",
      "2017-04-06 00:01:39.247109: step 55060, loss = (G: 10.03052044, D: 0.03192010) (0.224 sec/batch)\n",
      "2017-04-06 00:01:43.754351: step 55080, loss = (G: 5.16823578, D: 0.03038579) (0.225 sec/batch)\n",
      "2017-04-06 00:01:48.242392: step 55100, loss = (G: 5.71667862, D: 0.11266381) (0.223 sec/batch)\n",
      "2017-04-06 00:01:52.869189: step 55120, loss = (G: 7.88006878, D: 0.01056685) (0.226 sec/batch)\n",
      "2017-04-06 00:01:57.369561: step 55140, loss = (G: 3.41168880, D: 0.34146044) (0.223 sec/batch)\n",
      "2017-04-06 00:02:01.869699: step 55160, loss = (G: 6.20189428, D: 0.03591669) (0.225 sec/batch)\n",
      "2017-04-06 00:02:06.360467: step 55180, loss = (G: 5.38359356, D: 0.03434867) (0.225 sec/batch)\n",
      "2017-04-06 00:02:10.860790: step 55200, loss = (G: 14.10880661, D: 0.65358388) (0.225 sec/batch)\n",
      "2017-04-06 00:02:15.489503: step 55220, loss = (G: 9.12303352, D: 0.56121486) (0.224 sec/batch)\n",
      "2017-04-06 00:02:19.993946: step 55240, loss = (G: 6.70858049, D: 0.16567609) (0.225 sec/batch)\n",
      "2017-04-06 00:02:24.486279: step 55260, loss = (G: 6.02652502, D: 0.02268232) (0.224 sec/batch)\n",
      "2017-04-06 00:02:28.984406: step 55280, loss = (G: 6.05064106, D: 0.03412169) (0.227 sec/batch)\n",
      "2017-04-06 00:02:33.473282: step 55300, loss = (G: 4.89613390, D: 0.23454815) (0.224 sec/batch)\n",
      "2017-04-06 00:02:38.117840: step 55320, loss = (G: 5.39153337, D: 0.02406227) (0.224 sec/batch)\n",
      "2017-04-06 00:02:42.609436: step 55340, loss = (G: 7.18585253, D: 0.00703024) (0.225 sec/batch)\n",
      "2017-04-06 00:02:47.094153: step 55360, loss = (G: 5.90759659, D: 0.07408996) (0.225 sec/batch)\n",
      "2017-04-06 00:02:51.595729: step 55380, loss = (G: 9.02747822, D: 0.03712923) (0.224 sec/batch)\n",
      "2017-04-06 00:02:56.104407: step 55400, loss = (G: 10.04742908, D: 0.02981459) (0.225 sec/batch)\n",
      "2017-04-06 00:03:00.732317: step 55420, loss = (G: 5.59002161, D: 0.08937578) (0.224 sec/batch)\n",
      "2017-04-06 00:03:05.230600: step 55440, loss = (G: 6.80509996, D: 0.01112959) (0.225 sec/batch)\n",
      "2017-04-06 00:03:09.723690: step 55460, loss = (G: 8.80811405, D: 0.08268270) (0.225 sec/batch)\n",
      "2017-04-06 00:03:14.212573: step 55480, loss = (G: 3.54785132, D: 0.18552151) (0.224 sec/batch)\n",
      "2017-04-06 00:03:18.700487: step 55500, loss = (G: 6.17854452, D: 0.04044914) (0.226 sec/batch)\n",
      "2017-04-06 00:03:23.328744: step 55520, loss = (G: 6.20595455, D: 0.01135874) (0.224 sec/batch)\n",
      "2017-04-06 00:03:27.818575: step 55540, loss = (G: 5.01179695, D: 0.05683649) (0.224 sec/batch)\n",
      "2017-04-06 00:03:32.318039: step 55560, loss = (G: 7.24019098, D: 0.01109974) (0.224 sec/batch)\n",
      "2017-04-06 00:03:36.816496: step 55580, loss = (G: 7.92949438, D: 0.01559608) (0.225 sec/batch)\n",
      "2017-04-06 00:03:41.341691: step 55600, loss = (G: 5.74673557, D: 0.01935210) (0.225 sec/batch)\n",
      "2017-04-06 00:03:45.976795: step 55620, loss = (G: 7.78847361, D: 0.00579840) (0.224 sec/batch)\n",
      "2017-04-06 00:03:50.483532: step 55640, loss = (G: 5.66366386, D: 0.03765973) (0.224 sec/batch)\n",
      "2017-04-06 00:03:55.110456: step 55660, loss = (G: 13.16108799, D: 0.00298001) (0.231 sec/batch)\n",
      "2017-04-06 00:03:59.630718: step 55680, loss = (G: 11.03979588, D: 0.02464760) (0.225 sec/batch)\n",
      "2017-04-06 00:04:04.120066: step 55700, loss = (G: 6.23424578, D: 0.03549746) (0.224 sec/batch)\n",
      "2017-04-06 00:04:08.751512: step 55720, loss = (G: 19.65541077, D: 0.03143261) (0.224 sec/batch)\n",
      "2017-04-06 00:04:13.245771: step 55740, loss = (G: 9.32639599, D: 0.00217677) (0.224 sec/batch)\n",
      "2017-04-06 00:04:17.729326: step 55760, loss = (G: 4.90966415, D: 0.09277213) (0.224 sec/batch)\n",
      "2017-04-06 00:04:22.214222: step 55780, loss = (G: 7.21426916, D: 0.00747209) (0.225 sec/batch)\n",
      "2017-04-06 00:04:26.719186: step 55800, loss = (G: 4.54166126, D: 0.08324646) (0.224 sec/batch)\n",
      "2017-04-06 00:04:31.367107: step 55820, loss = (G: 11.48742390, D: 0.00368505) (0.225 sec/batch)\n",
      "2017-04-06 00:04:35.865434: step 55840, loss = (G: 6.72220993, D: 0.01471110) (0.227 sec/batch)\n",
      "2017-04-06 00:04:40.361792: step 55860, loss = (G: 9.53166676, D: 0.00274998) (0.225 sec/batch)\n",
      "2017-04-06 00:04:44.852443: step 55880, loss = (G: 10.93172264, D: 0.00276601) (0.224 sec/batch)\n",
      "2017-04-06 00:04:49.332321: step 55900, loss = (G: 7.87045097, D: 0.01297741) (0.224 sec/batch)\n",
      "2017-04-06 00:04:53.996629: step 55920, loss = (G: 7.44485092, D: 0.07927905) (0.224 sec/batch)\n",
      "2017-04-06 00:04:58.508806: step 55940, loss = (G: 7.18696594, D: 0.01475375) (0.227 sec/batch)\n",
      "2017-04-06 00:05:02.997522: step 55960, loss = (G: 8.73136330, D: 0.00433744) (0.224 sec/batch)\n",
      "2017-04-06 00:05:07.507383: step 55980, loss = (G: 4.97759533, D: 0.25207680) (0.223 sec/batch)\n",
      "2017-04-06 00:05:11.996418: step 56000, loss = (G: 9.18871307, D: 0.04066753) (0.224 sec/batch)\n",
      "2017-04-06 00:05:16.622375: step 56020, loss = (G: 5.52576399, D: 0.02456424) (0.224 sec/batch)\n",
      "2017-04-06 00:05:21.133420: step 56040, loss = (G: 2.33778930, D: 1.64143240) (0.223 sec/batch)\n",
      "2017-04-06 00:05:25.625643: step 56060, loss = (G: 15.03415108, D: 0.01760577) (0.224 sec/batch)\n",
      "2017-04-06 00:05:30.115879: step 56080, loss = (G: 6.78590012, D: 0.01097699) (0.224 sec/batch)\n",
      "2017-04-06 00:05:34.622243: step 56100, loss = (G: 7.73241043, D: 0.06144565) (0.225 sec/batch)\n",
      "2017-04-06 00:05:39.259154: step 56120, loss = (G: 5.66671324, D: 0.05492095) (0.225 sec/batch)\n",
      "2017-04-06 00:05:43.747995: step 56140, loss = (G: 7.10143661, D: 0.00580969) (0.224 sec/batch)\n",
      "2017-04-06 00:05:48.249537: step 56160, loss = (G: 5.92345953, D: 0.08136996) (0.225 sec/batch)\n",
      "2017-04-06 00:05:52.737975: step 56180, loss = (G: 10.67089462, D: 0.04759337) (0.224 sec/batch)\n",
      "2017-04-06 00:05:57.237079: step 56200, loss = (G: 7.89043283, D: 0.00815681) (0.224 sec/batch)\n",
      "2017-04-06 00:06:01.879439: step 56220, loss = (G: 7.28294563, D: 0.02370598) (0.226 sec/batch)\n",
      "2017-04-06 00:06:06.382327: step 56240, loss = (G: 7.25671244, D: 0.09393258) (0.225 sec/batch)\n",
      "2017-04-06 00:06:10.884216: step 56260, loss = (G: 5.95127106, D: 0.04177382) (0.231 sec/batch)\n",
      "2017-04-06 00:06:15.390061: step 56280, loss = (G: 5.96430492, D: 0.02284837) (0.225 sec/batch)\n",
      "2017-04-06 00:06:19.881505: step 56300, loss = (G: 5.69049454, D: 0.03225700) (0.225 sec/batch)\n",
      "2017-04-06 00:06:24.505236: step 56320, loss = (G: 4.75539589, D: 0.06994106) (0.224 sec/batch)\n",
      "2017-04-06 00:06:29.028670: step 56340, loss = (G: 10.22860146, D: 0.02902709) (0.227 sec/batch)\n",
      "2017-04-06 00:06:33.526743: step 56360, loss = (G: 7.00070858, D: 0.04670930) (0.226 sec/batch)\n",
      "2017-04-06 00:06:38.025910: step 56380, loss = (G: 7.62976360, D: 0.00428041) (0.224 sec/batch)\n",
      "2017-04-06 00:06:42.532142: step 56400, loss = (G: 7.28892374, D: 0.04589531) (0.225 sec/batch)\n",
      "2017-04-06 00:06:47.177772: step 56420, loss = (G: 10.15930080, D: 0.52977073) (0.225 sec/batch)\n",
      "2017-04-06 00:06:51.664270: step 56440, loss = (G: 10.85643387, D: 0.12133311) (0.226 sec/batch)\n",
      "2017-04-06 00:06:56.150997: step 56460, loss = (G: 6.26389170, D: 0.03402395) (0.226 sec/batch)\n",
      "2017-04-06 00:07:00.647959: step 56480, loss = (G: 6.65309477, D: 0.01085438) (0.223 sec/batch)\n",
      "2017-04-06 00:07:05.144740: step 56500, loss = (G: 7.12078094, D: 0.03251302) (0.225 sec/batch)\n",
      "2017-04-06 00:07:09.785639: step 56520, loss = (G: 6.99183750, D: 0.03400426) (0.225 sec/batch)\n",
      "2017-04-06 00:07:14.284195: step 56540, loss = (G: 12.30015469, D: 0.00239053) (0.224 sec/batch)\n",
      "2017-04-06 00:07:18.775753: step 56560, loss = (G: 5.17422724, D: 0.10288881) (0.224 sec/batch)\n",
      "2017-04-06 00:07:23.281272: step 56580, loss = (G: 4.98978424, D: 0.06962366) (0.226 sec/batch)\n",
      "2017-04-06 00:07:27.776653: step 56600, loss = (G: 4.04377604, D: 0.09977863) (0.225 sec/batch)\n",
      "2017-04-06 00:07:32.455402: step 56620, loss = (G: 7.15614414, D: 0.00782851) (0.226 sec/batch)\n",
      "2017-04-06 00:07:36.955562: step 56640, loss = (G: 10.17946053, D: 0.00275613) (0.225 sec/batch)\n",
      "2017-04-06 00:07:41.447948: step 56660, loss = (G: 7.47836065, D: 0.04099611) (0.225 sec/batch)\n",
      "2017-04-06 00:07:45.939311: step 56680, loss = (G: 5.96658373, D: 0.10089484) (0.225 sec/batch)\n",
      "2017-04-06 00:07:50.439039: step 56700, loss = (G: 7.71684647, D: 0.00506580) (0.224 sec/batch)\n",
      "2017-04-06 00:07:55.069169: step 56720, loss = (G: 4.56796885, D: 0.22307985) (0.226 sec/batch)\n",
      "2017-04-06 00:07:59.574944: step 56740, loss = (G: 6.80549669, D: 0.06866798) (0.223 sec/batch)\n",
      "2017-04-06 00:08:04.068987: step 56760, loss = (G: 6.89413261, D: 0.02008827) (0.224 sec/batch)\n",
      "2017-04-06 00:08:08.557800: step 56780, loss = (G: 7.76654863, D: 0.00278751) (0.224 sec/batch)\n",
      "2017-04-06 00:08:13.077497: step 56800, loss = (G: 4.72673321, D: 0.05831961) (0.224 sec/batch)\n",
      "2017-04-06 00:08:17.858982: step 56820, loss = (G: 2.09984946, D: 1.05059326) (0.227 sec/batch)\n",
      "2017-04-06 00:08:22.352218: step 56840, loss = (G: 5.59758902, D: 0.07408190) (0.224 sec/batch)\n",
      "2017-04-06 00:08:26.834976: step 56860, loss = (G: 10.75442696, D: 0.00201594) (0.223 sec/batch)\n",
      "2017-04-06 00:08:31.335517: step 56880, loss = (G: 4.61606693, D: 0.35899007) (0.223 sec/batch)\n",
      "2017-04-06 00:08:35.833329: step 56900, loss = (G: 8.68482685, D: 0.00492629) (0.223 sec/batch)\n",
      "2017-04-06 00:08:40.464155: step 56920, loss = (G: 6.70605803, D: 0.05133773) (0.224 sec/batch)\n",
      "2017-04-06 00:08:44.954688: step 56940, loss = (G: 3.20113635, D: 0.66391987) (0.225 sec/batch)\n",
      "2017-04-06 00:08:49.445007: step 56960, loss = (G: 10.25783825, D: 0.02197909) (0.223 sec/batch)\n",
      "2017-04-06 00:08:53.939236: step 56980, loss = (G: 7.36123943, D: 0.00624471) (0.226 sec/batch)\n",
      "2017-04-06 00:08:58.444798: step 57000, loss = (G: 2.23557401, D: 0.80992687) (0.224 sec/batch)\n",
      "2017-04-06 00:09:03.075026: step 57020, loss = (G: 10.81175709, D: 0.08987013) (0.225 sec/batch)\n",
      "2017-04-06 00:09:07.556314: step 57040, loss = (G: 7.72343540, D: 0.35155785) (0.224 sec/batch)\n",
      "2017-04-06 00:09:12.052024: step 57060, loss = (G: 6.96285200, D: 0.21378852) (0.226 sec/batch)\n",
      "2017-04-06 00:09:16.551303: step 57080, loss = (G: 1.92755890, D: 2.46763778) (0.225 sec/batch)\n",
      "2017-04-06 00:09:21.041970: step 57100, loss = (G: 5.30533743, D: 0.06289695) (0.224 sec/batch)\n",
      "2017-04-06 00:09:25.717296: step 57120, loss = (G: 13.33048439, D: 0.00044508) (0.225 sec/batch)\n",
      "2017-04-06 00:09:30.218716: step 57140, loss = (G: 8.43311310, D: 0.03637189) (0.224 sec/batch)\n",
      "2017-04-06 00:09:34.707950: step 57160, loss = (G: 9.68366337, D: 0.01478718) (0.224 sec/batch)\n",
      "2017-04-06 00:09:39.193557: step 57180, loss = (G: 6.62745714, D: 0.01756076) (0.224 sec/batch)\n",
      "2017-04-06 00:09:43.685739: step 57200, loss = (G: 8.73891926, D: 0.06923051) (0.226 sec/batch)\n",
      "2017-04-06 00:09:48.328591: step 57220, loss = (G: 15.56853580, D: 0.24705327) (0.226 sec/batch)\n",
      "2017-04-06 00:09:52.828908: step 57240, loss = (G: 8.56145000, D: 0.00529014) (0.225 sec/batch)\n",
      "2017-04-06 00:09:57.318872: step 57260, loss = (G: 9.10952282, D: 0.00250243) (0.224 sec/batch)\n",
      "2017-04-06 00:10:01.815710: step 57280, loss = (G: 7.70687103, D: 0.23763290) (0.225 sec/batch)\n",
      "2017-04-06 00:10:06.312989: step 57300, loss = (G: 4.14371681, D: 0.08631980) (0.225 sec/batch)\n",
      "2017-04-06 00:10:10.948101: step 57320, loss = (G: 5.87308550, D: 0.02412458) (0.226 sec/batch)\n",
      "2017-04-06 00:10:15.436873: step 57340, loss = (G: 6.45210934, D: 0.01869358) (0.224 sec/batch)\n",
      "2017-04-06 00:10:19.927375: step 57360, loss = (G: 10.20737648, D: 0.00066002) (0.224 sec/batch)\n",
      "2017-04-06 00:10:24.424387: step 57380, loss = (G: 5.05955935, D: 0.08446324) (0.224 sec/batch)\n",
      "2017-04-06 00:10:28.926390: step 57400, loss = (G: 7.48709345, D: 0.01866987) (0.224 sec/batch)\n",
      "2017-04-06 00:10:33.567672: step 57420, loss = (G: 5.04587364, D: 0.03834618) (0.226 sec/batch)\n",
      "2017-04-06 00:10:38.058414: step 57440, loss = (G: 7.63046265, D: 0.01677687) (0.224 sec/batch)\n",
      "2017-04-06 00:10:42.540970: step 57460, loss = (G: 7.36659002, D: 0.07000094) (0.224 sec/batch)\n",
      "2017-04-06 00:10:47.076469: step 57480, loss = (G: 7.47923231, D: 0.32728517) (0.224 sec/batch)\n",
      "2017-04-06 00:10:51.576878: step 57500, loss = (G: 7.89696932, D: 0.00509467) (0.224 sec/batch)\n",
      "2017-04-06 00:10:56.222593: step 57520, loss = (G: 5.83119392, D: 0.04005336) (0.225 sec/batch)\n",
      "2017-04-06 00:11:00.733790: step 57540, loss = (G: 4.90900898, D: 0.07127299) (0.230 sec/batch)\n",
      "2017-04-06 00:11:05.254624: step 57560, loss = (G: 7.49843884, D: 0.13405904) (0.227 sec/batch)\n",
      "2017-04-06 00:11:09.746067: step 57580, loss = (G: 7.88773870, D: 0.00588179) (0.224 sec/batch)\n",
      "2017-04-06 00:11:14.229523: step 57600, loss = (G: 6.12039185, D: 0.01886489) (0.223 sec/batch)\n",
      "2017-04-06 00:11:18.863824: step 57620, loss = (G: 5.64393234, D: 0.03637902) (0.224 sec/batch)\n",
      "2017-04-06 00:11:23.360982: step 57640, loss = (G: 18.91270638, D: 0.42183721) (0.224 sec/batch)\n",
      "2017-04-06 00:11:27.893943: step 57660, loss = (G: 8.28409386, D: 0.00432192) (0.224 sec/batch)\n",
      "2017-04-06 00:11:32.371490: step 57680, loss = (G: 7.44066381, D: 0.02087559) (0.224 sec/batch)\n",
      "2017-04-06 00:11:36.880232: step 57700, loss = (G: 5.47315884, D: 0.02386906) (0.224 sec/batch)\n",
      "2017-04-06 00:11:41.516021: step 57720, loss = (G: 6.41181326, D: 0.32138464) (0.228 sec/batch)\n",
      "2017-04-06 00:11:46.013911: step 57740, loss = (G: 9.91720676, D: 0.01827659) (0.225 sec/batch)\n",
      "2017-04-06 00:11:50.500569: step 57760, loss = (G: 6.28693151, D: 0.24578592) (0.224 sec/batch)\n",
      "2017-04-06 00:11:54.999796: step 57780, loss = (G: 5.66094494, D: 0.06477115) (0.225 sec/batch)\n",
      "2017-04-06 00:11:59.494777: step 57800, loss = (G: 10.45188141, D: 0.00732595) (0.226 sec/batch)\n",
      "2017-04-06 00:12:04.129207: step 57820, loss = (G: 9.52786732, D: 0.00183419) (0.224 sec/batch)\n",
      "2017-04-06 00:12:08.628073: step 57840, loss = (G: 4.50084591, D: 0.11027638) (0.226 sec/batch)\n",
      "2017-04-06 00:12:13.117862: step 57860, loss = (G: 7.24331808, D: 0.00939508) (0.223 sec/batch)\n",
      "2017-04-06 00:12:17.612478: step 57880, loss = (G: 6.13053560, D: 0.01402475) (0.225 sec/batch)\n",
      "2017-04-06 00:12:22.107966: step 57900, loss = (G: 5.92749596, D: 0.04733277) (0.225 sec/batch)\n",
      "2017-04-06 00:12:26.759784: step 57920, loss = (G: 5.41867018, D: 0.03703309) (0.225 sec/batch)\n",
      "2017-04-06 00:12:31.306870: step 57940, loss = (G: 4.69213629, D: 0.08187073) (0.224 sec/batch)\n",
      "2017-04-06 00:12:35.797119: step 57960, loss = (G: 8.56217766, D: 0.00220445) (0.225 sec/batch)\n",
      "2017-04-06 00:12:40.459539: step 57980, loss = (G: 8.31120300, D: 0.00862773) (0.225 sec/batch)\n",
      "2017-04-06 00:12:44.955887: step 58000, loss = (G: 6.89348984, D: 0.02320148) (0.224 sec/batch)\n",
      "2017-04-06 00:12:49.601769: step 58020, loss = (G: 6.14763689, D: 0.01993215) (0.224 sec/batch)\n",
      "2017-04-06 00:12:54.138865: step 58040, loss = (G: 19.81682014, D: 0.00829866) (0.228 sec/batch)\n",
      "2017-04-06 00:12:58.625616: step 58060, loss = (G: 7.11995459, D: 0.05751034) (0.224 sec/batch)\n",
      "2017-04-06 00:13:03.174626: step 58080, loss = (G: 6.73187447, D: 0.05640787) (0.225 sec/batch)\n",
      "2017-04-06 00:13:07.658739: step 58100, loss = (G: 6.09973860, D: 0.02430050) (0.224 sec/batch)\n",
      "2017-04-06 00:13:12.298645: step 58120, loss = (G: 7.84245491, D: 0.05749661) (0.226 sec/batch)\n",
      "2017-04-06 00:13:16.794449: step 58140, loss = (G: 11.62878132, D: 0.01853843) (0.224 sec/batch)\n",
      "2017-04-06 00:13:21.277734: step 58160, loss = (G: 6.08742952, D: 0.02358385) (0.224 sec/batch)\n",
      "2017-04-06 00:13:25.774819: step 58180, loss = (G: 5.85826778, D: 0.01747291) (0.226 sec/batch)\n",
      "2017-04-06 00:13:30.277215: step 58200, loss = (G: 10.02684784, D: 0.00195778) (0.227 sec/batch)\n",
      "2017-04-06 00:13:34.906121: step 58220, loss = (G: 11.75112724, D: 0.02171450) (0.227 sec/batch)\n",
      "2017-04-06 00:13:39.443820: step 58240, loss = (G: 11.70057297, D: 0.04626672) (0.231 sec/batch)\n",
      "2017-04-06 00:13:43.955788: step 58260, loss = (G: 5.60145473, D: 0.02356190) (0.229 sec/batch)\n",
      "2017-04-06 00:13:48.465664: step 58280, loss = (G: 7.24611378, D: 0.01170763) (0.224 sec/batch)\n",
      "2017-04-06 00:13:52.958859: step 58300, loss = (G: 6.67610598, D: 0.02839662) (0.225 sec/batch)\n",
      "2017-04-06 00:13:57.592817: step 58320, loss = (G: 6.25659275, D: 0.02877748) (0.225 sec/batch)\n",
      "2017-04-06 00:14:02.080066: step 58340, loss = (G: 6.46485901, D: 0.07194996) (0.224 sec/batch)\n",
      "2017-04-06 00:14:06.574835: step 58360, loss = (G: 6.60156965, D: 0.31213933) (0.226 sec/batch)\n",
      "2017-04-06 00:14:11.068579: step 58380, loss = (G: 8.65671444, D: 0.00425350) (0.226 sec/batch)\n",
      "2017-04-06 00:14:15.570654: step 58400, loss = (G: 9.39131165, D: 0.00300942) (0.228 sec/batch)\n",
      "2017-04-06 00:14:20.230975: step 58420, loss = (G: 9.55331612, D: 0.04210004) (0.226 sec/batch)\n",
      "2017-04-06 00:14:24.744653: step 58440, loss = (G: 6.51810551, D: 0.01303771) (0.226 sec/batch)\n",
      "2017-04-06 00:14:29.255730: step 58460, loss = (G: 8.41381168, D: 0.00846688) (0.227 sec/batch)\n",
      "2017-04-06 00:14:33.774429: step 58480, loss = (G: 5.59160233, D: 0.02342878) (0.225 sec/batch)\n",
      "2017-04-06 00:14:38.270397: step 58500, loss = (G: 5.46562815, D: 0.05119950) (0.225 sec/batch)\n",
      "2017-04-06 00:14:42.917233: step 58520, loss = (G: 8.74615288, D: 0.04870532) (0.224 sec/batch)\n",
      "2017-04-06 00:14:47.403599: step 58540, loss = (G: 7.27474499, D: 0.00865875) (0.223 sec/batch)\n",
      "2017-04-06 00:14:51.900415: step 58560, loss = (G: 9.38222122, D: 0.02053645) (0.224 sec/batch)\n",
      "2017-04-06 00:14:56.391147: step 58580, loss = (G: 6.12173557, D: 0.09644260) (0.224 sec/batch)\n",
      "2017-04-06 00:15:00.930936: step 58600, loss = (G: 8.00294304, D: 0.00350991) (0.224 sec/batch)\n",
      "2017-04-06 00:15:05.548620: step 58620, loss = (G: 9.51283169, D: 0.02860760) (0.224 sec/batch)\n",
      "2017-04-06 00:15:10.037532: step 58640, loss = (G: 6.32941437, D: 0.01340859) (0.225 sec/batch)\n",
      "2017-04-06 00:15:14.533531: step 58660, loss = (G: 9.05323982, D: 0.00817238) (0.226 sec/batch)\n",
      "2017-04-06 00:15:19.030070: step 58680, loss = (G: 7.13999939, D: 0.01343045) (0.225 sec/batch)\n",
      "2017-04-06 00:15:23.519793: step 58700, loss = (G: 6.50747299, D: 0.01240326) (0.224 sec/batch)\n",
      "2017-04-06 00:15:28.155415: step 58720, loss = (G: 9.59788418, D: 0.00266375) (0.224 sec/batch)\n",
      "2017-04-06 00:15:32.636777: step 58740, loss = (G: 6.59167290, D: 0.02848542) (0.224 sec/batch)\n",
      "2017-04-06 00:15:37.174889: step 58760, loss = (G: 7.91483212, D: 0.00321459) (0.226 sec/batch)\n",
      "2017-04-06 00:15:41.682297: step 58780, loss = (G: 12.37255669, D: 0.00077693) (0.227 sec/batch)\n",
      "2017-04-06 00:15:46.177251: step 58800, loss = (G: 7.30973244, D: 0.00985991) (0.225 sec/batch)\n",
      "2017-04-06 00:15:50.811654: step 58820, loss = (G: 12.97385311, D: 0.00255253) (0.225 sec/batch)\n",
      "2017-04-06 00:15:55.344027: step 58840, loss = (G: 6.60944891, D: 0.01328371) (0.226 sec/batch)\n",
      "2017-04-06 00:15:59.832926: step 58860, loss = (G: 8.67057991, D: 0.00362250) (0.225 sec/batch)\n",
      "2017-04-06 00:16:04.327995: step 58880, loss = (G: 8.51778316, D: 0.00220540) (0.224 sec/batch)\n",
      "2017-04-06 00:16:08.825865: step 58900, loss = (G: 6.81363487, D: 0.02216979) (0.228 sec/batch)\n",
      "2017-04-06 00:16:13.452764: step 58920, loss = (G: 22.21918106, D: 0.35706109) (0.225 sec/batch)\n",
      "2017-04-06 00:16:17.938811: step 58940, loss = (G: 6.06991148, D: 0.03080093) (0.224 sec/batch)\n",
      "2017-04-06 00:16:22.436602: step 58960, loss = (G: 5.48749399, D: 0.09262071) (0.224 sec/batch)\n",
      "2017-04-06 00:16:26.931046: step 58980, loss = (G: 5.86838341, D: 0.04556770) (0.224 sec/batch)\n",
      "2017-04-06 00:16:31.435073: step 59000, loss = (G: 5.79711533, D: 0.02784553) (0.224 sec/batch)\n",
      "2017-04-06 00:16:36.067258: step 59020, loss = (G: 6.19616079, D: 0.01548255) (0.224 sec/batch)\n",
      "2017-04-06 00:16:40.569604: step 59040, loss = (G: 3.05971813, D: 0.47855324) (0.223 sec/batch)\n",
      "2017-04-06 00:16:45.064756: step 59060, loss = (G: 5.35598421, D: 0.03907027) (0.224 sec/batch)\n",
      "2017-04-06 00:16:49.556607: step 59080, loss = (G: 4.83671856, D: 0.10823919) (0.224 sec/batch)\n",
      "2017-04-06 00:16:54.043098: step 59100, loss = (G: 1.31528604, D: 2.42469358) (0.225 sec/batch)\n",
      "2017-04-06 00:16:58.682119: step 59120, loss = (G: 9.78773975, D: 0.00791315) (0.225 sec/batch)\n",
      "2017-04-06 00:17:03.306696: step 59140, loss = (G: 6.10613060, D: 0.06925154) (0.226 sec/batch)\n",
      "2017-04-06 00:17:07.810661: step 59160, loss = (G: 6.82500982, D: 0.00839765) (0.224 sec/batch)\n",
      "2017-04-06 00:17:12.307477: step 59180, loss = (G: 7.16317940, D: 0.03141941) (0.224 sec/batch)\n",
      "2017-04-06 00:17:16.818224: step 59200, loss = (G: 17.51839066, D: 0.12499917) (0.226 sec/batch)\n",
      "2017-04-06 00:17:21.449739: step 59220, loss = (G: 7.09651375, D: 0.00666402) (0.224 sec/batch)\n",
      "2017-04-06 00:17:25.930331: step 59240, loss = (G: 6.11763287, D: 0.03124806) (0.225 sec/batch)\n",
      "2017-04-06 00:17:30.426856: step 59260, loss = (G: 12.37597466, D: 0.06610339) (0.224 sec/batch)\n",
      "2017-04-06 00:17:34.927692: step 59280, loss = (G: 6.44708633, D: 0.02328487) (0.225 sec/batch)\n",
      "2017-04-06 00:17:39.454750: step 59300, loss = (G: 4.83955145, D: 0.04618863) (0.225 sec/batch)\n",
      "2017-04-06 00:17:44.071674: step 59320, loss = (G: 5.24442387, D: 0.03949767) (0.223 sec/batch)\n",
      "2017-04-06 00:17:48.598852: step 59340, loss = (G: 8.92520046, D: 0.03259106) (0.224 sec/batch)\n",
      "2017-04-06 00:17:53.080708: step 59360, loss = (G: 6.56621933, D: 0.02395079) (0.224 sec/batch)\n",
      "2017-04-06 00:17:57.568480: step 59380, loss = (G: 19.29404449, D: 0.16097426) (0.224 sec/batch)\n",
      "2017-04-06 00:18:02.049366: step 59400, loss = (G: 4.75185394, D: 0.05498336) (0.225 sec/batch)\n",
      "2017-04-06 00:18:06.676104: step 59420, loss = (G: 7.22123241, D: 0.00761978) (0.224 sec/batch)\n",
      "2017-04-06 00:18:11.167326: step 59440, loss = (G: 3.27975869, D: 0.19067919) (0.226 sec/batch)\n",
      "2017-04-06 00:18:15.662568: step 59460, loss = (G: 9.11571407, D: 0.10024276) (0.223 sec/batch)\n",
      "2017-04-06 00:18:20.162116: step 59480, loss = (G: 3.99755740, D: 0.09214251) (0.224 sec/batch)\n",
      "2017-04-06 00:18:24.660062: step 59500, loss = (G: 6.75279903, D: 0.01886269) (0.225 sec/batch)\n",
      "2017-04-06 00:18:29.300329: step 59520, loss = (G: 15.92156887, D: 0.14379695) (0.224 sec/batch)\n",
      "2017-04-06 00:18:33.801426: step 59540, loss = (G: 4.98510551, D: 0.06237653) (0.223 sec/batch)\n",
      "2017-04-06 00:18:38.280459: step 59560, loss = (G: 6.22797441, D: 0.06266083) (0.224 sec/batch)\n",
      "2017-04-06 00:18:42.788795: step 59580, loss = (G: 11.76179886, D: 0.01939966) (0.226 sec/batch)\n",
      "2017-04-06 00:18:47.277016: step 59600, loss = (G: 7.13583374, D: 0.02283678) (0.225 sec/batch)\n",
      "2017-04-06 00:18:51.916659: step 59620, loss = (G: 8.71218014, D: 0.02205963) (0.223 sec/batch)\n",
      "2017-04-06 00:18:56.419256: step 59640, loss = (G: 6.44140148, D: 0.01474751) (0.229 sec/batch)\n",
      "2017-04-06 00:19:00.910903: step 59660, loss = (G: 8.30849552, D: 0.00412198) (0.224 sec/batch)\n",
      "2017-04-06 00:19:05.396596: step 59680, loss = (G: 4.08223534, D: 0.09229092) (0.226 sec/batch)\n",
      "2017-04-06 00:19:09.896398: step 59700, loss = (G: 5.54991102, D: 0.03435935) (0.225 sec/batch)\n",
      "2017-04-06 00:19:14.530715: step 59720, loss = (G: 6.61576462, D: 0.02192120) (0.226 sec/batch)\n",
      "2017-04-06 00:19:19.031070: step 59740, loss = (G: 7.01227903, D: 0.00969180) (0.227 sec/batch)\n",
      "2017-04-06 00:19:23.544006: step 59760, loss = (G: 5.18086195, D: 0.02595448) (0.231 sec/batch)\n",
      "2017-04-06 00:19:28.042656: step 59780, loss = (G: 5.45817089, D: 0.02399803) (0.225 sec/batch)\n",
      "2017-04-06 00:19:32.541512: step 59800, loss = (G: 7.87852287, D: 0.00717745) (0.225 sec/batch)\n",
      "2017-04-06 00:19:37.179296: step 59820, loss = (G: 6.70007277, D: 0.01853169) (0.224 sec/batch)\n",
      "2017-04-06 00:19:41.686358: step 59840, loss = (G: 7.00308084, D: 0.03154309) (0.224 sec/batch)\n",
      "2017-04-06 00:19:46.175416: step 59860, loss = (G: 5.83549929, D: 0.03319148) (0.224 sec/batch)\n",
      "2017-04-06 00:19:50.660187: step 59880, loss = (G: 5.07689190, D: 0.03632002) (0.224 sec/batch)\n",
      "2017-04-06 00:19:55.150655: step 59900, loss = (G: 11.48702812, D: 0.00064852) (0.225 sec/batch)\n",
      "2017-04-06 00:19:59.781463: step 59920, loss = (G: 8.98738766, D: 0.00200464) (0.225 sec/batch)\n",
      "2017-04-06 00:20:04.263022: step 59940, loss = (G: 4.33859253, D: 0.11429740) (0.224 sec/batch)\n",
      "2017-04-06 00:20:08.752249: step 59960, loss = (G: 1.23125720, D: 2.90348101) (0.225 sec/batch)\n",
      "2017-04-06 00:20:13.242838: step 59980, loss = (G: 7.26680183, D: 0.02842627) (0.224 sec/batch)\n",
      "2017-04-06 00:20:17.727306: step 60000, loss = (G: 6.78045464, D: 0.43185717) (0.225 sec/batch)\n",
      "2017-04-06 00:20:22.361168: step 60020, loss = (G: 6.54986382, D: 0.01621301) (0.227 sec/batch)\n",
      "2017-04-06 00:20:26.855075: step 60040, loss = (G: 9.57163048, D: 0.09140628) (0.224 sec/batch)\n",
      "2017-04-06 00:20:31.345739: step 60060, loss = (G: 10.39402199, D: 0.10799593) (0.224 sec/batch)\n",
      "2017-04-06 00:20:35.839134: step 60080, loss = (G: 6.30003023, D: 0.19634856) (0.224 sec/batch)\n",
      "2017-04-06 00:20:40.338277: step 60100, loss = (G: 9.01199913, D: 0.00480877) (0.225 sec/batch)\n",
      "2017-04-06 00:20:44.971362: step 60120, loss = (G: 5.88519001, D: 0.11130641) (0.225 sec/batch)\n",
      "2017-04-06 00:20:49.467455: step 60140, loss = (G: 6.02979565, D: 0.02543066) (0.224 sec/batch)\n",
      "2017-04-06 00:20:53.953664: step 60160, loss = (G: 6.29520130, D: 0.01720622) (0.224 sec/batch)\n",
      "2017-04-06 00:20:58.449617: step 60180, loss = (G: 8.13174915, D: 0.00679177) (0.224 sec/batch)\n",
      "2017-04-06 00:21:02.953529: step 60200, loss = (G: 4.43381214, D: 0.10602292) (0.224 sec/batch)\n",
      "2017-04-06 00:21:07.595727: step 60220, loss = (G: 6.48481607, D: 0.01469941) (0.224 sec/batch)\n",
      "2017-04-06 00:21:12.084227: step 60240, loss = (G: 6.44461489, D: 0.01139881) (0.224 sec/batch)\n",
      "2017-04-06 00:21:16.574370: step 60260, loss = (G: 7.79910946, D: 0.00626779) (0.226 sec/batch)\n",
      "2017-04-06 00:21:21.079657: step 60280, loss = (G: 4.71827030, D: 0.05766696) (0.224 sec/batch)\n",
      "2017-04-06 00:21:25.719335: step 60300, loss = (G: 7.28847075, D: 0.01399811) (0.230 sec/batch)\n",
      "2017-04-06 00:21:30.376494: step 60320, loss = (G: 7.53913164, D: 0.01427035) (0.225 sec/batch)\n",
      "2017-04-06 00:21:34.858544: step 60340, loss = (G: 9.88315678, D: 0.01253632) (0.223 sec/batch)\n",
      "2017-04-06 00:21:39.348559: step 60360, loss = (G: 10.22006702, D: 0.00285190) (0.225 sec/batch)\n",
      "2017-04-06 00:21:43.836945: step 60380, loss = (G: 5.84613419, D: 0.02154988) (0.225 sec/batch)\n",
      "2017-04-06 00:21:48.338389: step 60400, loss = (G: 11.68576145, D: 0.00209673) (0.225 sec/batch)\n",
      "2017-04-06 00:21:52.969371: step 60420, loss = (G: 3.33974504, D: 0.40518683) (0.225 sec/batch)\n",
      "2017-04-06 00:21:57.469137: step 60440, loss = (G: 8.66193199, D: 0.06275504) (0.225 sec/batch)\n",
      "2017-04-06 00:22:01.968322: step 60460, loss = (G: 5.73260212, D: 0.03420576) (0.225 sec/batch)\n",
      "2017-04-06 00:22:06.453744: step 60480, loss = (G: 5.68345976, D: 0.02834625) (0.225 sec/batch)\n",
      "2017-04-06 00:22:10.954478: step 60500, loss = (G: 6.41475821, D: 0.02981400) (0.230 sec/batch)\n",
      "2017-04-06 00:22:15.601536: step 60520, loss = (G: 14.08139706, D: 0.65373480) (0.225 sec/batch)\n",
      "2017-04-06 00:22:20.091358: step 60540, loss = (G: 6.05389786, D: 0.04073225) (0.223 sec/batch)\n",
      "2017-04-06 00:22:24.596238: step 60560, loss = (G: 6.01462507, D: 0.06117228) (0.225 sec/batch)\n",
      "2017-04-06 00:22:29.083826: step 60580, loss = (G: 8.61909485, D: 0.01970275) (0.223 sec/batch)\n",
      "2017-04-06 00:22:33.571298: step 60600, loss = (G: 8.99732304, D: 0.00597747) (0.224 sec/batch)\n",
      "2017-04-06 00:22:38.198030: step 60620, loss = (G: 2.19448066, D: 1.85592973) (0.224 sec/batch)\n",
      "2017-04-06 00:22:42.689492: step 60640, loss = (G: 7.61015749, D: 0.00903411) (0.224 sec/batch)\n",
      "2017-04-06 00:22:47.189140: step 60660, loss = (G: 11.32800007, D: 0.01607010) (0.223 sec/batch)\n",
      "2017-04-06 00:22:51.696428: step 60680, loss = (G: 7.76353502, D: 0.00437420) (0.224 sec/batch)\n",
      "2017-04-06 00:22:56.186262: step 60700, loss = (G: 5.11053324, D: 0.05568113) (0.224 sec/batch)\n",
      "2017-04-06 00:23:00.827719: step 60720, loss = (G: 7.43489456, D: 0.00492962) (0.227 sec/batch)\n",
      "2017-04-06 00:23:05.319887: step 60740, loss = (G: 9.80556679, D: 0.00245929) (0.226 sec/batch)\n",
      "2017-04-06 00:23:09.801304: step 60760, loss = (G: 4.94133425, D: 0.04249547) (0.224 sec/batch)\n",
      "2017-04-06 00:23:14.312459: step 60780, loss = (G: 6.56395864, D: 0.04327210) (0.224 sec/batch)\n",
      "2017-04-06 00:23:18.801032: step 60800, loss = (G: 5.93420792, D: 0.03741475) (0.223 sec/batch)\n",
      "2017-04-06 00:23:23.427420: step 60820, loss = (G: 4.39102650, D: 0.10042698) (0.224 sec/batch)\n",
      "2017-04-06 00:23:27.961657: step 60840, loss = (G: 6.46774006, D: 0.27634028) (0.225 sec/batch)\n",
      "2017-04-06 00:23:32.480908: step 60860, loss = (G: 8.59755802, D: 0.03559257) (0.224 sec/batch)\n",
      "2017-04-06 00:23:36.969861: step 60880, loss = (G: 6.12380123, D: 0.04208332) (0.224 sec/batch)\n",
      "2017-04-06 00:23:41.477922: step 60900, loss = (G: 9.90481377, D: 0.03693021) (0.224 sec/batch)\n",
      "2017-04-06 00:23:46.112064: step 60920, loss = (G: 6.59278870, D: 0.01876182) (0.224 sec/batch)\n",
      "2017-04-06 00:23:50.596925: step 60940, loss = (G: 6.98752022, D: 0.24763475) (0.224 sec/batch)\n",
      "2017-04-06 00:23:55.088347: step 60960, loss = (G: 7.43056679, D: 0.00555701) (0.224 sec/batch)\n",
      "2017-04-06 00:23:59.583344: step 60980, loss = (G: 7.26690435, D: 0.01383495) (0.224 sec/batch)\n",
      "2017-04-06 00:24:04.078245: step 61000, loss = (G: 6.83577204, D: 0.01833730) (0.224 sec/batch)\n",
      "2017-04-06 00:24:08.709001: step 61020, loss = (G: 6.54780865, D: 0.01279200) (0.225 sec/batch)\n",
      "2017-04-06 00:24:13.203660: step 61040, loss = (G: 5.10217762, D: 0.04898309) (0.224 sec/batch)\n",
      "2017-04-06 00:24:17.693849: step 61060, loss = (G: 18.38283539, D: 0.02110595) (0.225 sec/batch)\n",
      "2017-04-06 00:24:22.175844: step 61080, loss = (G: 10.03548813, D: 0.01919626) (0.224 sec/batch)\n",
      "2017-04-06 00:24:26.715544: step 61100, loss = (G: 9.85342789, D: 0.00590425) (0.224 sec/batch)\n",
      "2017-04-06 00:24:31.350530: step 61120, loss = (G: 3.80879068, D: 0.28953576) (0.225 sec/batch)\n",
      "2017-04-06 00:24:35.840082: step 61140, loss = (G: 7.34754944, D: 0.05160297) (0.224 sec/batch)\n",
      "2017-04-06 00:24:40.330073: step 61160, loss = (G: 6.23039818, D: 0.02456553) (0.224 sec/batch)\n",
      "2017-04-06 00:24:44.822161: step 61180, loss = (G: 6.22053719, D: 0.02142084) (0.224 sec/batch)\n",
      "2017-04-06 00:24:49.325927: step 61200, loss = (G: 6.52857018, D: 0.01411254) (0.225 sec/batch)\n",
      "2017-04-06 00:24:53.957460: step 61220, loss = (G: 6.20460415, D: 0.02401310) (0.224 sec/batch)\n",
      "2017-04-06 00:24:58.437531: step 61240, loss = (G: 6.45515919, D: 0.01609673) (0.224 sec/batch)\n",
      "2017-04-06 00:25:02.942824: step 61260, loss = (G: 7.67370987, D: 0.00421542) (0.224 sec/batch)\n",
      "2017-04-06 00:25:07.434700: step 61280, loss = (G: 3.18914032, D: 0.51462394) (0.224 sec/batch)\n",
      "2017-04-06 00:25:11.939813: step 61300, loss = (G: 8.66227055, D: 0.00239189) (0.227 sec/batch)\n",
      "2017-04-06 00:25:16.583078: step 61320, loss = (G: 7.84242296, D: 0.00496560) (0.225 sec/batch)\n",
      "2017-04-06 00:25:21.089151: step 61340, loss = (G: 7.01143122, D: 0.06577572) (0.225 sec/batch)\n",
      "2017-04-06 00:25:25.602435: step 61360, loss = (G: 4.89300966, D: 0.09698021) (0.228 sec/batch)\n",
      "2017-04-06 00:25:30.083909: step 61380, loss = (G: 7.27529287, D: 0.03871448) (0.224 sec/batch)\n",
      "2017-04-06 00:25:34.580019: step 61400, loss = (G: 5.09661055, D: 0.03522286) (0.225 sec/batch)\n",
      "2017-04-06 00:25:39.214164: step 61420, loss = (G: 9.44765949, D: 0.00461697) (0.225 sec/batch)\n",
      "2017-04-06 00:25:43.711959: step 61440, loss = (G: 8.07356262, D: 0.27644694) (0.227 sec/batch)\n",
      "2017-04-06 00:25:48.383617: step 61460, loss = (G: 8.78749752, D: 0.07262789) (0.223 sec/batch)\n",
      "2017-04-06 00:25:52.899052: step 61480, loss = (G: 5.79984951, D: 0.02466699) (0.224 sec/batch)\n",
      "2017-04-06 00:25:57.393054: step 61500, loss = (G: 7.38132334, D: 0.06148872) (0.224 sec/batch)\n",
      "2017-04-06 00:26:02.028794: step 61520, loss = (G: 8.55512619, D: 0.00937199) (0.224 sec/batch)\n",
      "2017-04-06 00:26:06.522530: step 61540, loss = (G: 3.70966125, D: 0.23822352) (0.224 sec/batch)\n",
      "2017-04-06 00:26:11.015861: step 61560, loss = (G: 8.30450630, D: 0.12993857) (0.226 sec/batch)\n",
      "2017-04-06 00:26:15.509833: step 61580, loss = (G: 6.96717262, D: 0.01025716) (0.225 sec/batch)\n",
      "2017-04-06 00:26:20.008752: step 61600, loss = (G: 6.20184898, D: 0.03743540) (0.224 sec/batch)\n",
      "2017-04-06 00:26:24.645506: step 61620, loss = (G: 6.32548428, D: 0.01146497) (0.224 sec/batch)\n",
      "2017-04-06 00:26:29.131011: step 61640, loss = (G: 9.01235771, D: 0.02984003) (0.224 sec/batch)\n",
      "2017-04-06 00:26:33.619132: step 61660, loss = (G: 7.06560564, D: 0.00883134) (0.225 sec/batch)\n",
      "2017-04-06 00:26:38.114815: step 61680, loss = (G: 9.61361599, D: 0.08400042) (0.224 sec/batch)\n",
      "2017-04-06 00:26:42.616266: step 61700, loss = (G: 8.39654636, D: 0.13625503) (0.226 sec/batch)\n",
      "2017-04-06 00:26:47.248454: step 61720, loss = (G: 7.80873871, D: 0.00806781) (0.224 sec/batch)\n",
      "2017-04-06 00:26:51.737190: step 61740, loss = (G: 7.30823326, D: 0.01599566) (0.224 sec/batch)\n",
      "2017-04-06 00:26:56.221413: step 61760, loss = (G: 5.23855209, D: 0.08009488) (0.224 sec/batch)\n",
      "2017-04-06 00:27:00.727861: step 61780, loss = (G: 7.12259769, D: 0.05652948) (0.226 sec/batch)\n",
      "2017-04-06 00:27:05.227664: step 61800, loss = (G: 11.61958122, D: 0.01226694) (0.224 sec/batch)\n",
      "2017-04-06 00:27:09.858450: step 61820, loss = (G: 9.70043182, D: 0.00119556) (0.224 sec/batch)\n",
      "2017-04-06 00:27:14.347255: step 61840, loss = (G: 6.26686859, D: 0.02085526) (0.224 sec/batch)\n",
      "2017-04-06 00:27:18.849124: step 61860, loss = (G: 5.99936485, D: 0.01807879) (0.225 sec/batch)\n",
      "2017-04-06 00:27:23.350481: step 61880, loss = (G: 5.83405685, D: 0.02557674) (0.225 sec/batch)\n",
      "2017-04-06 00:27:27.844586: step 61900, loss = (G: 5.61828709, D: 0.03871689) (0.224 sec/batch)\n",
      "2017-04-06 00:27:32.482145: step 61920, loss = (G: 6.03624773, D: 0.02548771) (0.224 sec/batch)\n",
      "2017-04-06 00:27:36.983338: step 61940, loss = (G: 5.20992231, D: 0.06293406) (0.224 sec/batch)\n",
      "2017-04-06 00:27:41.468309: step 61960, loss = (G: 12.80014038, D: 0.02038407) (0.224 sec/batch)\n",
      "2017-04-06 00:27:45.962437: step 61980, loss = (G: 7.23690605, D: 0.00769201) (0.224 sec/batch)\n",
      "2017-04-06 00:27:50.466716: step 62000, loss = (G: 10.83083820, D: 0.00725828) (0.224 sec/batch)\n",
      "2017-04-06 00:27:55.099926: step 62020, loss = (G: 5.19389868, D: 0.07860675) (0.226 sec/batch)\n",
      "2017-04-06 00:27:59.593551: step 62040, loss = (G: 6.14884329, D: 0.06005038) (0.224 sec/batch)\n",
      "2017-04-06 00:28:04.082197: step 62060, loss = (G: 7.35965300, D: 0.02141152) (0.224 sec/batch)\n",
      "2017-04-06 00:28:08.594436: step 62080, loss = (G: 5.73026752, D: 0.03729135) (0.226 sec/batch)\n",
      "2017-04-06 00:28:13.095267: step 62100, loss = (G: 9.02466774, D: 0.01015641) (0.225 sec/batch)\n",
      "2017-04-06 00:28:17.732706: step 62120, loss = (G: 6.76619530, D: 0.02318928) (0.227 sec/batch)\n",
      "2017-04-06 00:28:22.237783: step 62140, loss = (G: 6.22056246, D: 0.01164652) (0.225 sec/batch)\n",
      "2017-04-06 00:28:26.729794: step 62160, loss = (G: 4.62334871, D: 0.17591994) (0.223 sec/batch)\n",
      "2017-04-06 00:28:31.242985: step 62180, loss = (G: 5.45521450, D: 0.05851171) (0.224 sec/batch)\n",
      "2017-04-06 00:28:35.731795: step 62200, loss = (G: 9.37093830, D: 0.00173113) (0.225 sec/batch)\n",
      "2017-04-06 00:28:40.385655: step 62220, loss = (G: 8.25949478, D: 0.09276485) (0.228 sec/batch)\n",
      "2017-04-06 00:28:44.882454: step 62240, loss = (G: 5.64089155, D: 0.03386771) (0.228 sec/batch)\n",
      "2017-04-06 00:28:49.371680: step 62260, loss = (G: 8.34855652, D: 0.00755186) (0.225 sec/batch)\n",
      "2017-04-06 00:28:53.862987: step 62280, loss = (G: 6.01341581, D: 0.02023876) (0.225 sec/batch)\n",
      "2017-04-06 00:28:58.354229: step 62300, loss = (G: 4.87509012, D: 0.05633020) (0.224 sec/batch)\n",
      "2017-04-06 00:29:02.994888: step 62320, loss = (G: 7.17046642, D: 0.01447589) (0.226 sec/batch)\n",
      "2017-04-06 00:29:07.505641: step 62340, loss = (G: 8.80856609, D: 0.00143550) (0.225 sec/batch)\n",
      "2017-04-06 00:29:11.987367: step 62360, loss = (G: 8.38950825, D: 0.00436773) (0.225 sec/batch)\n",
      "2017-04-06 00:29:16.486269: step 62380, loss = (G: 7.96387815, D: 0.00497119) (0.231 sec/batch)\n",
      "2017-04-06 00:29:21.018349: step 62400, loss = (G: 9.25568771, D: 0.63273591) (0.225 sec/batch)\n",
      "2017-04-06 00:29:25.684382: step 62420, loss = (G: 6.62052298, D: 0.02393132) (0.230 sec/batch)\n",
      "2017-04-06 00:29:30.182421: step 62440, loss = (G: 7.15897703, D: 0.17416649) (0.225 sec/batch)\n",
      "2017-04-06 00:29:34.657512: step 62460, loss = (G: 6.89064598, D: 0.03680138) (0.225 sec/batch)\n",
      "2017-04-06 00:29:39.149967: step 62480, loss = (G: 7.88958836, D: 0.00384687) (0.225 sec/batch)\n",
      "2017-04-06 00:29:43.630268: step 62500, loss = (G: 8.60553169, D: 0.09644163) (0.224 sec/batch)\n",
      "2017-04-06 00:29:48.274914: step 62520, loss = (G: 9.28631783, D: 0.13649130) (0.227 sec/batch)\n",
      "2017-04-06 00:29:52.765793: step 62540, loss = (G: 7.41176844, D: 0.03025252) (0.225 sec/batch)\n",
      "2017-04-06 00:29:57.256493: step 62560, loss = (G: 7.14295101, D: 0.09388875) (0.224 sec/batch)\n",
      "2017-04-06 00:30:01.752842: step 62580, loss = (G: 6.96140957, D: 0.01519893) (0.225 sec/batch)\n",
      "2017-04-06 00:30:06.265416: step 62600, loss = (G: 6.03661156, D: 0.04490568) (0.224 sec/batch)\n",
      "2017-04-06 00:30:11.042824: step 62620, loss = (G: 1.58834076, D: 1.95942330) (0.226 sec/batch)\n",
      "2017-04-06 00:30:15.528697: step 62640, loss = (G: 6.26641512, D: 0.01199630) (0.226 sec/batch)\n",
      "2017-04-06 00:30:20.022772: step 62660, loss = (G: 7.86311340, D: 0.00475906) (0.223 sec/batch)\n",
      "2017-04-06 00:30:24.528796: step 62680, loss = (G: 5.93669748, D: 0.02121583) (0.225 sec/batch)\n",
      "2017-04-06 00:30:29.030144: step 62700, loss = (G: 7.24314594, D: 0.01116453) (0.230 sec/batch)\n",
      "2017-04-06 00:30:33.671602: step 62720, loss = (G: 7.27672482, D: 0.01743910) (0.224 sec/batch)\n",
      "2017-04-06 00:30:38.203921: step 62740, loss = (G: 7.39864063, D: 0.03581928) (0.224 sec/batch)\n",
      "2017-04-06 00:30:42.702573: step 62760, loss = (G: 13.07000160, D: 0.46882236) (0.224 sec/batch)\n",
      "2017-04-06 00:30:47.200410: step 62780, loss = (G: 7.27919149, D: 0.01173839) (0.224 sec/batch)\n",
      "2017-04-06 00:30:51.715560: step 62800, loss = (G: 8.35707664, D: 0.00197364) (0.224 sec/batch)\n",
      "2017-04-06 00:30:56.343555: step 62820, loss = (G: 7.94327641, D: 0.00553838) (0.224 sec/batch)\n",
      "2017-04-06 00:31:00.824896: step 62840, loss = (G: 5.52645206, D: 0.01959090) (0.225 sec/batch)\n",
      "2017-04-06 00:31:05.362754: step 62860, loss = (G: 19.05151749, D: 0.00320123) (0.224 sec/batch)\n",
      "2017-04-06 00:31:09.868136: step 62880, loss = (G: 7.09161520, D: 0.01867745) (0.226 sec/batch)\n",
      "2017-04-06 00:31:14.353362: step 62900, loss = (G: 7.62353420, D: 0.11421739) (0.225 sec/batch)\n",
      "2017-04-06 00:31:18.982886: step 62920, loss = (G: 3.94796085, D: 0.15148027) (0.224 sec/batch)\n",
      "2017-04-06 00:31:23.470914: step 62940, loss = (G: 4.76087379, D: 0.04677020) (0.224 sec/batch)\n",
      "2017-04-06 00:31:27.971061: step 62960, loss = (G: 5.98521614, D: 0.01646107) (0.225 sec/batch)\n",
      "2017-04-06 00:31:32.465214: step 62980, loss = (G: 9.28698635, D: 0.01723850) (0.225 sec/batch)\n",
      "2017-04-06 00:31:36.955495: step 63000, loss = (G: 5.36957169, D: 0.03774995) (0.225 sec/batch)\n",
      "2017-04-06 00:31:41.576786: step 63020, loss = (G: 8.24464798, D: 0.00530676) (0.224 sec/batch)\n",
      "2017-04-06 00:31:46.061996: step 63040, loss = (G: 9.62725258, D: 0.00541036) (0.224 sec/batch)\n",
      "2017-04-06 00:31:50.573259: step 63060, loss = (G: 5.94522047, D: 0.09954000) (0.227 sec/batch)\n",
      "2017-04-06 00:31:55.067805: step 63080, loss = (G: 7.05102968, D: 0.03956707) (0.225 sec/batch)\n",
      "2017-04-06 00:31:59.547123: step 63100, loss = (G: 7.63321209, D: 0.00637122) (0.224 sec/batch)\n",
      "2017-04-06 00:32:04.224540: step 63120, loss = (G: 8.85085106, D: 0.00108304) (0.223 sec/batch)\n",
      "2017-04-06 00:32:08.723690: step 63140, loss = (G: 4.71679640, D: 0.09917964) (0.225 sec/batch)\n",
      "2017-04-06 00:32:13.207682: step 63160, loss = (G: 8.43842411, D: 0.04542771) (0.226 sec/batch)\n",
      "2017-04-06 00:32:17.711744: step 63180, loss = (G: 7.37932539, D: 0.01183909) (0.227 sec/batch)\n",
      "2017-04-06 00:32:22.202868: step 63200, loss = (G: 7.30457497, D: 0.00655946) (0.224 sec/batch)\n",
      "2017-04-06 00:32:26.828577: step 63220, loss = (G: 6.90621710, D: 0.01831726) (0.223 sec/batch)\n",
      "2017-04-06 00:32:31.318590: step 63240, loss = (G: 6.72100115, D: 0.01254229) (0.224 sec/batch)\n",
      "2017-04-06 00:32:35.825267: step 63260, loss = (G: 10.75558186, D: 0.01529616) (0.224 sec/batch)\n",
      "2017-04-06 00:32:40.315511: step 63280, loss = (G: 8.59549809, D: 0.00153490) (0.225 sec/batch)\n",
      "2017-04-06 00:32:44.807552: step 63300, loss = (G: 6.89089632, D: 0.00799254) (0.225 sec/batch)\n",
      "2017-04-06 00:32:49.438866: step 63320, loss = (G: 8.09035015, D: 0.03347064) (0.225 sec/batch)\n",
      "2017-04-06 00:32:53.925700: step 63340, loss = (G: 4.28415537, D: 0.06110864) (0.224 sec/batch)\n",
      "2017-04-06 00:32:58.431568: step 63360, loss = (G: 6.55681658, D: 0.00990704) (0.224 sec/batch)\n",
      "2017-04-06 00:33:02.937895: step 63380, loss = (G: 7.87125969, D: 0.00725302) (0.224 sec/batch)\n",
      "2017-04-06 00:33:07.453440: step 63400, loss = (G: 12.35816383, D: 0.00014382) (0.227 sec/batch)\n",
      "2017-04-06 00:33:12.086255: step 63420, loss = (G: 10.19331169, D: 0.01503429) (0.223 sec/batch)\n",
      "2017-04-06 00:33:16.583005: step 63440, loss = (G: 6.68653774, D: 0.01634781) (0.225 sec/batch)\n",
      "2017-04-06 00:33:21.071617: step 63460, loss = (G: 8.06982899, D: 0.09732637) (0.224 sec/batch)\n",
      "2017-04-06 00:33:25.561830: step 63480, loss = (G: 5.59272003, D: 0.04816146) (0.223 sec/batch)\n",
      "2017-04-06 00:33:30.055040: step 63500, loss = (G: 11.42716789, D: 0.00045901) (0.225 sec/batch)\n",
      "2017-04-06 00:33:34.690805: step 63520, loss = (G: 7.97817421, D: 0.01110809) (0.225 sec/batch)\n",
      "2017-04-06 00:33:39.181690: step 63540, loss = (G: 4.83023977, D: 0.08564290) (0.223 sec/batch)\n",
      "2017-04-06 00:33:43.688323: step 63560, loss = (G: 5.72121572, D: 0.05138386) (0.224 sec/batch)\n",
      "2017-04-06 00:33:48.181032: step 63580, loss = (G: 6.00850630, D: 0.04421048) (0.224 sec/batch)\n",
      "2017-04-06 00:33:52.697512: step 63600, loss = (G: 9.10850906, D: 0.01288039) (0.231 sec/batch)\n",
      "2017-04-06 00:33:57.365651: step 63620, loss = (G: 6.84206247, D: 0.03837825) (0.227 sec/batch)\n",
      "2017-04-06 00:34:01.863791: step 63640, loss = (G: 4.90750599, D: 0.12145442) (0.222 sec/batch)\n",
      "2017-04-06 00:34:06.355131: step 63660, loss = (G: 6.78902483, D: 0.03059164) (0.226 sec/batch)\n",
      "2017-04-06 00:34:10.847312: step 63680, loss = (G: 6.60922003, D: 0.01394012) (0.224 sec/batch)\n",
      "2017-04-06 00:34:15.339707: step 63700, loss = (G: 16.34595490, D: 0.00733076) (0.225 sec/batch)\n",
      "2017-04-06 00:34:19.977657: step 63720, loss = (G: 6.60435009, D: 0.01028544) (0.225 sec/batch)\n",
      "2017-04-06 00:34:24.467405: step 63740, loss = (G: 6.23788929, D: 0.02424066) (0.224 sec/batch)\n",
      "2017-04-06 00:34:28.952769: step 63760, loss = (G: 8.23612118, D: 0.00287969) (0.224 sec/batch)\n",
      "2017-04-06 00:34:33.471980: step 63780, loss = (G: 8.32855797, D: 0.04302805) (0.224 sec/batch)\n",
      "2017-04-06 00:34:37.958582: step 63800, loss = (G: 11.42414951, D: 0.05919984) (0.225 sec/batch)\n",
      "2017-04-06 00:34:42.604893: step 63820, loss = (G: 7.56061459, D: 0.00636705) (0.225 sec/batch)\n",
      "2017-04-06 00:34:47.102442: step 63840, loss = (G: 6.89653397, D: 0.01626585) (0.230 sec/batch)\n",
      "2017-04-06 00:34:51.603650: step 63860, loss = (G: 5.79110146, D: 0.01637752) (0.224 sec/batch)\n",
      "2017-04-06 00:34:56.096902: step 63880, loss = (G: 6.31181908, D: 0.01360702) (0.230 sec/batch)\n",
      "2017-04-06 00:35:00.628096: step 63900, loss = (G: 7.06718445, D: 0.04822426) (0.224 sec/batch)\n",
      "2017-04-06 00:35:05.297157: step 63920, loss = (G: 7.87402964, D: 0.00410891) (0.230 sec/batch)\n",
      "2017-04-06 00:35:09.794107: step 63940, loss = (G: 9.15708065, D: 0.00358376) (0.224 sec/batch)\n",
      "2017-04-06 00:35:14.287419: step 63960, loss = (G: 6.15474033, D: 0.03305265) (0.224 sec/batch)\n",
      "2017-04-06 00:35:18.770067: step 63980, loss = (G: 7.08102608, D: 0.08191567) (0.223 sec/batch)\n",
      "2017-04-06 00:35:23.277950: step 64000, loss = (G: 5.04003859, D: 0.04193112) (0.224 sec/batch)\n",
      "2017-04-06 00:35:27.903197: step 64020, loss = (G: 9.35616016, D: 0.00216609) (0.224 sec/batch)\n",
      "2017-04-06 00:35:32.386573: step 64040, loss = (G: 8.04208565, D: 0.00330101) (0.226 sec/batch)\n",
      "2017-04-06 00:35:36.886249: step 64060, loss = (G: 8.92218399, D: 0.00365881) (0.226 sec/batch)\n",
      "2017-04-06 00:35:41.373993: step 64080, loss = (G: 10.22956657, D: 0.00031047) (0.226 sec/batch)\n",
      "2017-04-06 00:35:45.865851: step 64100, loss = (G: 8.86112595, D: 0.00346743) (0.225 sec/batch)\n",
      "2017-04-06 00:35:50.500093: step 64120, loss = (G: 7.34018135, D: 0.01193084) (0.224 sec/batch)\n",
      "2017-04-06 00:35:55.002674: step 64140, loss = (G: 6.68594551, D: 0.05110228) (0.224 sec/batch)\n",
      "2017-04-06 00:35:59.518335: step 64160, loss = (G: 2.75226450, D: 0.63502914) (0.224 sec/batch)\n",
      "2017-04-06 00:36:04.017506: step 64180, loss = (G: 5.09840775, D: 0.08711312) (0.226 sec/batch)\n",
      "2017-04-06 00:36:08.523948: step 64200, loss = (G: 6.82992697, D: 0.01335180) (0.226 sec/batch)\n",
      "2017-04-06 00:36:13.159926: step 64220, loss = (G: 17.02435493, D: 0.47901940) (0.223 sec/batch)\n",
      "2017-04-06 00:36:17.658476: step 64240, loss = (G: 7.09419632, D: 0.01258221) (0.226 sec/batch)\n",
      "2017-04-06 00:36:22.153989: step 64260, loss = (G: 8.29882145, D: 0.00612652) (0.225 sec/batch)\n",
      "2017-04-06 00:36:26.652464: step 64280, loss = (G: 8.97206211, D: 0.00631597) (0.224 sec/batch)\n",
      "2017-04-06 00:36:31.157116: step 64300, loss = (G: 10.58813000, D: 0.20110737) (0.225 sec/batch)\n",
      "2017-04-06 00:36:35.785189: step 64320, loss = (G: 8.04434967, D: 0.00564273) (0.224 sec/batch)\n",
      "2017-04-06 00:36:40.272499: step 64340, loss = (G: 6.08375311, D: 0.03149432) (0.224 sec/batch)\n",
      "2017-04-06 00:36:44.760139: step 64360, loss = (G: 8.87874126, D: 0.00200520) (0.224 sec/batch)\n",
      "2017-04-06 00:36:49.256774: step 64380, loss = (G: 9.24721718, D: 0.00278037) (0.229 sec/batch)\n",
      "2017-04-06 00:36:53.760896: step 64400, loss = (G: 8.63281727, D: 0.01186978) (0.230 sec/batch)\n",
      "2017-04-06 00:36:58.401713: step 64420, loss = (G: 10.81514549, D: 0.16913977) (0.224 sec/batch)\n",
      "2017-04-06 00:37:02.912293: step 64440, loss = (G: 6.67497063, D: 0.01576667) (0.224 sec/batch)\n",
      "2017-04-06 00:37:07.398364: step 64460, loss = (G: 6.57972431, D: 0.01038001) (0.225 sec/batch)\n",
      "2017-04-06 00:37:11.888333: step 64480, loss = (G: 8.99651241, D: 0.00141523) (0.224 sec/batch)\n",
      "2017-04-06 00:37:16.388198: step 64500, loss = (G: 6.30391979, D: 0.01811464) (0.225 sec/batch)\n",
      "2017-04-06 00:37:21.020768: step 64520, loss = (G: 5.77421761, D: 0.04063594) (0.225 sec/batch)\n",
      "2017-04-06 00:37:25.564387: step 64540, loss = (G: 7.05117226, D: 0.01316621) (0.227 sec/batch)\n",
      "2017-04-06 00:37:30.049382: step 64560, loss = (G: 7.61510944, D: 0.00466545) (0.224 sec/batch)\n",
      "2017-04-06 00:37:34.549234: step 64580, loss = (G: 6.13615751, D: 0.02008686) (0.225 sec/batch)\n",
      "2017-04-06 00:37:39.038272: step 64600, loss = (G: 6.03959608, D: 0.02496415) (0.224 sec/batch)\n",
      "2017-04-06 00:37:43.660178: step 64620, loss = (G: 9.61379242, D: 0.00155447) (0.224 sec/batch)\n",
      "2017-04-06 00:37:48.156801: step 64640, loss = (G: 7.17964458, D: 0.04473802) (0.224 sec/batch)\n",
      "2017-04-06 00:37:52.644919: step 64660, loss = (G: 6.25608206, D: 0.01476477) (0.224 sec/batch)\n",
      "2017-04-06 00:37:57.137313: step 64680, loss = (G: 13.09418869, D: 0.05262334) (0.224 sec/batch)\n",
      "2017-04-06 00:38:01.645229: step 64700, loss = (G: 6.49131823, D: 0.01191828) (0.230 sec/batch)\n",
      "2017-04-06 00:38:06.300586: step 64720, loss = (G: 6.75571871, D: 0.01004092) (0.227 sec/batch)\n",
      "2017-04-06 00:38:10.802925: step 64740, loss = (G: 9.13199043, D: 0.04864464) (0.225 sec/batch)\n",
      "2017-04-06 00:38:15.290932: step 64760, loss = (G: 6.66720104, D: 0.05517821) (0.224 sec/batch)\n",
      "2017-04-06 00:38:19.804534: step 64780, loss = (G: 5.57756424, D: 0.04455512) (0.224 sec/batch)\n",
      "2017-04-06 00:38:24.295001: step 64800, loss = (G: 15.19207954, D: 0.02950750) (0.224 sec/batch)\n",
      "2017-04-06 00:38:28.928175: step 64820, loss = (G: 7.82392502, D: 0.03580551) (0.224 sec/batch)\n",
      "2017-04-06 00:38:33.422362: step 64840, loss = (G: 5.06138134, D: 0.06872679) (0.224 sec/batch)\n",
      "2017-04-06 00:38:37.911801: step 64860, loss = (G: 8.67219448, D: 0.01301543) (0.223 sec/batch)\n",
      "2017-04-06 00:38:42.422347: step 64880, loss = (G: 6.09870625, D: 0.04613702) (0.225 sec/batch)\n",
      "2017-04-06 00:38:46.914975: step 64900, loss = (G: 8.35705280, D: 0.00467191) (0.225 sec/batch)\n",
      "2017-04-06 00:38:51.551908: step 64920, loss = (G: 6.24180317, D: 0.07486346) (0.225 sec/batch)\n",
      "2017-04-06 00:38:56.092831: step 64940, loss = (G: 7.62986803, D: 0.28111801) (0.225 sec/batch)\n",
      "2017-04-06 00:39:00.589861: step 64960, loss = (G: 6.13643074, D: 0.04249731) (0.226 sec/batch)\n",
      "2017-04-06 00:39:05.080207: step 64980, loss = (G: 7.57199240, D: 0.00838806) (0.224 sec/batch)\n",
      "2017-04-06 00:39:09.611359: step 65000, loss = (G: 1.78259945, D: 3.79918218) (0.228 sec/batch)\n",
      "2017-04-06 00:39:14.285098: step 65020, loss = (G: 6.27963924, D: 0.11417418) (0.228 sec/batch)\n",
      "2017-04-06 00:39:18.765123: step 65040, loss = (G: 4.99148655, D: 0.03614549) (0.225 sec/batch)\n",
      "2017-04-06 00:39:23.302412: step 65060, loss = (G: 6.14630270, D: 0.01942133) (0.223 sec/batch)\n",
      "2017-04-06 00:39:27.791436: step 65080, loss = (G: 7.85672855, D: 0.00952460) (0.224 sec/batch)\n",
      "2017-04-06 00:39:32.271207: step 65100, loss = (G: 3.69507456, D: 0.66735768) (0.224 sec/batch)\n",
      "2017-04-06 00:39:36.888057: step 65120, loss = (G: 7.92677259, D: 0.00540626) (0.224 sec/batch)\n",
      "2017-04-06 00:39:41.377677: step 65140, loss = (G: 6.96259689, D: 0.02732389) (0.225 sec/batch)\n",
      "2017-04-06 00:39:45.866277: step 65160, loss = (G: 7.55532980, D: 0.01687924) (0.225 sec/batch)\n",
      "2017-04-06 00:39:50.353440: step 65180, loss = (G: 4.36810684, D: 0.14363474) (0.224 sec/batch)\n",
      "2017-04-06 00:39:54.852869: step 65200, loss = (G: 7.99896431, D: 0.00571748) (0.226 sec/batch)\n",
      "2017-04-06 00:39:59.481985: step 65220, loss = (G: 6.17909670, D: 0.01645094) (0.224 sec/batch)\n",
      "2017-04-06 00:40:03.981100: step 65240, loss = (G: 9.76368523, D: 0.00300202) (0.226 sec/batch)\n",
      "2017-04-06 00:40:08.501077: step 65260, loss = (G: 12.32795429, D: 0.01018347) (0.225 sec/batch)\n",
      "2017-04-06 00:40:12.995397: step 65280, loss = (G: 7.17322588, D: 0.02054614) (0.224 sec/batch)\n",
      "2017-04-06 00:40:17.489409: step 65300, loss = (G: 5.67079353, D: 0.02071677) (0.224 sec/batch)\n",
      "2017-04-06 00:40:22.121633: step 65320, loss = (G: 7.53846884, D: 0.01637749) (0.226 sec/batch)\n",
      "2017-04-06 00:40:26.614752: step 65340, loss = (G: 4.68432283, D: 0.06521059) (0.225 sec/batch)\n",
      "2017-04-06 00:40:31.152969: step 65360, loss = (G: 7.33373594, D: 0.00764622) (0.228 sec/batch)\n",
      "2017-04-06 00:40:35.640281: step 65380, loss = (G: 11.76488972, D: 0.00762651) (0.223 sec/batch)\n",
      "2017-04-06 00:40:40.124584: step 65400, loss = (G: 10.51197910, D: 0.03178461) (0.224 sec/batch)\n",
      "2017-04-06 00:40:44.759730: step 65420, loss = (G: 10.92982864, D: 0.00312418) (0.224 sec/batch)\n",
      "2017-04-06 00:40:49.262808: step 65440, loss = (G: 7.91100311, D: 0.00349227) (0.225 sec/batch)\n",
      "2017-04-06 00:40:53.744762: step 65460, loss = (G: 5.53365231, D: 0.05402222) (0.224 sec/batch)\n",
      "2017-04-06 00:40:58.225537: step 65480, loss = (G: 7.64900637, D: 0.02624250) (0.225 sec/batch)\n",
      "2017-04-06 00:41:02.723910: step 65500, loss = (G: 6.22780657, D: 0.02453786) (0.225 sec/batch)\n",
      "2017-04-06 00:41:07.361728: step 65520, loss = (G: 6.74760103, D: 0.03174338) (0.225 sec/batch)\n",
      "2017-04-06 00:41:11.853475: step 65540, loss = (G: 8.89516926, D: 0.00154199) (0.226 sec/batch)\n",
      "2017-04-06 00:41:16.366418: step 65560, loss = (G: 8.46479511, D: 0.00311874) (0.224 sec/batch)\n",
      "2017-04-06 00:41:20.854608: step 65580, loss = (G: 8.47707558, D: 0.00928852) (0.225 sec/batch)\n",
      "2017-04-06 00:41:25.343601: step 65600, loss = (G: 7.40987730, D: 0.00919253) (0.226 sec/batch)\n",
      "2017-04-06 00:41:29.985847: step 65620, loss = (G: 7.98663521, D: 0.02250245) (0.231 sec/batch)\n",
      "2017-04-06 00:41:34.512663: step 65640, loss = (G: 6.62285900, D: 0.01823976) (0.225 sec/batch)\n",
      "2017-04-06 00:41:39.001744: step 65660, loss = (G: 6.36762238, D: 0.02183951) (0.225 sec/batch)\n",
      "2017-04-06 00:41:43.486139: step 65680, loss = (G: 7.49244499, D: 0.00531848) (0.224 sec/batch)\n",
      "2017-04-06 00:41:47.977078: step 65700, loss = (G: 5.99760818, D: 0.01590843) (0.225 sec/batch)\n",
      "2017-04-06 00:41:52.602488: step 65720, loss = (G: 6.08112144, D: 0.03521493) (0.225 sec/batch)\n",
      "2017-04-06 00:41:57.097062: step 65740, loss = (G: 7.06277227, D: 0.01382134) (0.226 sec/batch)\n",
      "2017-04-06 00:42:01.604005: step 65760, loss = (G: 18.31586647, D: 0.00051791) (0.225 sec/batch)\n",
      "2017-04-06 00:42:06.109885: step 65780, loss = (G: 6.05293369, D: 0.01373601) (0.225 sec/batch)\n",
      "2017-04-06 00:42:10.649477: step 65800, loss = (G: 6.86887407, D: 0.02240957) (0.225 sec/batch)\n",
      "2017-04-06 00:42:15.285167: step 65820, loss = (G: 4.93413877, D: 0.04128662) (0.225 sec/batch)\n",
      "2017-04-06 00:42:19.777602: step 65840, loss = (G: 4.72856236, D: 0.11234887) (0.223 sec/batch)\n",
      "2017-04-06 00:42:24.283284: step 65860, loss = (G: 7.29842567, D: 0.02182576) (0.225 sec/batch)\n",
      "2017-04-06 00:42:28.765626: step 65880, loss = (G: 1.94962287, D: 1.35032058) (0.225 sec/batch)\n",
      "2017-04-06 00:42:33.261442: step 65900, loss = (G: 13.12195396, D: 0.00485010) (0.225 sec/batch)\n",
      "2017-04-06 00:42:37.892650: step 65920, loss = (G: 5.25544357, D: 0.05273411) (0.224 sec/batch)\n",
      "2017-04-06 00:42:42.382058: step 65940, loss = (G: 6.79152918, D: 0.01851540) (0.225 sec/batch)\n",
      "2017-04-06 00:42:46.869964: step 65960, loss = (G: 7.49042797, D: 0.01264954) (0.223 sec/batch)\n",
      "2017-04-06 00:42:51.361637: step 65980, loss = (G: 12.41192913, D: 0.18731026) (0.224 sec/batch)\n",
      "2017-04-06 00:42:55.856819: step 66000, loss = (G: 8.11954212, D: 0.01029036) (0.225 sec/batch)\n",
      "2017-04-06 00:43:00.484791: step 66020, loss = (G: 8.13524055, D: 0.01143217) (0.223 sec/batch)\n",
      "2017-04-06 00:43:04.984670: step 66040, loss = (G: 5.17955208, D: 0.04944355) (0.223 sec/batch)\n",
      "2017-04-06 00:43:09.476068: step 66060, loss = (G: 10.28873634, D: 0.03576671) (0.225 sec/batch)\n",
      "2017-04-06 00:43:13.981428: step 66080, loss = (G: 8.22700214, D: 0.01930930) (0.224 sec/batch)\n",
      "2017-04-06 00:43:18.608232: step 66100, loss = (G: 6.12912941, D: 0.46731311) (0.224 sec/batch)\n",
      "2017-04-06 00:43:23.246082: step 66120, loss = (G: 10.65738869, D: 0.00070158) (0.225 sec/batch)\n",
      "2017-04-06 00:43:27.739043: step 66140, loss = (G: 8.65460014, D: 0.07349389) (0.224 sec/batch)\n",
      "2017-04-06 00:43:32.234463: step 66160, loss = (G: 8.39374733, D: 0.00764807) (0.231 sec/batch)\n",
      "2017-04-06 00:43:36.744518: step 66180, loss = (G: 5.89680243, D: 0.02248610) (0.224 sec/batch)\n",
      "2017-04-06 00:43:41.235759: step 66200, loss = (G: 8.29266739, D: 0.00570934) (0.225 sec/batch)\n",
      "2017-04-06 00:43:45.868144: step 66220, loss = (G: 5.63878822, D: 0.04017227) (0.230 sec/batch)\n",
      "2017-04-06 00:43:50.396428: step 66240, loss = (G: 6.47762632, D: 0.01505283) (0.226 sec/batch)\n",
      "2017-04-06 00:43:54.890234: step 66260, loss = (G: 6.88193703, D: 0.04608740) (0.224 sec/batch)\n",
      "2017-04-06 00:43:59.384157: step 66280, loss = (G: 7.92083263, D: 0.04907664) (0.225 sec/batch)\n",
      "2017-04-06 00:44:03.877204: step 66300, loss = (G: 5.79467010, D: 0.03778147) (0.224 sec/batch)\n",
      "2017-04-06 00:44:08.524551: step 66320, loss = (G: 9.98430634, D: 0.02862560) (0.224 sec/batch)\n",
      "2017-04-06 00:44:13.010107: step 66340, loss = (G: 6.72982025, D: 0.01100917) (0.224 sec/batch)\n",
      "2017-04-06 00:44:17.496836: step 66360, loss = (G: 4.89907980, D: 0.04471944) (0.224 sec/batch)\n",
      "2017-04-06 00:44:21.990702: step 66380, loss = (G: 5.58990669, D: 0.04869100) (0.226 sec/batch)\n",
      "2017-04-06 00:44:26.479140: step 66400, loss = (G: 7.29695320, D: 0.03907293) (0.224 sec/batch)\n",
      "2017-04-06 00:44:31.133907: step 66420, loss = (G: 3.70587683, D: 0.27385688) (0.224 sec/batch)\n",
      "2017-04-06 00:44:35.640043: step 66440, loss = (G: 5.74321842, D: 0.02100923) (0.225 sec/batch)\n",
      "2017-04-06 00:44:40.134044: step 66460, loss = (G: 16.13097382, D: 0.00285072) (0.224 sec/batch)\n",
      "2017-04-06 00:44:44.618784: step 66480, loss = (G: 8.57153416, D: 0.00270647) (0.224 sec/batch)\n",
      "2017-04-06 00:44:49.108965: step 66500, loss = (G: 6.27672958, D: 0.01753943) (0.224 sec/batch)\n",
      "2017-04-06 00:44:53.757311: step 66520, loss = (G: 8.46967411, D: 0.01328398) (0.224 sec/batch)\n",
      "2017-04-06 00:44:58.235829: step 66540, loss = (G: 9.78403568, D: 0.00366373) (0.224 sec/batch)\n",
      "2017-04-06 00:45:02.729601: step 66560, loss = (G: 12.34591961, D: 0.03816091) (0.223 sec/batch)\n",
      "2017-04-06 00:45:07.214721: step 66580, loss = (G: 7.84330273, D: 0.11488131) (0.223 sec/batch)\n",
      "2017-04-06 00:45:11.708121: step 66600, loss = (G: 5.46763372, D: 0.03154527) (0.224 sec/batch)\n",
      "2017-04-06 00:45:16.344766: step 66620, loss = (G: 12.76141644, D: 0.00213299) (0.225 sec/batch)\n",
      "2017-04-06 00:45:20.832069: step 66640, loss = (G: 8.36636829, D: 0.01709672) (0.225 sec/batch)\n",
      "2017-04-06 00:45:25.332366: step 66660, loss = (G: 8.09048176, D: 0.00287829) (0.225 sec/batch)\n",
      "2017-04-06 00:45:29.824149: step 66680, loss = (G: 8.39881992, D: 0.00288640) (0.224 sec/batch)\n",
      "2017-04-06 00:45:34.317312: step 66700, loss = (G: 6.58287907, D: 0.01409484) (0.225 sec/batch)\n",
      "2017-04-06 00:45:38.940605: step 66720, loss = (G: 7.80114269, D: 0.01390169) (0.225 sec/batch)\n",
      "2017-04-06 00:45:43.452684: step 66740, loss = (G: 10.90801334, D: 0.05546698) (0.224 sec/batch)\n",
      "2017-04-06 00:45:47.950286: step 66760, loss = (G: 11.42813683, D: 0.01164585) (0.225 sec/batch)\n",
      "2017-04-06 00:45:52.440277: step 66780, loss = (G: 7.38292313, D: 0.00771295) (0.224 sec/batch)\n",
      "2017-04-06 00:45:56.941852: step 66800, loss = (G: 7.04187584, D: 0.00904939) (0.226 sec/batch)\n",
      "2017-04-06 00:46:01.560017: step 66820, loss = (G: 10.60692883, D: 0.00323401) (0.223 sec/batch)\n",
      "2017-04-06 00:46:06.064871: step 66840, loss = (G: 7.23598814, D: 0.00776902) (0.224 sec/batch)\n",
      "2017-04-06 00:46:10.559185: step 66860, loss = (G: 6.74525452, D: 0.02466737) (0.225 sec/batch)\n",
      "2017-04-06 00:46:15.052792: step 66880, loss = (G: 7.82854652, D: 0.00642050) (0.225 sec/batch)\n",
      "2017-04-06 00:46:19.545082: step 66900, loss = (G: 6.16367626, D: 0.02292159) (0.224 sec/batch)\n",
      "2017-04-06 00:46:24.178664: step 66920, loss = (G: 6.73989248, D: 0.02470072) (0.224 sec/batch)\n",
      "2017-04-06 00:46:28.715976: step 66940, loss = (G: 8.66970825, D: 0.03918514) (0.224 sec/batch)\n",
      "2017-04-06 00:46:33.213938: step 66960, loss = (G: 5.27125072, D: 0.02994863) (0.225 sec/batch)\n",
      "2017-04-06 00:46:37.697716: step 66980, loss = (G: 9.35268879, D: 0.00067785) (0.224 sec/batch)\n",
      "2017-04-06 00:46:42.185348: step 67000, loss = (G: 6.53212929, D: 0.01630813) (0.226 sec/batch)\n",
      "2017-04-06 00:46:46.858381: step 67020, loss = (G: 7.75102043, D: 0.01260192) (0.224 sec/batch)\n",
      "2017-04-06 00:46:51.344664: step 67040, loss = (G: 7.49714422, D: 0.01717225) (0.224 sec/batch)\n",
      "2017-04-06 00:46:55.822562: step 67060, loss = (G: 15.91984940, D: 0.01182668) (0.224 sec/batch)\n",
      "2017-04-06 00:47:00.307708: step 67080, loss = (G: 7.43827915, D: 0.00560333) (0.226 sec/batch)\n",
      "2017-04-06 00:47:04.794296: step 67100, loss = (G: 5.44959068, D: 0.02885237) (0.224 sec/batch)\n",
      "2017-04-06 00:47:09.427328: step 67120, loss = (G: 9.75183487, D: 0.09265386) (0.225 sec/batch)\n",
      "2017-04-06 00:47:13.915021: step 67140, loss = (G: 4.85925865, D: 0.06154380) (0.223 sec/batch)\n",
      "2017-04-06 00:47:18.408810: step 67160, loss = (G: 8.35675621, D: 0.04559147) (0.224 sec/batch)\n",
      "2017-04-06 00:47:22.905441: step 67180, loss = (G: 8.13924313, D: 0.00260622) (0.224 sec/batch)\n",
      "2017-04-06 00:47:27.391549: step 67200, loss = (G: 4.10785484, D: 0.15096551) (0.225 sec/batch)\n",
      "2017-04-06 00:47:32.019926: step 67220, loss = (G: 6.36807537, D: 0.01523033) (0.223 sec/batch)\n",
      "2017-04-06 00:47:36.530776: step 67240, loss = (G: 7.92126179, D: 0.00876815) (0.226 sec/batch)\n",
      "2017-04-06 00:47:41.147660: step 67260, loss = (G: 3.35340238, D: 0.22145899) (0.224 sec/batch)\n",
      "2017-04-06 00:47:45.636601: step 67280, loss = (G: 9.39111996, D: 0.00243126) (0.224 sec/batch)\n",
      "2017-04-06 00:47:50.121700: step 67300, loss = (G: 7.10047293, D: 0.01287813) (0.224 sec/batch)\n",
      "2017-04-06 00:47:54.757380: step 67320, loss = (G: 9.85793972, D: 0.00132895) (0.225 sec/batch)\n",
      "2017-04-06 00:47:59.261145: step 67340, loss = (G: 8.86262035, D: 0.00416283) (0.230 sec/batch)\n",
      "2017-04-06 00:48:03.792576: step 67360, loss = (G: 6.35286903, D: 0.01537291) (0.224 sec/batch)\n",
      "2017-04-06 00:48:08.276486: step 67380, loss = (G: 10.63489342, D: 0.01093765) (0.224 sec/batch)\n",
      "2017-04-06 00:48:12.757512: step 67400, loss = (G: 8.26348400, D: 0.09984264) (0.225 sec/batch)\n",
      "2017-04-06 00:48:17.387048: step 67420, loss = (G: 5.99772549, D: 0.05984357) (0.224 sec/batch)\n",
      "2017-04-06 00:48:21.886019: step 67440, loss = (G: 16.51446152, D: 0.01666753) (0.225 sec/batch)\n",
      "2017-04-06 00:48:26.393200: step 67460, loss = (G: 7.73522711, D: 0.00910243) (0.226 sec/batch)\n",
      "2017-04-06 00:48:30.887416: step 67480, loss = (G: 13.39775085, D: 0.00025364) (0.224 sec/batch)\n",
      "2017-04-06 00:48:35.397678: step 67500, loss = (G: 7.81942701, D: 0.02249123) (0.226 sec/batch)\n",
      "2017-04-06 00:48:40.025930: step 67520, loss = (G: 9.39087486, D: 0.00426810) (0.226 sec/batch)\n",
      "2017-04-06 00:48:44.513430: step 67540, loss = (G: 5.43319082, D: 0.04023131) (0.225 sec/batch)\n",
      "2017-04-06 00:48:49.003799: step 67560, loss = (G: 6.63410330, D: 0.01265695) (0.224 sec/batch)\n",
      "2017-04-06 00:48:53.485055: step 67580, loss = (G: 7.23489094, D: 0.01449434) (0.225 sec/batch)\n",
      "2017-04-06 00:48:57.988757: step 67600, loss = (G: 10.21853065, D: 0.27535388) (0.225 sec/batch)\n",
      "2017-04-06 00:49:02.614325: step 67620, loss = (G: 12.43652916, D: 0.05073568) (0.225 sec/batch)\n",
      "2017-04-06 00:49:07.101517: step 67640, loss = (G: 20.52700043, D: 0.15610734) (0.225 sec/batch)\n",
      "2017-04-06 00:49:11.607776: step 67660, loss = (G: 9.61311150, D: 0.05715542) (0.226 sec/batch)\n",
      "2017-04-06 00:49:16.090251: step 67680, loss = (G: 4.66923475, D: 0.09304702) (0.225 sec/batch)\n",
      "2017-04-06 00:49:20.591660: step 67700, loss = (G: 2.76990557, D: 0.88295633) (0.224 sec/batch)\n",
      "2017-04-06 00:49:25.226194: step 67720, loss = (G: 7.84249783, D: 0.01862827) (0.224 sec/batch)\n",
      "2017-04-06 00:49:29.729870: step 67740, loss = (G: 10.75303078, D: 0.01821629) (0.225 sec/batch)\n",
      "2017-04-06 00:49:34.224739: step 67760, loss = (G: 8.61670208, D: 0.23818819) (0.224 sec/batch)\n",
      "2017-04-06 00:49:38.722397: step 67780, loss = (G: 13.14839077, D: 0.00499643) (0.224 sec/batch)\n",
      "2017-04-06 00:49:43.210300: step 67800, loss = (G: 9.04243088, D: 0.00166971) (0.224 sec/batch)\n",
      "2017-04-06 00:49:47.832567: step 67820, loss = (G: 8.55148029, D: 0.00523578) (0.223 sec/batch)\n",
      "2017-04-06 00:49:52.335440: step 67840, loss = (G: 6.48885202, D: 0.01519718) (0.226 sec/batch)\n",
      "2017-04-06 00:49:56.827351: step 67860, loss = (G: 9.94873142, D: 0.00084170) (0.224 sec/batch)\n",
      "2017-04-06 00:50:01.337925: step 67880, loss = (G: 9.09935093, D: 0.01980471) (0.224 sec/batch)\n",
      "2017-04-06 00:50:05.820697: step 67900, loss = (G: 11.16479206, D: 0.00054799) (0.224 sec/batch)\n",
      "2017-04-06 00:50:10.453904: step 67920, loss = (G: 8.18357277, D: 0.01450266) (0.226 sec/batch)\n",
      "2017-04-06 00:50:14.948388: step 67940, loss = (G: 10.15816402, D: 0.00493793) (0.226 sec/batch)\n",
      "2017-04-06 00:50:19.434773: step 67960, loss = (G: 9.75908089, D: 0.29110932) (0.225 sec/batch)\n",
      "2017-04-06 00:50:23.918951: step 67980, loss = (G: 6.65703154, D: 0.01075388) (0.224 sec/batch)\n",
      "2017-04-06 00:50:28.404295: step 68000, loss = (G: 7.51207352, D: 0.28653717) (0.226 sec/batch)\n",
      "2017-04-06 00:50:33.042667: step 68020, loss = (G: 8.89397049, D: 0.00199840) (0.225 sec/batch)\n",
      "2017-04-06 00:50:37.536732: step 68040, loss = (G: 7.69859457, D: 0.00740458) (0.224 sec/batch)\n",
      "2017-04-06 00:50:42.030795: step 68060, loss = (G: 5.70026636, D: 0.25259343) (0.226 sec/batch)\n",
      "2017-04-06 00:50:46.532027: step 68080, loss = (G: 8.48017883, D: 0.05912309) (0.227 sec/batch)\n",
      "2017-04-06 00:50:51.026039: step 68100, loss = (G: 11.86451530, D: 0.06491072) (0.225 sec/batch)\n",
      "2017-04-06 00:50:55.657060: step 68120, loss = (G: 7.31434917, D: 0.00594160) (0.224 sec/batch)\n",
      "2017-04-06 00:51:00.161712: step 68140, loss = (G: 7.59669781, D: 0.00773289) (0.226 sec/batch)\n",
      "2017-04-06 00:51:04.651403: step 68160, loss = (G: 5.05305767, D: 0.06778451) (0.224 sec/batch)\n",
      "2017-04-06 00:51:09.141826: step 68180, loss = (G: 7.58103466, D: 0.01762328) (0.225 sec/batch)\n",
      "2017-04-06 00:51:13.625192: step 68200, loss = (G: 9.17514229, D: 0.01478923) (0.224 sec/batch)\n",
      "2017-04-06 00:51:18.260370: step 68220, loss = (G: 4.57162094, D: 0.06090349) (0.225 sec/batch)\n",
      "2017-04-06 00:51:22.746621: step 68240, loss = (G: 5.04951859, D: 0.07230374) (0.223 sec/batch)\n",
      "2017-04-06 00:51:27.251912: step 68260, loss = (G: 7.89339447, D: 0.00410721) (0.224 sec/batch)\n",
      "2017-04-06 00:51:31.744274: step 68280, loss = (G: 5.59810734, D: 0.03884980) (0.225 sec/batch)\n",
      "2017-04-06 00:51:36.234763: step 68300, loss = (G: 10.23915672, D: 0.00089754) (0.225 sec/batch)\n",
      "2017-04-06 00:51:40.856536: step 68320, loss = (G: 13.10176373, D: 0.01820421) (0.224 sec/batch)\n",
      "2017-04-06 00:51:45.352327: step 68340, loss = (G: 6.04429722, D: 0.03194635) (0.225 sec/batch)\n",
      "2017-04-06 00:51:49.836633: step 68360, loss = (G: 8.68432140, D: 0.02321078) (0.224 sec/batch)\n",
      "2017-04-06 00:51:54.323025: step 68380, loss = (G: 7.58811378, D: 0.00546464) (0.224 sec/batch)\n",
      "2017-04-06 00:51:58.811747: step 68400, loss = (G: 6.74342346, D: 0.01089029) (0.228 sec/batch)\n",
      "2017-04-06 00:52:03.598698: step 68420, loss = (G: 6.00647306, D: 0.03097077) (0.225 sec/batch)\n",
      "2017-04-06 00:52:08.099856: step 68440, loss = (G: 7.82312775, D: 0.00344591) (0.223 sec/batch)\n",
      "2017-04-06 00:52:12.580686: step 68460, loss = (G: 10.69965649, D: 0.36329925) (0.224 sec/batch)\n",
      "2017-04-06 00:52:17.075577: step 68480, loss = (G: 9.07277393, D: 0.02458643) (0.225 sec/batch)\n",
      "2017-04-06 00:52:21.570222: step 68500, loss = (G: 5.07707882, D: 0.07788092) (0.225 sec/batch)\n",
      "2017-04-06 00:52:26.194887: step 68520, loss = (G: 8.42571735, D: 0.00299889) (0.224 sec/batch)\n",
      "2017-04-06 00:52:30.689070: step 68540, loss = (G: 5.97234678, D: 0.12746286) (0.224 sec/batch)\n",
      "2017-04-06 00:52:35.190988: step 68560, loss = (G: 15.96040630, D: 0.00050692) (0.224 sec/batch)\n",
      "2017-04-06 00:52:39.682273: step 68580, loss = (G: 9.84708405, D: 0.07833201) (0.226 sec/batch)\n",
      "2017-04-06 00:52:44.204846: step 68600, loss = (G: 8.24494362, D: 0.01865525) (0.230 sec/batch)\n",
      "2017-04-06 00:52:48.848548: step 68620, loss = (G: 7.32238102, D: 0.07851341) (0.223 sec/batch)\n",
      "2017-04-06 00:52:53.385775: step 68640, loss = (G: 9.44453049, D: 0.02587754) (0.226 sec/batch)\n",
      "2017-04-06 00:52:57.865546: step 68660, loss = (G: 7.23876762, D: 0.02562161) (0.224 sec/batch)\n",
      "2017-04-06 00:53:02.350462: step 68680, loss = (G: 6.01798677, D: 0.03428867) (0.224 sec/batch)\n",
      "2017-04-06 00:53:06.850285: step 68700, loss = (G: 5.79686165, D: 0.02139853) (0.225 sec/batch)\n",
      "2017-04-06 00:53:11.493899: step 68720, loss = (G: 9.65144920, D: 0.00133278) (0.224 sec/batch)\n",
      "2017-04-06 00:53:15.977689: step 68740, loss = (G: 7.50752115, D: 0.00936222) (0.225 sec/batch)\n",
      "2017-04-06 00:53:20.489895: step 68760, loss = (G: 4.91673613, D: 0.04239207) (0.229 sec/batch)\n",
      "2017-04-06 00:53:24.988559: step 68780, loss = (G: 11.06645489, D: 0.03261394) (0.224 sec/batch)\n",
      "2017-04-06 00:53:29.474169: step 68800, loss = (G: 7.90155649, D: 0.01489442) (0.225 sec/batch)\n",
      "2017-04-06 00:53:34.097245: step 68820, loss = (G: 3.37317395, D: 0.21484779) (0.223 sec/batch)\n",
      "2017-04-06 00:53:38.586552: step 68840, loss = (G: 7.83783627, D: 0.00674580) (0.224 sec/batch)\n",
      "2017-04-06 00:53:43.077401: step 68860, loss = (G: 8.09736252, D: 0.09667428) (0.224 sec/batch)\n",
      "2017-04-06 00:53:47.578718: step 68880, loss = (G: 11.50697041, D: 0.00114742) (0.224 sec/batch)\n",
      "2017-04-06 00:53:52.075752: step 68900, loss = (G: 7.28854942, D: 0.00724495) (0.224 sec/batch)\n",
      "2017-04-06 00:53:56.697347: step 68920, loss = (G: 5.72603512, D: 0.01757100) (0.225 sec/batch)\n",
      "2017-04-06 00:54:01.196372: step 68940, loss = (G: 8.38319874, D: 0.00299073) (0.225 sec/batch)\n",
      "2017-04-06 00:54:05.700114: step 68960, loss = (G: 6.31564760, D: 0.01562065) (0.229 sec/batch)\n",
      "2017-04-06 00:54:10.197159: step 68980, loss = (G: 7.82386398, D: 0.00402898) (0.223 sec/batch)\n",
      "2017-04-06 00:54:14.695605: step 69000, loss = (G: 6.98548126, D: 0.00993958) (0.225 sec/batch)\n",
      "2017-04-06 00:54:19.320028: step 69020, loss = (G: 4.88479328, D: 0.06574016) (0.224 sec/batch)\n",
      "2017-04-06 00:54:23.816657: step 69040, loss = (G: 6.93305016, D: 0.02379615) (0.224 sec/batch)\n",
      "2017-04-06 00:54:28.321605: step 69060, loss = (G: 10.75408554, D: 0.06928526) (0.224 sec/batch)\n",
      "2017-04-06 00:54:32.802740: step 69080, loss = (G: 11.85747528, D: 0.00028968) (0.224 sec/batch)\n",
      "2017-04-06 00:54:37.289297: step 69100, loss = (G: 8.36260033, D: 0.03834069) (0.225 sec/batch)\n",
      "2017-04-06 00:54:41.919890: step 69120, loss = (G: 12.39501667, D: 0.00572746) (0.226 sec/batch)\n",
      "2017-04-06 00:54:46.405771: step 69140, loss = (G: 7.50552654, D: 0.00885007) (0.224 sec/batch)\n",
      "2017-04-06 00:54:50.909615: step 69160, loss = (G: 8.52902412, D: 0.02837689) (0.227 sec/batch)\n",
      "2017-04-06 00:54:55.408715: step 69180, loss = (G: 4.83162403, D: 0.11960731) (0.225 sec/batch)\n",
      "2017-04-06 00:54:59.911211: step 69200, loss = (G: 8.02714443, D: 0.01372575) (0.225 sec/batch)\n",
      "2017-04-06 00:55:04.532857: step 69220, loss = (G: 9.46537971, D: 0.00331988) (0.224 sec/batch)\n",
      "2017-04-06 00:55:09.073673: step 69240, loss = (G: 5.56762218, D: 0.03445138) (0.226 sec/batch)\n",
      "2017-04-06 00:55:13.560415: step 69260, loss = (G: 4.56632566, D: 0.12224295) (0.225 sec/batch)\n",
      "2017-04-06 00:55:18.072574: step 69280, loss = (G: 8.40985680, D: 0.00530121) (0.224 sec/batch)\n",
      "2017-04-06 00:55:22.572170: step 69300, loss = (G: 8.64918995, D: 0.00406541) (0.225 sec/batch)\n",
      "2017-04-06 00:55:27.199393: step 69320, loss = (G: 9.93176651, D: 0.00383604) (0.224 sec/batch)\n",
      "2017-04-06 00:55:31.686036: step 69340, loss = (G: 9.75598907, D: 0.70039147) (0.224 sec/batch)\n",
      "2017-04-06 00:55:36.173172: step 69360, loss = (G: 11.67140865, D: 0.07608479) (0.224 sec/batch)\n",
      "2017-04-06 00:55:40.670656: step 69380, loss = (G: 5.45597124, D: 0.11161925) (0.225 sec/batch)\n",
      "2017-04-06 00:55:45.154794: step 69400, loss = (G: 9.55680084, D: 0.02929146) (0.224 sec/batch)\n",
      "2017-04-06 00:55:49.807984: step 69420, loss = (G: 8.34339809, D: 0.00312334) (0.225 sec/batch)\n",
      "2017-04-06 00:55:54.292673: step 69440, loss = (G: 10.31354141, D: 0.04907781) (0.226 sec/batch)\n",
      "2017-04-06 00:55:58.775868: step 69460, loss = (G: 11.94965553, D: 0.00673248) (0.224 sec/batch)\n",
      "2017-04-06 00:56:03.266498: step 69480, loss = (G: 7.64844751, D: 0.00563695) (0.225 sec/batch)\n",
      "2017-04-06 00:56:07.753992: step 69500, loss = (G: 14.97385788, D: 0.07375224) (0.226 sec/batch)\n",
      "2017-04-06 00:56:12.383831: step 69520, loss = (G: 9.05335140, D: 0.01845480) (0.224 sec/batch)\n",
      "2017-04-06 00:56:16.868659: step 69540, loss = (G: 5.38461685, D: 0.04091274) (0.224 sec/batch)\n",
      "2017-04-06 00:56:21.356600: step 69560, loss = (G: 6.14710045, D: 0.02027715) (0.224 sec/batch)\n",
      "2017-04-06 00:56:25.919945: step 69580, loss = (G: 8.44955349, D: 0.00434600) (0.222 sec/batch)\n",
      "2017-04-06 00:56:30.444201: step 69600, loss = (G: 8.25017738, D: 0.01116502) (0.224 sec/batch)\n",
      "2017-04-06 00:56:35.070431: step 69620, loss = (G: 8.22936821, D: 0.00293759) (0.224 sec/batch)\n",
      "2017-04-06 00:56:39.562120: step 69640, loss = (G: 7.66929770, D: 0.00761972) (0.225 sec/batch)\n",
      "2017-04-06 00:56:44.043977: step 69660, loss = (G: 5.73400497, D: 0.01763433) (0.224 sec/batch)\n",
      "2017-04-06 00:56:48.582842: step 69680, loss = (G: 4.98425865, D: 0.05228229) (0.224 sec/batch)\n",
      "2017-04-06 00:56:53.087726: step 69700, loss = (G: 8.21428585, D: 0.00448478) (0.223 sec/batch)\n",
      "2017-04-06 00:56:57.719355: step 69720, loss = (G: 6.51269197, D: 0.01551491) (0.224 sec/batch)\n",
      "2017-04-06 00:57:02.222061: step 69740, loss = (G: 6.90142345, D: 0.01688873) (0.223 sec/batch)\n",
      "2017-04-06 00:57:06.727935: step 69760, loss = (G: 10.05920982, D: 0.02771343) (0.225 sec/batch)\n",
      "2017-04-06 00:57:11.205601: step 69780, loss = (G: 10.65768909, D: 0.07693943) (0.223 sec/batch)\n",
      "2017-04-06 00:57:15.696336: step 69800, loss = (G: 10.96702576, D: 0.06300209) (0.223 sec/batch)\n",
      "2017-04-06 00:57:20.320098: step 69820, loss = (G: 8.26594543, D: 0.00223405) (0.225 sec/batch)\n",
      "2017-04-06 00:57:24.810138: step 69840, loss = (G: 11.78949738, D: 0.00875945) (0.224 sec/batch)\n",
      "2017-04-06 00:57:29.318901: step 69860, loss = (G: 7.35589743, D: 0.00904575) (0.225 sec/batch)\n",
      "2017-04-06 00:57:33.808121: step 69880, loss = (G: 19.39969635, D: 0.00703656) (0.228 sec/batch)\n",
      "2017-04-06 00:57:38.301821: step 69900, loss = (G: 5.69481945, D: 0.02407604) (0.226 sec/batch)\n",
      "2017-04-06 00:57:42.950000: step 69920, loss = (G: 7.97597504, D: 0.00956325) (0.224 sec/batch)\n",
      "2017-04-06 00:57:47.493114: step 69940, loss = (G: 6.57676458, D: 0.01804767) (0.224 sec/batch)\n",
      "2017-04-06 00:57:51.975506: step 69960, loss = (G: 7.33279991, D: 0.00716983) (0.224 sec/batch)\n",
      "2017-04-06 00:57:56.482886: step 69980, loss = (G: 8.75186443, D: 0.01340848) (0.224 sec/batch)\n",
      "2017-04-06 00:58:00.964668: step 70000, loss = (G: 8.15226936, D: 0.00534894) (0.225 sec/batch)\n",
      "2017-04-06 00:58:05.638942: step 70020, loss = (G: 6.22647524, D: 0.05357885) (0.226 sec/batch)\n",
      "2017-04-06 00:58:10.137940: step 70040, loss = (G: 5.03795433, D: 0.13305566) (0.224 sec/batch)\n",
      "2017-04-06 00:58:14.622899: step 70060, loss = (G: 6.88672638, D: 0.01205319) (0.224 sec/batch)\n",
      "2017-04-06 00:58:19.110998: step 70080, loss = (G: 7.31307697, D: 0.00918799) (0.224 sec/batch)\n",
      "2017-04-06 00:58:23.596465: step 70100, loss = (G: 7.21081686, D: 0.00746774) (0.224 sec/batch)\n",
      "2017-04-06 00:58:28.238789: step 70120, loss = (G: 9.59662056, D: 0.02654457) (0.224 sec/batch)\n",
      "2017-04-06 00:58:32.730635: step 70140, loss = (G: 7.69613028, D: 0.03247848) (0.224 sec/batch)\n",
      "2017-04-06 00:58:37.225760: step 70160, loss = (G: 8.74152756, D: 0.00256655) (0.224 sec/batch)\n",
      "2017-04-06 00:58:41.731481: step 70180, loss = (G: 7.19777822, D: 0.22973612) (0.223 sec/batch)\n",
      "2017-04-06 00:58:46.215135: step 70200, loss = (G: 9.07732010, D: 0.00171132) (0.225 sec/batch)\n",
      "2017-04-06 00:58:50.859569: step 70220, loss = (G: 15.33671665, D: 0.24963073) (0.225 sec/batch)\n",
      "2017-04-06 00:58:55.349388: step 70240, loss = (G: 4.53772163, D: 0.13757382) (0.224 sec/batch)\n",
      "2017-04-06 00:58:59.838130: step 70260, loss = (G: 6.66104317, D: 0.00852718) (0.225 sec/batch)\n",
      "2017-04-06 00:59:04.331323: step 70280, loss = (G: 7.27625656, D: 0.00468836) (0.225 sec/batch)\n",
      "2017-04-06 00:59:08.828316: step 70300, loss = (G: 6.66188192, D: 0.01468139) (0.225 sec/batch)\n",
      "2017-04-06 00:59:13.457009: step 70320, loss = (G: 10.54249573, D: 0.00254272) (0.224 sec/batch)\n",
      "2017-04-06 00:59:17.951792: step 70340, loss = (G: 10.42985821, D: 0.42359069) (0.225 sec/batch)\n",
      "2017-04-06 00:59:22.438945: step 70360, loss = (G: 6.52890968, D: 0.03103517) (0.225 sec/batch)\n",
      "2017-04-06 00:59:26.938383: step 70380, loss = (G: 9.84110737, D: 0.00139594) (0.226 sec/batch)\n",
      "2017-04-06 00:59:31.433459: step 70400, loss = (G: 7.76070118, D: 0.06458404) (0.225 sec/batch)\n",
      "2017-04-06 00:59:36.062215: step 70420, loss = (G: 6.39358664, D: 0.01356721) (0.224 sec/batch)\n",
      "2017-04-06 00:59:40.602375: step 70440, loss = (G: 6.45802212, D: 0.01979954) (0.224 sec/batch)\n",
      "2017-04-06 00:59:45.080211: step 70460, loss = (G: 13.61960888, D: 0.12737432) (0.226 sec/batch)\n",
      "2017-04-06 00:59:49.570472: step 70480, loss = (G: 5.90715408, D: 0.04678176) (0.225 sec/batch)\n",
      "2017-04-06 00:59:54.071023: step 70500, loss = (G: 10.46076870, D: 0.78991455) (0.225 sec/batch)\n",
      "2017-04-06 00:59:58.739938: step 70520, loss = (G: 9.60894680, D: 0.00314388) (0.224 sec/batch)\n",
      "2017-04-06 01:00:03.225948: step 70540, loss = (G: 12.40198612, D: 0.05178907) (0.230 sec/batch)\n",
      "2017-04-06 01:00:07.744345: step 70560, loss = (G: 6.66199875, D: 0.00968894) (0.224 sec/batch)\n",
      "2017-04-06 01:00:12.226798: step 70580, loss = (G: 6.55397177, D: 0.01049441) (0.228 sec/batch)\n",
      "2017-04-06 01:00:16.715769: step 70600, loss = (G: 9.68519497, D: 0.00122228) (0.224 sec/batch)\n",
      "2017-04-06 01:00:21.343749: step 70620, loss = (G: 18.96595192, D: 0.27922779) (0.226 sec/batch)\n",
      "2017-04-06 01:00:25.831291: step 70640, loss = (G: 13.17676640, D: 0.00122642) (0.224 sec/batch)\n",
      "2017-04-06 01:00:30.317036: step 70660, loss = (G: 7.09841156, D: 0.01045956) (0.224 sec/batch)\n",
      "2017-04-06 01:00:34.809045: step 70680, loss = (G: 4.58880138, D: 0.04925048) (0.225 sec/batch)\n",
      "2017-04-06 01:00:39.297055: step 70700, loss = (G: 6.28855038, D: 0.01454128) (0.224 sec/batch)\n",
      "2017-04-06 01:00:43.922407: step 70720, loss = (G: 8.56523228, D: 0.00392012) (0.224 sec/batch)\n",
      "2017-04-06 01:00:48.549204: step 70740, loss = (G: 5.43656635, D: 0.02653763) (0.225 sec/batch)\n",
      "2017-04-06 01:00:53.051276: step 70760, loss = (G: 6.74361467, D: 0.01132049) (0.227 sec/batch)\n",
      "2017-04-06 01:00:57.556095: step 70780, loss = (G: 12.38459301, D: 0.03222981) (0.228 sec/batch)\n",
      "2017-04-06 01:01:02.051032: step 70800, loss = (G: 7.82336712, D: 0.00380523) (0.225 sec/batch)\n",
      "2017-04-06 01:01:06.687731: step 70820, loss = (G: 8.18908405, D: 0.00591245) (0.224 sec/batch)\n",
      "2017-04-06 01:01:11.168425: step 70840, loss = (G: 6.27790070, D: 0.11063175) (0.224 sec/batch)\n",
      "2017-04-06 01:01:15.653419: step 70860, loss = (G: 7.26235294, D: 0.04605151) (0.225 sec/batch)\n",
      "2017-04-06 01:01:20.142622: step 70880, loss = (G: 8.10966778, D: 0.11925641) (0.224 sec/batch)\n",
      "2017-04-06 01:01:24.640988: step 70900, loss = (G: 9.40846348, D: 0.00095477) (0.223 sec/batch)\n",
      "2017-04-06 01:01:29.278114: step 70920, loss = (G: 6.13487101, D: 0.01652323) (0.231 sec/batch)\n",
      "2017-04-06 01:01:33.812819: step 70940, loss = (G: 6.87194920, D: 0.03903546) (0.225 sec/batch)\n",
      "2017-04-06 01:01:38.294256: step 70960, loss = (G: 7.66803694, D: 0.00446324) (0.225 sec/batch)\n",
      "2017-04-06 01:01:42.778653: step 70980, loss = (G: 8.60081863, D: 0.14656726) (0.225 sec/batch)\n",
      "2017-04-06 01:01:47.270054: step 71000, loss = (G: 8.05111980, D: 0.01094276) (0.226 sec/batch)\n",
      "2017-04-06 01:01:51.894189: step 71020, loss = (G: 8.11457729, D: 0.00767912) (0.224 sec/batch)\n",
      "2017-04-06 01:01:56.394259: step 71040, loss = (G: 4.60877514, D: 0.09002884) (0.225 sec/batch)\n",
      "2017-04-06 01:02:00.891882: step 71060, loss = (G: 6.84816933, D: 0.03562882) (0.224 sec/batch)\n",
      "2017-04-06 01:02:05.397016: step 71080, loss = (G: 6.51283503, D: 0.08425637) (0.225 sec/batch)\n",
      "2017-04-06 01:02:09.898035: step 71100, loss = (G: 8.74443340, D: 0.00310625) (0.226 sec/batch)\n",
      "2017-04-06 01:02:14.535779: step 71120, loss = (G: 2.66225290, D: 1.49118412) (0.225 sec/batch)\n",
      "2017-04-06 01:02:19.031242: step 71140, loss = (G: 6.16325045, D: 0.02167607) (0.224 sec/batch)\n",
      "2017-04-06 01:02:23.520429: step 71160, loss = (G: 7.16842365, D: 0.00536596) (0.224 sec/batch)\n",
      "2017-04-06 01:02:28.026381: step 71180, loss = (G: 9.38740540, D: 0.08843620) (0.224 sec/batch)\n",
      "2017-04-06 01:02:32.546306: step 71200, loss = (G: 6.44484186, D: 0.01258921) (0.225 sec/batch)\n",
      "2017-04-06 01:02:37.177214: step 71220, loss = (G: 8.10939789, D: 0.00738879) (0.224 sec/batch)\n",
      "2017-04-06 01:02:41.670332: step 71240, loss = (G: 5.13011408, D: 0.04167815) (0.227 sec/batch)\n",
      "2017-04-06 01:02:46.166968: step 71260, loss = (G: 7.42272568, D: 0.00706545) (0.224 sec/batch)\n",
      "2017-04-06 01:02:50.653678: step 71280, loss = (G: 9.79222679, D: 0.02116954) (0.224 sec/batch)\n",
      "2017-04-06 01:02:55.152870: step 71300, loss = (G: 9.63872910, D: 0.15797769) (0.224 sec/batch)\n",
      "2017-04-06 01:02:59.793524: step 71320, loss = (G: 12.07912731, D: 0.43972540) (0.225 sec/batch)\n",
      "2017-04-06 01:03:04.285112: step 71340, loss = (G: 6.74272346, D: 0.01385626) (0.223 sec/batch)\n",
      "2017-04-06 01:03:08.793042: step 71360, loss = (G: 7.67991257, D: 0.00506171) (0.226 sec/batch)\n",
      "2017-04-06 01:03:13.305733: step 71380, loss = (G: 11.83114815, D: 0.00014568) (0.224 sec/batch)\n",
      "2017-04-06 01:03:17.802925: step 71400, loss = (G: 11.00748920, D: 0.00024850) (0.224 sec/batch)\n",
      "2017-04-06 01:03:22.433459: step 71420, loss = (G: 6.36514664, D: 0.01692270) (0.224 sec/batch)\n",
      "2017-04-06 01:03:26.916412: step 71440, loss = (G: 23.17932320, D: 2.92551136) (0.226 sec/batch)\n",
      "2017-04-06 01:03:31.417247: step 71460, loss = (G: 16.48402405, D: 0.01253073) (0.224 sec/batch)\n",
      "2017-04-06 01:03:35.909708: step 71480, loss = (G: 8.49179173, D: 0.00301267) (0.224 sec/batch)\n",
      "2017-04-06 01:03:40.394066: step 71500, loss = (G: 4.42413473, D: 0.11175605) (0.224 sec/batch)\n",
      "2017-04-06 01:03:45.020273: step 71520, loss = (G: 6.44434166, D: 0.01509954) (0.223 sec/batch)\n",
      "2017-04-06 01:03:49.516358: step 71540, loss = (G: 7.44711733, D: 0.16292407) (0.225 sec/batch)\n",
      "2017-04-06 01:03:54.015576: step 71560, loss = (G: 7.35357332, D: 0.00676702) (0.227 sec/batch)\n",
      "2017-04-06 01:03:58.507703: step 71580, loss = (G: 5.69862843, D: 0.02275171) (0.229 sec/batch)\n",
      "2017-04-06 01:04:03.015944: step 71600, loss = (G: 8.16491413, D: 0.04042130) (0.225 sec/batch)\n",
      "2017-04-06 01:04:07.650872: step 71620, loss = (G: 12.89209080, D: 0.00043509) (0.226 sec/batch)\n",
      "2017-04-06 01:04:12.144617: step 71640, loss = (G: 5.40523815, D: 0.05624298) (0.225 sec/batch)\n",
      "2017-04-06 01:04:16.654690: step 71660, loss = (G: 9.03057003, D: 0.00348839) (0.223 sec/batch)\n",
      "2017-04-06 01:04:21.148656: step 71680, loss = (G: 5.93100929, D: 0.03995541) (0.224 sec/batch)\n",
      "2017-04-06 01:04:25.638870: step 71700, loss = (G: 5.94682264, D: 0.02484811) (0.224 sec/batch)\n",
      "2017-04-06 01:04:30.260347: step 71720, loss = (G: 4.13673592, D: 0.15241955) (0.224 sec/batch)\n",
      "2017-04-06 01:04:34.780181: step 71740, loss = (G: 10.30667496, D: 0.00442647) (0.230 sec/batch)\n",
      "2017-04-06 01:04:39.285716: step 71760, loss = (G: 3.25496960, D: 0.15420063) (0.224 sec/batch)\n",
      "2017-04-06 01:04:43.769453: step 71780, loss = (G: 8.05663872, D: 0.04110794) (0.224 sec/batch)\n",
      "2017-04-06 01:04:48.263866: step 71800, loss = (G: 7.79779291, D: 0.00761904) (0.224 sec/batch)\n",
      "2017-04-06 01:04:52.893177: step 71820, loss = (G: 13.78343391, D: 0.08217485) (0.225 sec/batch)\n",
      "2017-04-06 01:04:57.381619: step 71840, loss = (G: 17.71573257, D: 0.47634274) (0.224 sec/batch)\n",
      "2017-04-06 01:05:01.868903: step 71860, loss = (G: 9.68406868, D: 0.00254579) (0.224 sec/batch)\n",
      "2017-04-06 01:05:06.356456: step 71880, loss = (G: 8.49348068, D: 0.00411498) (0.226 sec/batch)\n",
      "2017-04-06 01:05:10.918648: step 71900, loss = (G: 5.07917500, D: 0.03904309) (0.223 sec/batch)\n",
      "2017-04-06 01:05:15.558832: step 71920, loss = (G: 9.72656727, D: 0.01610953) (0.228 sec/batch)\n",
      "2017-04-06 01:05:20.050367: step 71940, loss = (G: 7.78409863, D: 0.04419792) (0.224 sec/batch)\n",
      "2017-04-06 01:05:24.542788: step 71960, loss = (G: 7.30149412, D: 0.00510158) (0.225 sec/batch)\n",
      "2017-04-06 01:05:29.038729: step 71980, loss = (G: 8.79814816, D: 0.00299624) (0.228 sec/batch)\n",
      "2017-04-06 01:05:33.524397: step 72000, loss = (G: 4.33271933, D: 0.09422943) (0.223 sec/batch)\n",
      "2017-04-06 01:05:38.171713: step 72020, loss = (G: 5.68656349, D: 0.02821919) (0.224 sec/batch)\n",
      "2017-04-06 01:05:42.661770: step 72040, loss = (G: 5.76457071, D: 0.02709091) (0.224 sec/batch)\n",
      "2017-04-06 01:05:47.153344: step 72060, loss = (G: 6.66404629, D: 0.08028245) (0.226 sec/batch)\n",
      "2017-04-06 01:05:51.654185: step 72080, loss = (G: 7.40461540, D: 0.03035470) (0.226 sec/batch)\n",
      "2017-04-06 01:05:56.147884: step 72100, loss = (G: 9.71278477, D: 0.00202005) (0.224 sec/batch)\n",
      "2017-04-06 01:06:00.774013: step 72120, loss = (G: 5.89806557, D: 0.04244304) (0.225 sec/batch)\n",
      "2017-04-06 01:06:05.274935: step 72140, loss = (G: 11.67116165, D: 0.41080251) (0.225 sec/batch)\n",
      "2017-04-06 01:06:09.787755: step 72160, loss = (G: 7.04034376, D: 0.01416862) (0.224 sec/batch)\n",
      "2017-04-06 01:06:14.284396: step 72180, loss = (G: 7.35352850, D: 0.01253890) (0.224 sec/batch)\n",
      "2017-04-06 01:06:18.806218: step 72200, loss = (G: 7.09716606, D: 0.00825100) (0.228 sec/batch)\n",
      "2017-04-06 01:06:23.434246: step 72220, loss = (G: 9.46494389, D: 0.00153046) (0.225 sec/batch)\n",
      "2017-04-06 01:06:27.922545: step 72240, loss = (G: 6.69698620, D: 0.00657013) (0.226 sec/batch)\n",
      "2017-04-06 01:06:32.413390: step 72260, loss = (G: 8.88877869, D: 0.00409341) (0.224 sec/batch)\n",
      "2017-04-06 01:06:36.895881: step 72280, loss = (G: 6.54146862, D: 0.00750884) (0.224 sec/batch)\n",
      "2017-04-06 01:06:41.384058: step 72300, loss = (G: 9.37146473, D: 0.04506992) (0.224 sec/batch)\n",
      "2017-04-06 01:06:46.027924: step 72320, loss = (G: 7.89451790, D: 0.00466011) (0.224 sec/batch)\n",
      "2017-04-06 01:06:50.516529: step 72340, loss = (G: 5.86098289, D: 0.06528565) (0.224 sec/batch)\n",
      "2017-04-06 01:06:55.010287: step 72360, loss = (G: 6.68315506, D: 0.01026780) (0.225 sec/batch)\n",
      "2017-04-06 01:06:59.539004: step 72380, loss = (G: 10.56086254, D: 0.00235725) (0.230 sec/batch)\n",
      "2017-04-06 01:07:04.028282: step 72400, loss = (G: 11.46758270, D: 0.00224091) (0.224 sec/batch)\n",
      "2017-04-06 01:07:08.660740: step 72420, loss = (G: 7.77122450, D: 0.01217428) (0.223 sec/batch)\n",
      "2017-04-06 01:07:13.154160: step 72440, loss = (G: 10.44028378, D: 0.01003379) (0.224 sec/batch)\n",
      "2017-04-06 01:07:17.652363: step 72460, loss = (G: 9.12883568, D: 0.00911147) (0.224 sec/batch)\n",
      "2017-04-06 01:07:22.146672: step 72480, loss = (G: 6.51802349, D: 0.00646874) (0.225 sec/batch)\n",
      "2017-04-06 01:07:26.640117: step 72500, loss = (G: 10.26338768, D: 0.07627160) (0.225 sec/batch)\n",
      "2017-04-06 01:07:31.272396: step 72520, loss = (G: 3.63333702, D: 0.27076757) (0.226 sec/batch)\n",
      "2017-04-06 01:07:35.766567: step 72540, loss = (G: 6.06595612, D: 0.04509545) (0.227 sec/batch)\n",
      "2017-04-06 01:07:40.263147: step 72560, loss = (G: 6.63599968, D: 0.00718442) (0.226 sec/batch)\n",
      "2017-04-06 01:07:44.760955: step 72580, loss = (G: 6.13588142, D: 0.03677360) (0.225 sec/batch)\n",
      "2017-04-06 01:07:49.256137: step 72600, loss = (G: 7.78903151, D: 0.05674599) (0.224 sec/batch)\n",
      "2017-04-06 01:07:53.890171: step 72620, loss = (G: 8.20020771, D: 0.00163619) (0.224 sec/batch)\n",
      "2017-04-06 01:07:58.407161: step 72640, loss = (G: 9.89347935, D: 0.02313527) (0.224 sec/batch)\n",
      "2017-04-06 01:08:02.896596: step 72660, loss = (G: 4.57169008, D: 0.14791332) (0.225 sec/batch)\n",
      "2017-04-06 01:08:07.391881: step 72680, loss = (G: 9.88808346, D: 0.00344631) (0.225 sec/batch)\n",
      "2017-04-06 01:08:11.903953: step 72700, loss = (G: 5.99945164, D: 0.02778653) (0.224 sec/batch)\n",
      "2017-04-06 01:08:16.582130: step 72720, loss = (G: 8.38652420, D: 0.00308239) (0.228 sec/batch)\n",
      "2017-04-06 01:08:21.074649: step 72740, loss = (G: 12.04042053, D: 0.00117303) (0.227 sec/batch)\n",
      "2017-04-06 01:08:25.568992: step 72760, loss = (G: 8.03929329, D: 0.00467857) (0.223 sec/batch)\n",
      "2017-04-06 01:08:30.079917: step 72780, loss = (G: 4.93909454, D: 0.04528677) (0.226 sec/batch)\n",
      "2017-04-06 01:08:34.576172: step 72800, loss = (G: 6.40314341, D: 0.01396232) (0.229 sec/batch)\n",
      "2017-04-06 01:08:39.210076: step 72820, loss = (G: 22.66608047, D: 0.13395712) (0.231 sec/batch)\n",
      "2017-04-06 01:08:43.726432: step 72840, loss = (G: 5.86158514, D: 0.02286033) (0.223 sec/batch)\n",
      "2017-04-06 01:08:48.212621: step 72860, loss = (G: 4.79079103, D: 0.04855553) (0.224 sec/batch)\n",
      "2017-04-06 01:08:52.705585: step 72880, loss = (G: 7.09717178, D: 0.01965907) (0.225 sec/batch)\n",
      "2017-04-06 01:08:57.192662: step 72900, loss = (G: 7.17584515, D: 0.01865877) (0.224 sec/batch)\n",
      "2017-04-06 01:09:01.823383: step 72920, loss = (G: 7.10720348, D: 0.04611070) (0.225 sec/batch)\n",
      "2017-04-06 01:09:06.365691: step 72940, loss = (G: 4.78538132, D: 0.10035906) (0.225 sec/batch)\n",
      "2017-04-06 01:09:10.905230: step 72960, loss = (G: 4.83567858, D: 0.16722262) (0.225 sec/batch)\n",
      "2017-04-06 01:09:15.387383: step 72980, loss = (G: 8.11059093, D: 0.00602382) (0.224 sec/batch)\n",
      "2017-04-06 01:09:19.873685: step 73000, loss = (G: 6.62058449, D: 0.01575148) (0.223 sec/batch)\n",
      "2017-04-06 01:09:24.500187: step 73020, loss = (G: 9.73277092, D: 0.02584407) (0.223 sec/batch)\n",
      "2017-04-06 01:09:28.980233: step 73040, loss = (G: 6.70929003, D: 0.01169311) (0.225 sec/batch)\n",
      "2017-04-06 01:09:33.597572: step 73060, loss = (G: 7.43619347, D: 0.01259781) (0.224 sec/batch)\n",
      "2017-04-06 01:09:38.075672: step 73080, loss = (G: 9.07584667, D: 0.01598090) (0.223 sec/batch)\n",
      "2017-04-06 01:09:42.566481: step 73100, loss = (G: 6.29578352, D: 0.01638616) (0.225 sec/batch)\n",
      "2017-04-06 01:09:47.196091: step 73120, loss = (G: 14.07008743, D: 0.01504693) (0.225 sec/batch)\n",
      "2017-04-06 01:09:51.679442: step 73140, loss = (G: 8.75888252, D: 0.00194881) (0.225 sec/batch)\n",
      "2017-04-06 01:09:56.174906: step 73160, loss = (G: 7.25951195, D: 0.10051890) (0.224 sec/batch)\n",
      "2017-04-06 01:10:00.712177: step 73180, loss = (G: 4.53556204, D: 0.07383698) (0.228 sec/batch)\n",
      "2017-04-06 01:10:05.198436: step 73200, loss = (G: 4.01096296, D: 0.15512396) (0.225 sec/batch)\n",
      "2017-04-06 01:10:09.834296: step 73220, loss = (G: 6.06227589, D: 0.02231576) (0.224 sec/batch)\n",
      "2017-04-06 01:10:14.315935: step 73240, loss = (G: 10.76219082, D: 0.00307170) (0.223 sec/batch)\n",
      "2017-04-06 01:10:18.839980: step 73260, loss = (G: 5.63352346, D: 0.02483174) (0.224 sec/batch)\n",
      "2017-04-06 01:10:23.319205: step 73280, loss = (G: 10.55675983, D: 0.02429719) (0.224 sec/batch)\n",
      "2017-04-06 01:10:27.825709: step 73300, loss = (G: 8.93114948, D: 0.01903859) (0.224 sec/batch)\n",
      "2017-04-06 01:10:32.459862: step 73320, loss = (G: 13.53297520, D: 0.01793497) (0.226 sec/batch)\n",
      "2017-04-06 01:10:36.953445: step 73340, loss = (G: 7.84854460, D: 0.00831846) (0.225 sec/batch)\n",
      "2017-04-06 01:10:41.439952: step 73360, loss = (G: 4.64266491, D: 0.14856824) (0.224 sec/batch)\n",
      "2017-04-06 01:10:45.923450: step 73380, loss = (G: 5.34975529, D: 0.05269043) (0.224 sec/batch)\n",
      "2017-04-06 01:10:50.440261: step 73400, loss = (G: 6.40557814, D: 0.01063707) (0.223 sec/batch)\n",
      "2017-04-06 01:10:55.062460: step 73420, loss = (G: 7.28504801, D: 0.01211990) (0.223 sec/batch)\n",
      "2017-04-06 01:10:59.594965: step 73440, loss = (G: 7.93685150, D: 0.05181145) (0.228 sec/batch)\n",
      "2017-04-06 01:11:04.092546: step 73460, loss = (G: 6.27867651, D: 0.01281494) (0.225 sec/batch)\n",
      "2017-04-06 01:11:08.585916: step 73480, loss = (G: 8.06128025, D: 0.00716269) (0.226 sec/batch)\n",
      "2017-04-06 01:11:13.071257: step 73500, loss = (G: 9.49186802, D: 0.00839207) (0.225 sec/batch)\n",
      "2017-04-06 01:11:17.702230: step 73520, loss = (G: 9.04095459, D: 0.00456405) (0.231 sec/batch)\n",
      "2017-04-06 01:11:22.229671: step 73540, loss = (G: 9.60646725, D: 0.00166030) (0.225 sec/batch)\n",
      "2017-04-06 01:11:26.762384: step 73560, loss = (G: 5.58228636, D: 0.03049484) (0.226 sec/batch)\n",
      "2017-04-06 01:11:31.240577: step 73580, loss = (G: 5.24309063, D: 0.06504834) (0.224 sec/batch)\n",
      "2017-04-06 01:11:35.745602: step 73600, loss = (G: 6.83332253, D: 0.02563843) (0.223 sec/batch)\n",
      "2017-04-06 01:11:40.365824: step 73620, loss = (G: 4.87728930, D: 1.68239248) (0.224 sec/batch)\n",
      "2017-04-06 01:11:44.848163: step 73640, loss = (G: 8.46212387, D: 0.00379294) (0.226 sec/batch)\n",
      "2017-04-06 01:11:49.341975: step 73660, loss = (G: 5.01410484, D: 0.04768039) (0.228 sec/batch)\n",
      "2017-04-06 01:11:53.832746: step 73680, loss = (G: 6.69683743, D: 0.01146991) (0.225 sec/batch)\n",
      "2017-04-06 01:11:58.319313: step 73700, loss = (G: 8.60105705, D: 0.00166429) (0.225 sec/batch)\n",
      "2017-04-06 01:12:02.944281: step 73720, loss = (G: 5.42739916, D: 0.04269569) (0.223 sec/batch)\n",
      "2017-04-06 01:12:07.444550: step 73740, loss = (G: 8.72120285, D: 0.00256203) (0.225 sec/batch)\n",
      "2017-04-06 01:12:11.952595: step 73760, loss = (G: 7.43596554, D: 0.00949788) (0.225 sec/batch)\n",
      "2017-04-06 01:12:16.436755: step 73780, loss = (G: 5.99825621, D: 0.02281989) (0.224 sec/batch)\n",
      "2017-04-06 01:12:20.930720: step 73800, loss = (G: 10.83142853, D: 0.01226384) (0.225 sec/batch)\n",
      "2017-04-06 01:12:25.574326: step 73820, loss = (G: 3.75977993, D: 0.11763209) (0.225 sec/batch)\n",
      "2017-04-06 01:12:30.071913: step 73840, loss = (G: 6.59727621, D: 0.00825449) (0.224 sec/batch)\n",
      "2017-04-06 01:12:34.556023: step 73860, loss = (G: 4.61581945, D: 0.14228047) (0.224 sec/batch)\n",
      "2017-04-06 01:12:39.070611: step 73880, loss = (G: 7.54849958, D: 0.00849142) (0.224 sec/batch)\n",
      "2017-04-06 01:12:43.571913: step 73900, loss = (G: 5.90869761, D: 0.02306694) (0.231 sec/batch)\n",
      "2017-04-06 01:12:48.217459: step 73920, loss = (G: 6.22416878, D: 0.01220625) (0.223 sec/batch)\n",
      "2017-04-06 01:12:52.733519: step 73940, loss = (G: 8.33260345, D: 0.00409891) (0.226 sec/batch)\n",
      "2017-04-06 01:12:57.266928: step 73960, loss = (G: 2.73414874, D: 0.33878842) (0.226 sec/batch)\n",
      "2017-04-06 01:13:01.764801: step 73980, loss = (G: 8.03616333, D: 0.00777486) (0.230 sec/batch)\n",
      "2017-04-06 01:13:06.257021: step 74000, loss = (G: 6.03125238, D: 0.01746473) (0.224 sec/batch)\n",
      "2017-04-06 01:13:10.876705: step 74020, loss = (G: 8.52570724, D: 0.03262387) (0.224 sec/batch)\n",
      "2017-04-06 01:13:15.369804: step 74040, loss = (G: 5.23062992, D: 0.07311346) (0.226 sec/batch)\n",
      "2017-04-06 01:13:19.865225: step 74060, loss = (G: 9.36793995, D: 0.00217442) (0.223 sec/batch)\n",
      "2017-04-06 01:13:24.358475: step 74080, loss = (G: 11.33123493, D: 0.01243393) (0.224 sec/batch)\n",
      "2017-04-06 01:13:28.847028: step 74100, loss = (G: 3.58815193, D: 0.22792770) (0.226 sec/batch)\n",
      "2017-04-06 01:13:33.484670: step 74120, loss = (G: 4.91390991, D: 0.04951035) (0.225 sec/batch)\n",
      "2017-04-06 01:13:37.969987: step 74140, loss = (G: 6.65824032, D: 0.02229057) (0.225 sec/batch)\n",
      "2017-04-06 01:13:42.461388: step 74160, loss = (G: 7.87707186, D: 0.00466990) (0.224 sec/batch)\n",
      "2017-04-06 01:13:46.946782: step 74180, loss = (G: 7.81886196, D: 0.00927984) (0.224 sec/batch)\n",
      "2017-04-06 01:13:51.449587: step 74200, loss = (G: 7.20605326, D: 0.00710910) (0.226 sec/batch)\n",
      "2017-04-06 01:13:56.152582: step 74220, loss = (G: 7.89089394, D: 0.00492930) (0.225 sec/batch)\n",
      "2017-04-06 01:14:00.637867: step 74240, loss = (G: 12.36816406, D: 0.01357700) (0.224 sec/batch)\n",
      "2017-04-06 01:14:05.175749: step 74260, loss = (G: 8.62433147, D: 0.00300753) (0.224 sec/batch)\n",
      "2017-04-06 01:14:09.676797: step 74280, loss = (G: 6.87055349, D: 0.01322439) (0.225 sec/batch)\n",
      "2017-04-06 01:14:14.164206: step 74300, loss = (G: 9.96737099, D: 0.00132594) (0.224 sec/batch)\n",
      "2017-04-06 01:14:18.801479: step 74320, loss = (G: 9.70079517, D: 0.00241404) (0.226 sec/batch)\n",
      "2017-04-06 01:14:23.290816: step 74340, loss = (G: 9.45814133, D: 0.00163856) (0.225 sec/batch)\n",
      "2017-04-06 01:14:27.783151: step 74360, loss = (G: 8.11814117, D: 0.00209399) (0.224 sec/batch)\n",
      "2017-04-06 01:14:32.269887: step 74380, loss = (G: 6.72985935, D: 0.00795116) (0.225 sec/batch)\n",
      "2017-04-06 01:14:36.760974: step 74400, loss = (G: 3.34376049, D: 0.27176067) (0.223 sec/batch)\n",
      "2017-04-06 01:14:41.392471: step 74420, loss = (G: 9.06550026, D: 0.00546565) (0.224 sec/batch)\n",
      "2017-04-06 01:14:45.882200: step 74440, loss = (G: 6.98250246, D: 0.00975502) (0.225 sec/batch)\n",
      "2017-04-06 01:14:50.381713: step 74460, loss = (G: 9.55842972, D: 0.00069332) (0.224 sec/batch)\n",
      "2017-04-06 01:14:54.870625: step 74480, loss = (G: 6.91679192, D: 0.06755108) (0.224 sec/batch)\n",
      "2017-04-06 01:14:59.377763: step 74500, loss = (G: 5.52404165, D: 0.02535449) (0.228 sec/batch)\n",
      "2017-04-06 01:15:04.027179: step 74520, loss = (G: 8.36583710, D: 0.00306615) (0.226 sec/batch)\n",
      "2017-04-06 01:15:08.524474: step 74540, loss = (G: 6.34801722, D: 0.01687977) (0.224 sec/batch)\n",
      "2017-04-06 01:15:13.006023: step 74560, loss = (G: 7.60803413, D: 0.06092965) (0.224 sec/batch)\n",
      "2017-04-06 01:15:17.487506: step 74580, loss = (G: 8.73603153, D: 0.00301761) (0.224 sec/batch)\n",
      "2017-04-06 01:15:21.983940: step 74600, loss = (G: 6.95535040, D: 0.01023360) (0.225 sec/batch)\n",
      "2017-04-06 01:15:26.611018: step 74620, loss = (G: 9.28606224, D: 0.00294032) (0.224 sec/batch)\n",
      "2017-04-06 01:15:31.101587: step 74640, loss = (G: 7.51150560, D: 0.03114687) (0.225 sec/batch)\n",
      "2017-04-06 01:15:35.593530: step 74660, loss = (G: 10.32955456, D: 0.00054999) (0.224 sec/batch)\n",
      "2017-04-06 01:15:40.072191: step 74680, loss = (G: 9.35100365, D: 0.00296199) (0.224 sec/batch)\n",
      "2017-04-06 01:15:44.568909: step 74700, loss = (G: 7.91489124, D: 0.00325880) (0.225 sec/batch)\n",
      "2017-04-06 01:15:49.207002: step 74720, loss = (G: 7.74944496, D: 0.00548743) (0.225 sec/batch)\n",
      "2017-04-06 01:15:53.706627: step 74740, loss = (G: 3.13631058, D: 0.61378014) (0.225 sec/batch)\n",
      "2017-04-06 01:15:58.195075: step 74760, loss = (G: 14.67528725, D: 0.02394593) (0.224 sec/batch)\n",
      "2017-04-06 01:16:02.691168: step 74780, loss = (G: 6.18259907, D: 0.02690959) (0.229 sec/batch)\n",
      "2017-04-06 01:16:07.191066: step 74800, loss = (G: 4.65361023, D: 0.06210339) (0.225 sec/batch)\n",
      "2017-04-06 01:16:11.817471: step 74820, loss = (G: 8.85268593, D: 0.03105613) (0.224 sec/batch)\n",
      "2017-04-06 01:16:16.302456: step 74840, loss = (G: 9.66677475, D: 0.00261501) (0.224 sec/batch)\n",
      "2017-04-06 01:16:20.795843: step 74860, loss = (G: 5.11258793, D: 0.03107852) (0.226 sec/batch)\n",
      "2017-04-06 01:16:25.329352: step 74880, loss = (G: 16.25148773, D: 0.00020037) (0.225 sec/batch)\n",
      "2017-04-06 01:16:29.835204: step 74900, loss = (G: 9.90062237, D: 0.06764549) (0.225 sec/batch)\n",
      "2017-04-06 01:16:34.454181: step 74920, loss = (G: 5.02705097, D: 0.08895520) (0.224 sec/batch)\n",
      "2017-04-06 01:16:38.946371: step 74940, loss = (G: 12.84397984, D: 0.00236770) (0.225 sec/batch)\n",
      "2017-04-06 01:16:43.432943: step 74960, loss = (G: 5.65945005, D: 0.02655167) (0.224 sec/batch)\n",
      "2017-04-06 01:16:47.920170: step 74980, loss = (G: 16.32356071, D: 0.06520357) (0.224 sec/batch)\n",
      "2017-04-06 01:16:52.414309: step 75000, loss = (G: 5.79530764, D: 0.01504040) (0.224 sec/batch)\n",
      "2017-04-06 01:16:57.046739: step 75020, loss = (G: 9.80424690, D: 0.10730902) (0.225 sec/batch)\n",
      "2017-04-06 01:17:01.535148: step 75040, loss = (G: 5.72276688, D: 0.03947189) (0.229 sec/batch)\n",
      "2017-04-06 01:17:06.034374: step 75060, loss = (G: 6.10753059, D: 0.02554664) (0.225 sec/batch)\n",
      "2017-04-06 01:17:10.520798: step 75080, loss = (G: 8.11687279, D: 0.00490184) (0.225 sec/batch)\n",
      "2017-04-06 01:17:15.009594: step 75100, loss = (G: 4.61526680, D: 0.08149795) (0.224 sec/batch)\n",
      "2017-04-06 01:17:19.637407: step 75120, loss = (G: 7.36067390, D: 0.01464783) (0.227 sec/batch)\n",
      "2017-04-06 01:17:24.142249: step 75140, loss = (G: 9.37745285, D: 0.01191348) (0.224 sec/batch)\n",
      "2017-04-06 01:17:28.639539: step 75160, loss = (G: 6.94282293, D: 0.00836393) (0.225 sec/batch)\n",
      "2017-04-06 01:17:33.145268: step 75180, loss = (G: 5.68112278, D: 0.11716028) (0.225 sec/batch)\n",
      "2017-04-06 01:17:37.632773: step 75200, loss = (G: 12.61281776, D: 0.00118763) (0.224 sec/batch)\n",
      "2017-04-06 01:17:42.266123: step 75220, loss = (G: 9.97833443, D: 0.00702352) (0.226 sec/batch)\n",
      "2017-04-06 01:17:46.789731: step 75240, loss = (G: 8.61198330, D: 0.17168543) (0.226 sec/batch)\n",
      "2017-04-06 01:17:51.289745: step 75260, loss = (G: 5.73819065, D: 0.03298038) (0.227 sec/batch)\n",
      "2017-04-06 01:17:55.823319: step 75280, loss = (G: 8.19295692, D: 0.00306183) (0.226 sec/batch)\n",
      "2017-04-06 01:18:00.302590: step 75300, loss = (G: 8.23097038, D: 0.00493276) (0.225 sec/batch)\n",
      "2017-04-06 01:18:04.925423: step 75320, loss = (G: 10.24742222, D: 0.00592607) (0.226 sec/batch)\n",
      "2017-04-06 01:18:09.425352: step 75340, loss = (G: 4.66769886, D: 0.06085438) (0.224 sec/batch)\n",
      "2017-04-06 01:18:13.906064: step 75360, loss = (G: 7.76567554, D: 0.00472697) (0.224 sec/batch)\n",
      "2017-04-06 01:18:18.578178: step 75380, loss = (G: 7.65887022, D: 0.01656126) (0.225 sec/batch)\n",
      "2017-04-06 01:18:23.066284: step 75400, loss = (G: 9.79811668, D: 0.00582728) (0.225 sec/batch)\n",
      "2017-04-06 01:18:27.699727: step 75420, loss = (G: 6.86057901, D: 0.00922464) (0.225 sec/batch)\n",
      "2017-04-06 01:18:32.190201: step 75440, loss = (G: 10.06016541, D: 0.42366374) (0.224 sec/batch)\n",
      "2017-04-06 01:18:36.681508: step 75460, loss = (G: 9.72076797, D: 0.00379887) (0.224 sec/batch)\n",
      "2017-04-06 01:18:41.177920: step 75480, loss = (G: 9.09265518, D: 0.00456960) (0.224 sec/batch)\n",
      "2017-04-06 01:18:45.663711: step 75500, loss = (G: 8.39956665, D: 0.00453128) (0.224 sec/batch)\n",
      "2017-04-06 01:18:50.291645: step 75520, loss = (G: 7.22818708, D: 0.00446003) (0.225 sec/batch)\n",
      "2017-04-06 01:18:54.790562: step 75540, loss = (G: 9.16628075, D: 0.00102818) (0.231 sec/batch)\n",
      "2017-04-06 01:18:59.298061: step 75560, loss = (G: 5.65364313, D: 0.02768922) (0.224 sec/batch)\n",
      "2017-04-06 01:19:03.779122: step 75580, loss = (G: 7.41601562, D: 0.11575136) (0.223 sec/batch)\n",
      "2017-04-06 01:19:08.310977: step 75600, loss = (G: 8.60967636, D: 0.01138480) (0.223 sec/batch)\n",
      "2017-04-06 01:19:12.960438: step 75620, loss = (G: 11.64586067, D: 0.00053554) (0.224 sec/batch)\n",
      "2017-04-06 01:19:17.443670: step 75640, loss = (G: 6.08942652, D: 0.01557935) (0.224 sec/batch)\n",
      "2017-04-06 01:19:21.920470: step 75660, loss = (G: 10.36898136, D: 0.07343063) (0.224 sec/batch)\n",
      "2017-04-06 01:19:26.411580: step 75680, loss = (G: 5.80946541, D: 0.02212020) (0.225 sec/batch)\n",
      "2017-04-06 01:19:30.892172: step 75700, loss = (G: 8.42914677, D: 0.04224052) (0.227 sec/batch)\n",
      "2017-04-06 01:19:35.530988: step 75720, loss = (G: 6.91612577, D: 0.02241680) (0.224 sec/batch)\n",
      "2017-04-06 01:19:40.022253: step 75740, loss = (G: 6.91643143, D: 0.01340740) (0.223 sec/batch)\n",
      "2017-04-06 01:19:44.514254: step 75760, loss = (G: 6.96519852, D: 0.01155093) (0.224 sec/batch)\n",
      "2017-04-06 01:19:48.998367: step 75780, loss = (G: 9.65849209, D: 0.00345729) (0.223 sec/batch)\n",
      "2017-04-06 01:19:53.485439: step 75800, loss = (G: 6.79380798, D: 0.01239347) (0.224 sec/batch)\n",
      "2017-04-06 01:19:58.125559: step 75820, loss = (G: 8.01704311, D: 0.00381660) (0.225 sec/batch)\n",
      "2017-04-06 01:20:02.630546: step 75840, loss = (G: 7.30984211, D: 0.01048208) (0.224 sec/batch)\n",
      "2017-04-06 01:20:07.116035: step 75860, loss = (G: 8.85084724, D: 0.00702757) (0.224 sec/batch)\n",
      "2017-04-06 01:20:11.608417: step 75880, loss = (G: 6.71045303, D: 0.00925856) (0.225 sec/batch)\n",
      "2017-04-06 01:20:16.106789: step 75900, loss = (G: 6.29050350, D: 0.03524288) (0.224 sec/batch)\n",
      "2017-04-06 01:20:20.743928: step 75920, loss = (G: 6.59191608, D: 0.01374076) (0.225 sec/batch)\n",
      "2017-04-06 01:20:25.260624: step 75940, loss = (G: 8.46622753, D: 0.00593609) (0.229 sec/batch)\n",
      "2017-04-06 01:20:29.775231: step 75960, loss = (G: 7.55143785, D: 0.01995674) (0.224 sec/batch)\n",
      "2017-04-06 01:20:34.283622: step 75980, loss = (G: 7.83900642, D: 0.04693082) (0.224 sec/batch)\n",
      "2017-04-06 01:20:38.770678: step 76000, loss = (G: 13.23176193, D: 0.00271406) (0.224 sec/batch)\n",
      "2017-04-06 01:20:43.411475: step 76020, loss = (G: 6.60589552, D: 0.01000924) (0.231 sec/batch)\n",
      "2017-04-06 01:20:47.928242: step 76040, loss = (G: 8.58467007, D: 0.00154037) (0.224 sec/batch)\n",
      "2017-04-06 01:20:52.425318: step 76060, loss = (G: 8.10686398, D: 0.00343896) (0.224 sec/batch)\n",
      "2017-04-06 01:20:56.912739: step 76080, loss = (G: 15.78374386, D: 0.00009319) (0.225 sec/batch)\n",
      "2017-04-06 01:21:01.398305: step 76100, loss = (G: 9.79421997, D: 0.00180877) (0.228 sec/batch)\n",
      "2017-04-06 01:21:06.029448: step 76120, loss = (G: 8.82936096, D: 0.02894926) (0.223 sec/batch)\n",
      "2017-04-06 01:21:10.524310: step 76140, loss = (G: 7.72149706, D: 0.00837241) (0.225 sec/batch)\n",
      "2017-04-06 01:21:15.011291: step 76160, loss = (G: 11.98179340, D: 0.00102567) (0.225 sec/batch)\n",
      "2017-04-06 01:21:19.505874: step 76180, loss = (G: 8.17826080, D: 0.00465746) (0.224 sec/batch)\n",
      "2017-04-06 01:21:23.988595: step 76200, loss = (G: 10.38018036, D: 0.00041570) (0.224 sec/batch)\n",
      "2017-04-06 01:21:28.622753: step 76220, loss = (G: 8.96323872, D: 0.00192347) (0.226 sec/batch)\n",
      "2017-04-06 01:21:33.108251: step 76240, loss = (G: 7.20025444, D: 0.00600959) (0.224 sec/batch)\n",
      "2017-04-06 01:21:37.593729: step 76260, loss = (G: 5.84649849, D: 0.03862244) (0.224 sec/batch)\n",
      "2017-04-06 01:21:42.081541: step 76280, loss = (G: 9.83129978, D: 0.00137615) (0.225 sec/batch)\n",
      "2017-04-06 01:21:46.577130: step 76300, loss = (G: 10.40029526, D: 0.00060081) (0.224 sec/batch)\n",
      "2017-04-06 01:21:51.226246: step 76320, loss = (G: 6.50559092, D: 0.02230520) (0.225 sec/batch)\n",
      "2017-04-06 01:21:55.720650: step 76340, loss = (G: 7.76523399, D: 0.00487360) (0.227 sec/batch)\n",
      "2017-04-06 01:22:00.227041: step 76360, loss = (G: 6.16208458, D: 0.01197768) (0.225 sec/batch)\n",
      "2017-04-06 01:22:04.720433: step 76380, loss = (G: 10.55688763, D: 0.00781159) (0.224 sec/batch)\n",
      "2017-04-06 01:22:09.213472: step 76400, loss = (G: 4.63903666, D: 0.07337185) (0.224 sec/batch)\n",
      "2017-04-06 01:22:13.845876: step 76420, loss = (G: 9.60275269, D: 0.01337995) (0.224 sec/batch)\n",
      "2017-04-06 01:22:18.384906: step 76440, loss = (G: 8.80192661, D: 0.10517376) (0.226 sec/batch)\n",
      "2017-04-06 01:22:22.869490: step 76460, loss = (G: 6.32851553, D: 0.02521892) (0.225 sec/batch)\n",
      "2017-04-06 01:22:27.351379: step 76480, loss = (G: 7.68252182, D: 0.00804349) (0.224 sec/batch)\n",
      "2017-04-06 01:22:31.831980: step 76500, loss = (G: 8.86385250, D: 0.09022515) (0.224 sec/batch)\n",
      "2017-04-06 01:22:36.478374: step 76520, loss = (G: 6.96685219, D: 0.01632360) (0.224 sec/batch)\n",
      "2017-04-06 01:22:41.100129: step 76540, loss = (G: 10.58296204, D: 0.00376635) (0.223 sec/batch)\n",
      "2017-04-06 01:22:45.629810: step 76560, loss = (G: 8.32602406, D: 0.01178714) (0.224 sec/batch)\n",
      "2017-04-06 01:22:50.119614: step 76580, loss = (G: 6.28490257, D: 0.04026916) (0.225 sec/batch)\n",
      "2017-04-06 01:22:54.602616: step 76600, loss = (G: 7.28555679, D: 0.01440740) (0.225 sec/batch)\n",
      "2017-04-06 01:22:59.231073: step 76620, loss = (G: 7.48764515, D: 0.00977097) (0.225 sec/batch)\n",
      "2017-04-06 01:23:03.732570: step 76640, loss = (G: 6.82047415, D: 0.01222997) (0.224 sec/batch)\n",
      "2017-04-06 01:23:08.262668: step 76660, loss = (G: 6.54883099, D: 0.06779277) (0.225 sec/batch)\n",
      "2017-04-06 01:23:12.751795: step 76680, loss = (G: 16.81983185, D: 0.00065371) (0.226 sec/batch)\n",
      "2017-04-06 01:23:17.233777: step 76700, loss = (G: 6.83392048, D: 0.01220884) (0.224 sec/batch)\n",
      "2017-04-06 01:23:21.869431: step 76720, loss = (G: 6.43790007, D: 0.03400853) (0.224 sec/batch)\n",
      "2017-04-06 01:23:26.352473: step 76740, loss = (G: 8.00129986, D: 0.00349081) (0.224 sec/batch)\n",
      "2017-04-06 01:23:30.835966: step 76760, loss = (G: 13.96681595, D: 0.00015194) (0.225 sec/batch)\n",
      "2017-04-06 01:23:35.325179: step 76780, loss = (G: 8.19750595, D: 0.00662269) (0.224 sec/batch)\n",
      "2017-04-06 01:23:39.814677: step 76800, loss = (G: 4.68404102, D: 0.10911024) (0.223 sec/batch)\n",
      "2017-04-06 01:23:44.449390: step 76820, loss = (G: 13.53662491, D: 0.04185869) (0.225 sec/batch)\n",
      "2017-04-06 01:23:48.944434: step 76840, loss = (G: 7.69657278, D: 0.00810418) (0.230 sec/batch)\n",
      "2017-04-06 01:23:53.473671: step 76860, loss = (G: 8.54077148, D: 0.00210537) (0.224 sec/batch)\n",
      "2017-04-06 01:23:57.948825: step 76880, loss = (G: 4.43902588, D: 0.09881270) (0.224 sec/batch)\n",
      "2017-04-06 01:24:02.455626: step 76900, loss = (G: 8.78802395, D: 0.00485586) (0.224 sec/batch)\n",
      "2017-04-06 01:24:07.089315: step 76920, loss = (G: 7.66648388, D: 0.00419141) (0.225 sec/batch)\n",
      "2017-04-06 01:24:11.594736: step 76940, loss = (G: 6.42457438, D: 0.01344719) (0.224 sec/batch)\n",
      "2017-04-06 01:24:16.104454: step 76960, loss = (G: 11.77720451, D: 0.04807737) (0.224 sec/batch)\n",
      "2017-04-06 01:24:20.586083: step 76980, loss = (G: 10.15348244, D: 0.00547329) (0.224 sec/batch)\n",
      "2017-04-06 01:24:25.073099: step 77000, loss = (G: 11.04417610, D: 0.00461826) (0.224 sec/batch)\n",
      "2017-04-06 01:24:29.698831: step 77020, loss = (G: 4.92519093, D: 0.05510620) (0.225 sec/batch)\n",
      "2017-04-06 01:24:34.193050: step 77040, loss = (G: 6.72836494, D: 0.03958355) (0.225 sec/batch)\n",
      "2017-04-06 01:24:38.683300: step 77060, loss = (G: 6.73095608, D: 0.01028282) (0.223 sec/batch)\n",
      "2017-04-06 01:24:43.184168: step 77080, loss = (G: 7.71121216, D: 0.04298935) (0.225 sec/batch)\n",
      "2017-04-06 01:24:47.674130: step 77100, loss = (G: 5.82313538, D: 0.03856307) (0.223 sec/batch)\n",
      "2017-04-06 01:24:52.301052: step 77120, loss = (G: 7.10805750, D: 0.02085636) (0.225 sec/batch)\n",
      "2017-04-06 01:24:56.797050: step 77140, loss = (G: 5.46777964, D: 0.02385016) (0.227 sec/batch)\n",
      "2017-04-06 01:25:01.298113: step 77160, loss = (G: 7.07089567, D: 0.00481269) (0.225 sec/batch)\n",
      "2017-04-06 01:25:05.785872: step 77180, loss = (G: 18.16818047, D: 0.00060954) (0.223 sec/batch)\n",
      "2017-04-06 01:25:10.277067: step 77200, loss = (G: 8.60905647, D: 0.00170300) (0.224 sec/batch)\n",
      "2017-04-06 01:25:14.902462: step 77220, loss = (G: 9.49150753, D: 0.00541264) (0.224 sec/batch)\n",
      "2017-04-06 01:25:19.385787: step 77240, loss = (G: 7.23241043, D: 0.00769423) (0.224 sec/batch)\n",
      "2017-04-06 01:25:23.891150: step 77260, loss = (G: 8.69475651, D: 0.00448915) (0.225 sec/batch)\n",
      "2017-04-06 01:25:28.380856: step 77280, loss = (G: 8.94150734, D: 0.00130825) (0.224 sec/batch)\n",
      "2017-04-06 01:25:32.871677: step 77300, loss = (G: 7.68056870, D: 0.00368740) (0.224 sec/batch)\n",
      "2017-04-06 01:25:37.508458: step 77320, loss = (G: 7.19496107, D: 0.00941398) (0.228 sec/batch)\n",
      "2017-04-06 01:25:41.995578: step 77340, loss = (G: 6.80824566, D: 0.02305106) (0.225 sec/batch)\n",
      "2017-04-06 01:25:46.517201: step 77360, loss = (G: 12.07618523, D: 0.03427652) (0.230 sec/batch)\n",
      "2017-04-06 01:25:51.024873: step 77380, loss = (G: 10.65455627, D: 0.00150746) (0.226 sec/batch)\n",
      "2017-04-06 01:25:55.514063: step 77400, loss = (G: 6.42665911, D: 0.02232263) (0.224 sec/batch)\n",
      "2017-04-06 01:26:00.134695: step 77420, loss = (G: 10.52591038, D: 0.41420776) (0.225 sec/batch)\n",
      "2017-04-06 01:26:04.627436: step 77440, loss = (G: 6.14158535, D: 0.02307620) (0.223 sec/batch)\n",
      "2017-04-06 01:26:09.120343: step 77460, loss = (G: 6.64391899, D: 0.04195776) (0.226 sec/batch)\n",
      "2017-04-06 01:26:13.610401: step 77480, loss = (G: 9.51140594, D: 0.00627208) (0.225 sec/batch)\n",
      "2017-04-06 01:26:18.095762: step 77500, loss = (G: 12.40534115, D: 0.00047746) (0.224 sec/batch)\n",
      "2017-04-06 01:26:22.723963: step 77520, loss = (G: 10.15134811, D: 0.01020882) (0.223 sec/batch)\n",
      "2017-04-06 01:26:27.218661: step 77540, loss = (G: 9.76352024, D: 0.00167568) (0.226 sec/batch)\n",
      "2017-04-06 01:26:31.714279: step 77560, loss = (G: 9.19491196, D: 0.02111181) (0.225 sec/batch)\n",
      "2017-04-06 01:26:36.208559: step 77580, loss = (G: 9.09248352, D: 0.00179869) (0.224 sec/batch)\n",
      "2017-04-06 01:26:40.701141: step 77600, loss = (G: 4.37919712, D: 0.20856422) (0.224 sec/batch)\n",
      "2017-04-06 01:26:45.352888: step 77620, loss = (G: 8.21592617, D: 0.00661169) (0.224 sec/batch)\n",
      "2017-04-06 01:26:49.834292: step 77640, loss = (G: 6.27465534, D: 0.01849403) (0.223 sec/batch)\n",
      "2017-04-06 01:26:54.325587: step 77660, loss = (G: 7.99464893, D: 0.01592570) (0.228 sec/batch)\n",
      "2017-04-06 01:26:58.820261: step 77680, loss = (G: 6.34843779, D: 0.03103296) (0.224 sec/batch)\n",
      "2017-04-06 01:27:03.354509: step 77700, loss = (G: 6.96809864, D: 0.02624846) (0.223 sec/batch)\n",
      "2017-04-06 01:27:08.009725: step 77720, loss = (G: 22.75522041, D: 0.07232559) (0.230 sec/batch)\n",
      "2017-04-06 01:27:12.496425: step 77740, loss = (G: 11.12063408, D: 0.00021312) (0.226 sec/batch)\n",
      "2017-04-06 01:27:16.988305: step 77760, loss = (G: 7.30110741, D: 0.00422782) (0.224 sec/batch)\n",
      "2017-04-06 01:27:21.475617: step 77780, loss = (G: 7.12090731, D: 0.01629581) (0.224 sec/batch)\n",
      "2017-04-06 01:27:26.018203: step 77800, loss = (G: 9.94080734, D: 0.00718393) (0.226 sec/batch)\n",
      "2017-04-06 01:27:30.660118: step 77820, loss = (G: 7.52469778, D: 0.00696664) (0.227 sec/batch)\n",
      "2017-04-06 01:27:35.147538: step 77840, loss = (G: 5.46070194, D: 0.03429522) (0.224 sec/batch)\n",
      "2017-04-06 01:27:39.632501: step 77860, loss = (G: 10.97342587, D: 0.05567482) (0.225 sec/batch)\n",
      "2017-04-06 01:27:44.113315: step 77880, loss = (G: 9.70955849, D: 0.01033481) (0.224 sec/batch)\n",
      "2017-04-06 01:27:48.629107: step 77900, loss = (G: 6.79433393, D: 0.01097510) (0.223 sec/batch)\n",
      "2017-04-06 01:27:53.252918: step 77920, loss = (G: 6.23328972, D: 0.08049368) (0.225 sec/batch)\n",
      "2017-04-06 01:27:57.739913: step 77940, loss = (G: 8.83208656, D: 0.00600128) (0.225 sec/batch)\n",
      "2017-04-06 01:28:02.232995: step 77960, loss = (G: 10.86102962, D: 0.00019864) (0.224 sec/batch)\n",
      "2017-04-06 01:28:06.711922: step 77980, loss = (G: 8.50660896, D: 0.00320057) (0.223 sec/batch)\n",
      "2017-04-06 01:28:11.215104: step 78000, loss = (G: 7.55160666, D: 0.00537399) (0.223 sec/batch)\n",
      "2017-04-06 01:28:15.849041: step 78020, loss = (G: 6.07590532, D: 0.02433217) (0.226 sec/batch)\n",
      "2017-04-06 01:28:20.352318: step 78040, loss = (G: 6.03584146, D: 0.01568414) (0.228 sec/batch)\n",
      "2017-04-06 01:28:24.841034: step 78060, loss = (G: 8.04712105, D: 0.00579827) (0.225 sec/batch)\n",
      "2017-04-06 01:28:29.323126: step 78080, loss = (G: 9.77987289, D: 0.00513407) (0.223 sec/batch)\n",
      "2017-04-06 01:28:33.811592: step 78100, loss = (G: 7.13978958, D: 0.01165838) (0.225 sec/batch)\n",
      "2017-04-06 01:28:38.442132: step 78120, loss = (G: 7.26883078, D: 0.01665547) (0.225 sec/batch)\n",
      "2017-04-06 01:28:42.925626: step 78140, loss = (G: 11.44761562, D: 0.00029082) (0.225 sec/batch)\n",
      "2017-04-06 01:28:47.427583: step 78160, loss = (G: 5.54592800, D: 0.02864869) (0.226 sec/batch)\n",
      "2017-04-06 01:28:51.912264: step 78180, loss = (G: 7.18179703, D: 0.01217436) (0.225 sec/batch)\n",
      "2017-04-06 01:28:56.415442: step 78200, loss = (G: 7.58907509, D: 0.13624889) (0.223 sec/batch)\n",
      "2017-04-06 01:29:01.036273: step 78220, loss = (G: 7.11451387, D: 0.00697521) (0.223 sec/batch)\n",
      "2017-04-06 01:29:05.550320: step 78240, loss = (G: 7.82409191, D: 0.00532086) (0.226 sec/batch)\n",
      "2017-04-06 01:29:10.038079: step 78260, loss = (G: 6.63728237, D: 0.03171284) (0.231 sec/batch)\n",
      "2017-04-06 01:29:14.564779: step 78280, loss = (G: 7.36666822, D: 0.00520088) (0.224 sec/batch)\n",
      "2017-04-06 01:29:19.053021: step 78300, loss = (G: 8.45042515, D: 0.02319803) (0.224 sec/batch)\n",
      "2017-04-06 01:29:23.683259: step 78320, loss = (G: 4.81496286, D: 0.09199534) (0.226 sec/batch)\n",
      "2017-04-06 01:29:28.181981: step 78340, loss = (G: 9.39151001, D: 0.00463507) (0.224 sec/batch)\n",
      "2017-04-06 01:29:32.662923: step 78360, loss = (G: 8.94611359, D: 0.00371781) (0.224 sec/batch)\n",
      "2017-04-06 01:29:37.160634: step 78380, loss = (G: 7.64221001, D: 0.01236288) (0.224 sec/batch)\n",
      "2017-04-06 01:29:41.654860: step 78400, loss = (G: 7.91551161, D: 0.00462706) (0.225 sec/batch)\n",
      "2017-04-06 01:29:46.278506: step 78420, loss = (G: 6.83756161, D: 0.01048339) (0.225 sec/batch)\n",
      "2017-04-06 01:29:50.765236: step 78440, loss = (G: 8.59889126, D: 0.01305585) (0.224 sec/batch)\n",
      "2017-04-06 01:29:55.255975: step 78460, loss = (G: 4.57223129, D: 0.12314887) (0.224 sec/batch)\n",
      "2017-04-06 01:29:59.741409: step 78480, loss = (G: 9.84764481, D: 0.00195309) (0.223 sec/batch)\n",
      "2017-04-06 01:30:04.244271: step 78500, loss = (G: 9.96875477, D: 0.01732344) (0.224 sec/batch)\n",
      "2017-04-06 01:30:08.880012: step 78520, loss = (G: 8.46411610, D: 0.00349163) (0.223 sec/batch)\n",
      "2017-04-06 01:30:13.358701: step 78540, loss = (G: 5.39218712, D: 0.02820232) (0.223 sec/batch)\n",
      "2017-04-06 01:30:17.840831: step 78560, loss = (G: 7.06366014, D: 0.00777878) (0.225 sec/batch)\n",
      "2017-04-06 01:30:22.328638: step 78580, loss = (G: 6.69374228, D: 0.01676144) (0.231 sec/batch)\n",
      "2017-04-06 01:30:26.827413: step 78600, loss = (G: 8.95735741, D: 1.02011383) (0.224 sec/batch)\n",
      "2017-04-06 01:30:31.462989: step 78620, loss = (G: 10.19102383, D: 0.00413539) (0.223 sec/batch)\n",
      "2017-04-06 01:30:35.983637: step 78640, loss = (G: 13.97822857, D: 0.08589806) (0.224 sec/batch)\n",
      "2017-04-06 01:30:40.475522: step 78660, loss = (G: 9.77926731, D: 0.02307343) (0.225 sec/batch)\n",
      "2017-04-06 01:30:45.009075: step 78680, loss = (G: 17.34101486, D: 0.00027601) (0.223 sec/batch)\n",
      "2017-04-06 01:30:49.489554: step 78700, loss = (G: 8.71424198, D: 0.00245002) (0.223 sec/batch)\n",
      "2017-04-06 01:30:54.126643: step 78720, loss = (G: 9.09441853, D: 0.00275679) (0.225 sec/batch)\n",
      "2017-04-06 01:30:58.619215: step 78740, loss = (G: 10.66664219, D: 0.00672469) (0.224 sec/batch)\n",
      "2017-04-06 01:31:03.111068: step 78760, loss = (G: 7.67722511, D: 0.00928153) (0.223 sec/batch)\n",
      "2017-04-06 01:31:07.605149: step 78780, loss = (G: 6.38375854, D: 0.03647312) (0.224 sec/batch)\n",
      "2017-04-06 01:31:12.095729: step 78800, loss = (G: 4.79577827, D: 0.06235663) (0.225 sec/batch)\n",
      "2017-04-06 01:31:16.720934: step 78820, loss = (G: 11.85994244, D: 0.00062590) (0.224 sec/batch)\n",
      "2017-04-06 01:31:21.232951: step 78840, loss = (G: 7.95819426, D: 0.00520411) (0.226 sec/batch)\n",
      "2017-04-06 01:31:25.852835: step 78860, loss = (G: 7.81086731, D: 0.00517201) (0.223 sec/batch)\n",
      "2017-04-06 01:31:30.347325: step 78880, loss = (G: 8.90674400, D: 0.00151554) (0.224 sec/batch)\n",
      "2017-04-06 01:31:34.841072: step 78900, loss = (G: 7.58051014, D: 0.00433730) (0.224 sec/batch)\n",
      "2017-04-06 01:31:39.472769: step 78920, loss = (G: 5.44080448, D: 0.04364432) (0.225 sec/batch)\n",
      "2017-04-06 01:31:43.968028: step 78940, loss = (G: 8.09342480, D: 0.01322160) (0.227 sec/batch)\n",
      "2017-04-06 01:31:48.473143: step 78960, loss = (G: 10.00796413, D: 0.00066017) (0.229 sec/batch)\n",
      "2017-04-06 01:31:52.968949: step 78980, loss = (G: 11.79027557, D: 0.00046737) (0.224 sec/batch)\n",
      "2017-04-06 01:31:57.454051: step 79000, loss = (G: 11.94042301, D: 0.00748166) (0.224 sec/batch)\n",
      "2017-04-06 01:32:02.084491: step 79020, loss = (G: 9.37854481, D: 0.00188229) (0.224 sec/batch)\n",
      "2017-04-06 01:32:06.570871: step 79040, loss = (G: 12.03905106, D: 0.00014719) (0.224 sec/batch)\n",
      "2017-04-06 01:32:11.055074: step 79060, loss = (G: 5.80487204, D: 0.04409761) (0.225 sec/batch)\n",
      "2017-04-06 01:32:15.541250: step 79080, loss = (G: 10.62184238, D: 0.00360812) (0.224 sec/batch)\n",
      "2017-04-06 01:32:20.074174: step 79100, loss = (G: 8.02600288, D: 0.01220456) (0.225 sec/batch)\n",
      "2017-04-06 01:32:24.699160: step 79120, loss = (G: 8.08272076, D: 0.00443337) (0.224 sec/batch)\n",
      "2017-04-06 01:32:29.189834: step 79140, loss = (G: 8.99622822, D: 0.01731938) (0.227 sec/batch)\n",
      "2017-04-06 01:32:33.676832: step 79160, loss = (G: 5.60576010, D: 0.06384195) (0.224 sec/batch)\n",
      "2017-04-06 01:32:38.162206: step 79180, loss = (G: 6.96763802, D: 0.14585407) (0.225 sec/batch)\n",
      "2017-04-06 01:32:42.647774: step 79200, loss = (G: 10.84551525, D: 0.21846521) (0.225 sec/batch)\n",
      "2017-04-06 01:32:47.279437: step 79220, loss = (G: 10.28276062, D: 0.00381934) (0.224 sec/batch)\n",
      "2017-04-06 01:32:51.807294: step 79240, loss = (G: 10.80105019, D: 0.00099403) (0.223 sec/batch)\n",
      "2017-04-06 01:32:56.331018: step 79260, loss = (G: 9.27798748, D: 0.00137282) (0.227 sec/batch)\n",
      "2017-04-06 01:33:00.809091: step 79280, loss = (G: 10.08512020, D: 0.00201888) (0.224 sec/batch)\n",
      "2017-04-06 01:33:05.288809: step 79300, loss = (G: 4.95327759, D: 0.05137713) (0.224 sec/batch)\n",
      "2017-04-06 01:33:09.913668: step 79320, loss = (G: 6.38767815, D: 0.05248327) (0.225 sec/batch)\n",
      "2017-04-06 01:33:14.410329: step 79340, loss = (G: 6.54381847, D: 0.00847484) (0.225 sec/batch)\n",
      "2017-04-06 01:33:18.911767: step 79360, loss = (G: 11.36265469, D: 0.01837141) (0.226 sec/batch)\n",
      "2017-04-06 01:33:23.427405: step 79380, loss = (G: 17.76913452, D: 0.00145888) (0.225 sec/batch)\n",
      "2017-04-06 01:33:27.949586: step 79400, loss = (G: 8.45703983, D: 0.00260197) (0.229 sec/batch)\n",
      "2017-04-06 01:33:32.579262: step 79420, loss = (G: 6.53005409, D: 0.03717550) (0.223 sec/batch)\n",
      "2017-04-06 01:33:37.081400: step 79440, loss = (G: 7.56752825, D: 0.00649837) (0.224 sec/batch)\n",
      "2017-04-06 01:33:41.614923: step 79460, loss = (G: 8.29472637, D: 0.00854919) (0.224 sec/batch)\n",
      "2017-04-06 01:33:46.092716: step 79480, loss = (G: 7.37406683, D: 0.00689116) (0.224 sec/batch)\n",
      "2017-04-06 01:33:50.576348: step 79500, loss = (G: 7.62080383, D: 0.05838817) (0.224 sec/batch)\n",
      "2017-04-06 01:33:55.225583: step 79520, loss = (G: 4.80430841, D: 0.04211419) (0.229 sec/batch)\n",
      "2017-04-06 01:33:59.713784: step 79540, loss = (G: 10.48420238, D: 0.01563713) (0.225 sec/batch)\n",
      "2017-04-06 01:34:04.191986: step 79560, loss = (G: 6.58169317, D: 0.02528365) (0.224 sec/batch)\n",
      "2017-04-06 01:34:08.671994: step 79580, loss = (G: 6.23057032, D: 0.02469146) (0.224 sec/batch)\n",
      "2017-04-06 01:34:13.202763: step 79600, loss = (G: 7.48425150, D: 0.01493926) (0.223 sec/batch)\n",
      "2017-04-06 01:34:17.825743: step 79620, loss = (G: 10.15111732, D: 0.00092184) (0.224 sec/batch)\n",
      "2017-04-06 01:34:22.311009: step 79640, loss = (G: 8.76921368, D: 0.00390066) (0.223 sec/batch)\n",
      "2017-04-06 01:34:26.812170: step 79660, loss = (G: 5.80356932, D: 0.20144635) (0.227 sec/batch)\n",
      "2017-04-06 01:34:31.309666: step 79680, loss = (G: 6.90073872, D: 0.02569928) (0.225 sec/batch)\n",
      "2017-04-06 01:34:35.800565: step 79700, loss = (G: 8.02537918, D: 0.00417461) (0.224 sec/batch)\n",
      "2017-04-06 01:34:40.424251: step 79720, loss = (G: 7.51842737, D: 0.00618336) (0.225 sec/batch)\n",
      "2017-04-06 01:34:44.909797: step 79740, loss = (G: 9.11477470, D: 0.00266515) (0.225 sec/batch)\n",
      "2017-04-06 01:34:49.403855: step 79760, loss = (G: 7.88017130, D: 0.00455171) (0.224 sec/batch)\n",
      "2017-04-06 01:34:53.896714: step 79780, loss = (G: 7.21580076, D: 0.02455678) (0.224 sec/batch)\n",
      "2017-04-06 01:34:58.380691: step 79800, loss = (G: 9.16513252, D: 0.04484722) (0.225 sec/batch)\n",
      "2017-04-06 01:35:03.012894: step 79820, loss = (G: 6.43476200, D: 0.01332738) (0.227 sec/batch)\n",
      "2017-04-06 01:35:07.496665: step 79840, loss = (G: 8.15156555, D: 0.00543320) (0.225 sec/batch)\n",
      "2017-04-06 01:35:12.008198: step 79860, loss = (G: 9.13948536, D: 0.00865930) (0.225 sec/batch)\n",
      "2017-04-06 01:35:16.493619: step 79880, loss = (G: 6.63018370, D: 0.05804154) (0.225 sec/batch)\n",
      "2017-04-06 01:35:20.985677: step 79900, loss = (G: 6.89792442, D: 0.05146602) (0.225 sec/batch)\n",
      "2017-04-06 01:35:25.615667: step 79920, loss = (G: 9.58297253, D: 0.00100258) (0.225 sec/batch)\n",
      "2017-04-06 01:35:30.118224: step 79940, loss = (G: 9.97858524, D: 0.00062011) (0.224 sec/batch)\n",
      "2017-04-06 01:35:34.624442: step 79960, loss = (G: 7.88896942, D: 0.00652383) (0.225 sec/batch)\n",
      "2017-04-06 01:35:39.112032: step 79980, loss = (G: 15.69976807, D: 0.01661220) (0.225 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# feature matching\n",
    "graph = tf.get_default_graph()\n",
    "features_g = tf.reduce_mean(graph.get_tensor_by_name('dg/d/conv4/outputs:0'), 0)\n",
    "features_t = tf.reduce_mean(graph.get_tensor_by_name('dt/d/conv4/outputs:0'), 0)\n",
    "losses[dcgan.g] += tf.multiply(tf.nn.l2_loss(features_g - features_t), 0.05)\n",
    "\n",
    "tf.summary.scalar('g loss', losses[dcgan.g])\n",
    "tf.summary.scalar('d loss', losses[dcgan.d])\n",
    "train_op = dcgan.train(losses, learning_rate=0.0001)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "g_saver = tf.train.Saver(dcgan.g.variables, max_to_keep=15)\n",
    "d_saver = tf.train.Saver(dcgan.d.variables, max_to_keep=15)\n",
    "g_checkpoint_path = os.path.join(FLAGS.logdir, 'g.ckpt')\n",
    "d_checkpoint_path = os.path.join(FLAGS.logdir, 'd.ckpt')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC' \n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.logdir, graph=sess.graph)\n",
    "    # restore or initialize generator\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if os.path.exists(g_checkpoint_path):\n",
    "        print('restore variables:')\n",
    "        for v in dcgan.g.variables:\n",
    "            print('  ' + v.name)\n",
    "        g_saver.restore(sess, g_checkpoint_path)\n",
    "    if os.path.exists(d_checkpoint_path):\n",
    "        print('restore variables:')\n",
    "        for v in dcgan.d.variables:\n",
    "            print('  ' + v.name)\n",
    "        d_saver.restore(sess, d_checkpoint_path)\n",
    "\n",
    "    # setup for monitoring\n",
    "    sample_z = sess.run(tf.random_uniform([dcgan.batch_size, dcgan.z_dim], minval=-1.0, maxval=1.0))\n",
    "    images = dcgan.sample_images(5, 5, inputs=sample_z)\n",
    "\n",
    "    # start training\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    for step in range(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        _, g_loss, d_loss = sess.run([train_op, losses[dcgan.g], losses[dcgan.d]])\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if step%20 == 0:\n",
    "            print('{}: step {:5d}, loss = (G: {:.8f}, D: {:.8f}) ({:.3f} sec/batch)'.format(\n",
    "                datetime.now(), step, g_loss, d_loss, duration))\n",
    "\n",
    "        # save generated images\n",
    "        if step % 100 == 0:\n",
    "            # summary\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            # sample images\n",
    "            filename = os.path.join(FLAGS.images_dir, '%05d.jpg' % step)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(sess.run(images))\n",
    "        # save variables\n",
    "        '''\n",
    "        if (step+1) % 10000 == 0:\n",
    "            g_saver.save(sess, g_checkpoint_path, global_step=step)\n",
    "            d_saver.save(sess, d_checkpoint_path, global_step=step)\n",
    "        '''\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
