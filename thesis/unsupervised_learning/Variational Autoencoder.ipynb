{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Variational Autoencoder\n",
    "*Zhiang Chen, March 2017*\n",
    "\n",
    "Adapted from the blog here: https://jmetzen.github.io/2015-11-27/vae.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import deepdish as dd\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training dataset', (427680, 11), (427680, 10), (427680, 40, 40, 1))\n",
      "('validation dataset', (71226, 11), (71226, 10), (71226, 40, 40, 1))\n",
      "('testing dataset', (71280, 11), (71280, 10), (71280, 40, 40, 1))\n"
     ]
    }
   ],
   "source": [
    "wd = os.getcwd()\n",
    "os.chdir('..')\n",
    "file_name = 'depth_data.h5'\n",
    "\n",
    "save = dd.io.load(file_name)\n",
    "\n",
    "train_objects = save['train_objects']\n",
    "train_orientations = save['train_orientations']\n",
    "train_values = save['train_values']\n",
    "valid_objects = save['valid_objects']\n",
    "valid_orientations = save['valid_orientations']\n",
    "valid_values = save['valid_values']\n",
    "test_objects = save['test_objects']\n",
    "test_orientations = save['test_orientations']\n",
    "test_values = save['test_values']\n",
    "value2object = save['value2object']\n",
    "object2value = save['object2value']\n",
    "del save\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "print('training dataset', train_objects.shape, train_orientations.shape, train_values.shape)\n",
    "print('validation dataset', valid_objects.shape, valid_orientations.shape, valid_values.shape)\n",
    "print('testing dataset', test_objects.shape, test_orientations.shape, test_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size = 40\n",
    "\n",
    "def randomize(dataset, classes, angles):\n",
    "    permutation = np.random.permutation(classes.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_classes = classes[permutation]\n",
    "    shuffled_angles = angles[permutation]\n",
    "    return shuffled_dataset, shuffled_classes, shuffled_angles\n",
    "\n",
    "train_dataset, train_classes, train_angles = randomize(train_values, train_objects, train_orientations)\n",
    "valid_dataset, valid_classes, valid_angles = randomize(valid_values, valid_objects, valid_orientations)\n",
    "test_dataset, test_classes, test_angles = randomize(test_values, test_objects, test_orientations)\n",
    "\n",
    "valid_dataset = valid_dataset[:5000,:,:,:]\n",
    "valid_angles = valid_angles[:5000,:]\n",
    "valid_classes = valid_classes[:5000,:]\n",
    "\n",
    "test_dataset = test_dataset[:5000,:,:,:]\n",
    "test_angles = test_angles[:5000,:]\n",
    "test_classes = test_classes[:5000,:]\n",
    "\n",
    "train_dataset = train_dataset.reshape(-1,image_size*image_size)\n",
    "test_dataset = test_dataset.reshape(-1,image_size*image_size)\n",
    "n_samples = train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "'''MNIST'''\n",
    "mnist = read_data_sets('MNIST_data', one_hot=True)\n",
    "n_samples = mnist.train.num_examples\n",
    "train_dataset = mnist.train.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5. VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [batch_size, network_architecture[\"n_input\"]])\n",
    "        self.image_size = float(network_architecture[\"n_input\"])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth=True\n",
    "        config.log_device_placement = True\n",
    "        config.gpu_options.allocator_type = 'BFC' \n",
    "        self.sess = tf.InteractiveSession(config = config)\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(network_weights[\"weights_recog\"], \n",
    "                                      network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean, self.x_reconstr_log_sigma_sq = \\\n",
    "            self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                    network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        x_reconstr_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),biases['out_mean'])\n",
    "        \n",
    "        #x_reconstr_mean = tf.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']),biases['out_mean']))\n",
    "            \n",
    "        x_reconstr_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']),\n",
    "                                 biases['out_log_sigma'])\n",
    "            \n",
    "        return x_reconstr_mean, x_reconstr_log_sigma_sq\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        '''\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        self.disp = self.x_reconstr_mean\n",
    "        self.reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "                           1)\n",
    "        '''\n",
    "        # The p(x|z) has multi-Gaussian distribution\n",
    "        sigma_sq = tf.exp(self.x_reconstr_log_sigma_sq)\n",
    "        std_dev = tf.sqrt(sigma_sq)\n",
    "        reconstr_p = tf.contrib.distributions.MultivariateNormalDiag(self.x_reconstr_mean, std_dev)\n",
    "        inverse_sigma_sq = tf.matrix_diag(tf.reciprocal(sigma_sq))\n",
    "        #scalar = tf.matmul(tf.matmul(tf.transpose(self.x-self.x_reconstr_mean),inverse_sigma_sq),(self.x-self.x_reconstr_mean))\n",
    "        \n",
    "        self.reconstr_loss = -tf.log(1e-10+reconstr_p.pdf(self.x))\n",
    "        \n",
    "        x_u = tf.reshape(tf.transpose(self.x-self.x_reconstr_mean),[self.batch_size, tf.shape(self.x)[1],1])\n",
    "        energe = -0.5*tf.matmul(tf.matmul(tf.transpose(x_u, perm=[0, 2, 1]),inverse_sigma_sq), x_u)\n",
    "        s = tf.reciprocal(tf.sqrt(tf.matrix_determinant(tf.matrix_diag(sigma_sq))))\n",
    "        p = tf.constant((2*pi)**(-self.image_size/2))\n",
    "        normalizer = s\n",
    "        \n",
    "        print sigma_sq[0,:]\n",
    "        self.disp = sigma_sq[0,:]\n",
    "        #print(reconstr_p.get_batch_shape())\n",
    "        #print(reconstr_p.get_event_shape())\n",
    "        #'''\n",
    "        \n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        self.latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(self.reconstr_loss + self.latent_loss)   # average over batch\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost,l,r,disp = self.sess.run((self.optimizer, self.cost, self.latent_loss, self.reconstr_loss, self.disp), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost,l,r,disp\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_15:0\", shape=(784,), dtype=float32)\n",
      "('Epoch:', '20', 'cost', '28.946263027')\n",
      "latent_loss:\n",
      "[ 0.53423083  0.56153119  0.49477789  0.52286398  0.54887748  0.58194822\n",
      "  0.49725628  0.52852809  0.70528102  0.50914663  0.54700601  0.48735878\n",
      "  0.49582055  0.46773821  0.52785158  0.50089711  0.46263644  0.53390533\n",
      "  0.48505658  0.45232892  0.49491671  0.5308575   0.53437519  0.50982392\n",
      "  0.50953197  0.55981255  0.48890838  0.5134275   0.52018821  0.49800813\n",
      "  0.54097009  0.52017736  0.46686879  0.50900853  0.54870844  0.51936209\n",
      "  0.48227972  0.52016705  0.62236685  0.5699631   0.5065285   0.47114313\n",
      "  0.53471529  0.4585453   0.53601122  0.44192076  0.63972855  0.52239805\n",
      "  0.53770924  0.55516434  0.50524259  0.5052352   0.59945911  0.47508726\n",
      "  0.55262017  0.50247592  0.50859547  0.51506102  0.49597043  0.48381329\n",
      "  0.50146019  0.46316636  0.56508231  0.48723429  0.52209067  0.4962545\n",
      "  0.50982273  0.48732674  0.5661484   0.4992269   0.52476645  0.45819545\n",
      "  0.54657495  0.52914166  0.47435805  0.58297223  0.56521392  0.45864862\n",
      "  0.54018259  0.45753154  0.50880373  0.53365064  0.52869534  0.41436926\n",
      "  0.50605971  0.52462101  0.52005148  0.47193611  0.58955663  0.54655576\n",
      "  0.50583142  0.5259999   0.53408372  0.44583085  0.53835106  0.50775135\n",
      "  0.64621693  0.51094329  0.59057349  0.46729711]\n",
      "reconstr_loss\n",
      "[ 23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503]\n",
      "disp\n",
      "[  0.88780206   0.84793365   0.21167441   1.20651364   0.88436133\n",
      "   0.87546164   0.49270904   0.87764055   0.64270455   0.57385242\n",
      "   5.27301598   0.66416132   1.38385677   0.55298954   1.93713582\n",
      "   0.45434213   2.10110545   1.0734601    0.57382226   1.94249308\n",
      "   3.8986609    3.10593033   1.73268628   0.52659833   3.4620111\n",
      "   2.41949153   0.32672939   2.24990416   3.24836588   1.9359628\n",
      "   0.71002924   1.60518837   1.20303631   1.22935128   1.17328513\n",
      "   0.62800789   0.56666082   1.58433747   0.70906436   3.04869509\n",
      "   0.36831987   0.77390695   0.43373328   6.00991678   1.54774213\n",
      "   1.04072297   1.15516853   0.58372396   0.1917313    1.13569379\n",
      "   0.54042363   1.49531782   1.6611743    2.68021178   1.13057041\n",
      "   0.49088943   0.34227628   1.35135245   5.10378361   1.33541179\n",
      "   0.27701727   2.01213455   1.24852753   2.28242993   2.34102106\n",
      "   0.66497767   0.74709034   2.31816411   0.55312997   1.10580659\n",
      "   0.5139122    0.4224385    3.46876001   1.63913858   1.37774599\n",
      "   3.07378054   0.8290233    0.51589996   1.19430864   0.64712936\n",
      "   0.45620841   3.23900652   1.25388741   0.42324942   0.9471662\n",
      "   1.72872555   0.88557231   0.57654554   2.14660907   3.77451396\n",
      "   1.18980312   1.1190598    0.79793298   0.65551686   2.9447093\n",
      "   1.94198859   0.38635254   3.68579555   0.41096205   0.93983835\n",
      "   0.83537239   1.38871574   3.68653965   0.47211248   0.58228743\n",
      "   0.84759349   0.75411272   1.07462716   1.48995113   0.50897342\n",
      "   1.49098039   1.56789947   0.87629545   2.20151281   0.05877215\n",
      "   1.80238557   0.91791904   1.65462136   0.14571795   4.03468657\n",
      "   1.05600572   1.64510608   0.30407169   0.82112062   2.07549524\n",
      "   8.35221958   1.3710928    2.75360823   2.79317188   1.91255105\n",
      "   1.15076768   0.54929048   1.17651916   0.49334589   0.45056736\n",
      "   7.33343601   0.78222394   0.34043255   2.06865549   0.85363251\n",
      "   0.90390527   1.11628044   0.68918848   0.98082727   4.13299322\n",
      "   0.53458273   0.62911928   0.61859471   1.0110184    1.51717174\n",
      "   0.5206058    0.77746505   0.78300118   0.44669643   1.76541841\n",
      "   3.10229802   0.26570976   0.68639469   0.55175406   1.5836041\n",
      "   0.50599998   1.19911277   0.23928198   2.25072193   1.05335486\n",
      "   0.98891789   1.13290071   2.16287661   1.11387908   0.82658273\n",
      "   0.53342038   3.37979078   1.44179392   0.35902005   6.31472683\n",
      "   1.02541125   0.70801473   2.14978075   0.94890642   0.60509437\n",
      "   0.62321872   0.17907575  10.38569927   0.51446426   1.07203948\n",
      "   0.46856582   0.60152411   1.93550861   3.48674369   1.59017861\n",
      "   0.81607366   0.62737674   0.31755787   0.22721161   0.40167037\n",
      "   1.84224534   4.17116261   0.75033796   0.6844613    0.38837442\n",
      "   0.86811835   1.35735941   7.13735437   7.34149456   1.88004935\n",
      "   1.77420056   0.72431785   1.53067029   1.58046591   0.23598965\n",
      "   3.23614407   1.38972139   2.03333783   0.44048205   0.85919929\n",
      "   0.61364198   0.59272122   0.34297857   0.08088101   0.31408113\n",
      "   1.178725     1.0308733    1.86198282   0.82677549   1.12767434\n",
      "   1.40588582   0.67139578   0.78875798   0.9867475    4.14773083\n",
      "   0.72671551   3.36468005   3.2891922    0.84131455   0.59284699\n",
      "   1.33695817   1.19021082   0.29894096   0.38237488   0.44287845\n",
      "   0.67014021   1.66784286   2.35945702   0.5062229    2.08705902\n",
      "   0.91160303   0.3299771    5.95308924   2.44059515   2.30159521\n",
      "   1.08435237   1.06393874   1.06565857   0.64301753   0.33310622\n",
      "   2.68937969   0.7273463    1.07146466   0.89288467   1.48313904\n",
      "   0.82174188   1.40174055   1.45102334   1.55280864   0.78155577\n",
      "   2.22908092   1.02528036   1.0497278    0.55028856   0.62373209\n",
      "   0.47676545   0.4219974    2.11227965   1.48079026   1.20788944\n",
      "   0.28750634   3.33612609   1.17850327   1.40844989   0.44644007\n",
      "   0.16833282   4.67612028   0.20061041   0.82033205   0.65825659\n",
      "   2.94846559   2.08238935   1.76649201   0.58979076   2.11261415\n",
      "   0.47401828   1.33109367   0.84806365   1.36140823   1.75832212\n",
      "   1.13586009   0.34413505   2.73543024   0.65259469   1.26831603\n",
      "   0.50138873   0.74781358   1.10434163   0.71281147   0.31806868\n",
      "   1.39151907   2.85728407   0.6367923    1.75019705   4.12629509\n",
      "   0.29779834   1.60910439   2.032197     0.57035881   0.26878244\n",
      "   2.97813845   0.58350724   0.5869143    0.89730471   0.4361074\n",
      "   0.93670934   0.59211719   0.99582714   2.24478292   7.57337904\n",
      "   0.65186048   0.97686434   1.77222669   0.41242582   0.19291131\n",
      "   1.30635178   1.58024907   1.02903652   0.95917726   1.30909991\n",
      "   0.56096309   2.10223293   0.33776602   0.25649685   0.78124762\n",
      "   1.28166151   2.31149483   0.84104538   0.33808511   2.08048248\n",
      "   1.4735682    1.9921124    1.87104034   0.65257919   2.82426548\n",
      "   2.56407976   0.74463469   0.93837881   0.71437639   0.32568291\n",
      "   1.18262064   0.4969064    0.77295011   2.6847868    0.86290199\n",
      "   2.33528399   1.26761317   0.74997461   1.0451827    0.59589654\n",
      "   1.56143308   1.18814802   0.56416297   0.2191301    0.76946008\n",
      "   1.59750366   4.76537085   0.88287199   0.43068042   0.59255397\n",
      "   8.38685513   0.68904579   1.24676943   2.04470062   1.77497745\n",
      "   5.96257067   2.62090063   0.72683203   1.69190335   0.58470893\n",
      "   0.53550923   0.83861434   4.08873081   0.40589076   1.25487125\n",
      "   3.22912264   2.71275973   0.92742628   0.96124274   2.1557076\n",
      "   0.85146064   1.5736022    1.20405388   2.34711409   0.366234\n",
      "   2.06525135   0.51354009   0.13900207   1.33172631   0.55731505\n",
      "   0.40537485   0.38862666   1.36879265   0.28339198   0.58768404\n",
      "   1.3877871    5.02205467   0.33951521   2.07697725   0.63190973\n",
      "   0.49284875   0.86029184   1.38793969   0.50458992   1.58466172\n",
      "   0.45234272   0.83867347   1.47145295   0.53629994   1.08241522\n",
      "   3.36663032   1.01860034   0.7999717    0.24398586   0.51599467\n",
      "   0.19677007   1.47186518   1.97633898   0.23312689   1.48833537\n",
      "   1.19485629   0.35998434   6.52770138   0.66936886   0.67954528\n",
      "   0.18716979   1.10548449   1.75959074   0.35115197   0.75688422\n",
      "   0.71104038   2.06782961   2.04123092   1.06774008   0.97802222\n",
      "   5.21162033   1.2553786    0.28643674   1.41262758   4.53845263\n",
      "   1.8322252    3.4650991    1.74966419   1.96462226   1.60311365\n",
      "   5.46570253   1.75205684   0.18341464   1.05822206   1.05277896\n",
      "   0.45829105   0.56932086   1.26366472   2.63987613   1.10204697\n",
      "   0.34568149   1.59185314   0.23531219   1.49723411   1.32848454\n",
      "   0.97342384   0.5593279    0.65789998   0.22317842   0.32443658\n",
      "   4.06266022   0.50142771   0.7117871    5.068995     1.05130363\n",
      "   1.28323793   1.88070416   1.44581723   1.24896765   1.06904602\n",
      "   1.38912416   1.04614103   0.93865579   0.80669641   0.3568694\n",
      "   1.54738545   0.3349086    0.53013521   1.75566733   5.38017225\n",
      "   0.52659547   0.60055584   1.19683242   2.17257786   4.48640919\n",
      "   0.69178331   0.51763958   1.37588596   0.17535125   2.546664\n",
      "   0.85052693   0.40127039   3.39941955   1.72585535   1.88494253\n",
      "   0.54655248   0.87538153   0.63468623   0.15811732   4.12787962\n",
      "   1.63111138   2.1896379    0.89328718   0.60334414   1.66198075\n",
      "   1.30710936   0.61847138   0.4859542    0.26837197   3.3335166\n",
      "   0.76350337   2.69533348   2.19969797   1.20651698   0.45949119\n",
      "   0.81114095   0.75079817   1.14124036   3.45959067   3.91228938\n",
      "   0.34409684   0.98055637   2.60782886   8.04930592   0.75215578\n",
      "   2.00949621   1.63895166   4.58982992   1.69711494   1.43651056\n",
      "   1.85894537   2.43941116   1.56742692   1.64120221   0.52874088\n",
      "   7.60037804   1.11126709   2.27924395   1.59027505   0.36462352\n",
      "   0.49487895   0.86058259   1.20130503   1.44824457   0.71533644\n",
      "   0.73216969   0.59382719   4.51360035   2.20297122   0.99451196\n",
      "   2.16313076   0.19017756   1.5442301    0.5483852    0.6551407\n",
      "   2.14613724   0.69533563   1.43899393   0.5872426    2.19466853\n",
      "   2.55889463   2.81384969   0.36020958   1.81651044   1.13771319\n",
      "   1.58339894   0.66133106   0.38186634   0.99735671   0.51116377\n",
      "   1.00393283   2.44561338   2.5451293    0.61473942   2.51196527\n",
      "   0.37808067   0.42450392   0.43247128   0.40527493   2.20745111\n",
      "   1.706496     3.2036314    1.32543421   0.78269845   1.24423313\n",
      "   0.51146173   1.06251967   0.29026994   0.71855724   1.25915825\n",
      "   1.77133965   3.11975908   1.82038558   0.46681261   1.58360445\n",
      "   1.70594072   0.9710499    0.32841694   1.33704579   1.49918032\n",
      "   1.58311737   0.77236772   2.09277606   1.28884864   2.61186361\n",
      "   3.05380678   0.21936089   0.33384138   0.63795745   0.93909204\n",
      "   0.23269686   0.10943367   0.45170042   0.53691375   0.29002768\n",
      "   0.54687434   1.53735626   1.07569599   0.27943778   0.26690018\n",
      "   1.00939155   1.02597368   1.62725592   1.35139823   1.55965984\n",
      "   0.33892667   0.65361786   0.31751692   0.5732075    0.8205837\n",
      "   0.52867168   0.26527077   4.50323915   0.71796042   0.52115488\n",
      "   1.92565131   0.80890274   0.91383553   0.59226626   1.41297996\n",
      "   0.19173647   0.35197967   0.5909183    1.41856599   0.38053843\n",
      "   1.32074606   1.83593464   0.97786379   0.34049425   1.08132315\n",
      "   2.16607046   0.51510757   0.61548996   0.47512263   1.71465099\n",
      "   2.36562085   3.2835238    0.55038613   4.36125469   0.52668577\n",
      "   1.26036096   0.60892624   1.69978547   2.89509749   0.97882885\n",
      "   1.38829446   2.28078461   3.49751949   0.6909405    0.40564412\n",
      "   0.47750422   7.20261908   0.55076045   1.14341962   1.28865099\n",
      "   1.32431972   5.64335155   0.6494863    1.37421966   1.43908334\n",
      "   3.66759109   0.63589615   2.50989366   1.68316889   1.39914024\n",
      "   1.32528281   0.54492372   1.7458601    0.67903292   0.78267664\n",
      "   2.49241447   1.53624749   1.73055685   2.52876306   0.13626784\n",
      "   0.46656924   1.51084626   0.31437495   1.45831549   1.31841779\n",
      "   0.63747501   0.98700744   0.37286451   0.84301454   0.20916253\n",
      "   0.94894874   3.79341936   1.22801948   1.25030613   2.29087162\n",
      "   0.82161003   2.23215413   0.62037146   4.72267532   0.93043661\n",
      "   1.8072387    0.84138221   1.8511765    2.31805134   0.4321292\n",
      "   4.63574982   0.61925471   1.18001282   1.01325774   0.83008355\n",
      "   3.17197776   0.62303519   2.40693474   0.47089651   0.47354311\n",
      "   1.08123589   1.47478986   0.693726     0.966416     0.27241975\n",
      "   2.95224476   2.07686782   0.61664855   2.16983008   0.24851616\n",
      "   0.70333123   1.48760796   1.70804679   1.86910343   4.08615494\n",
      "   0.8334356    0.63722622   1.24470615   4.06575871   0.20134829\n",
      "   0.66179478   0.39590701   1.13455224   0.2362704    1.51941347\n",
      "   1.20668948   1.13213849   3.83702731   0.64819211   1.43399096\n",
      "   0.45967907   0.41367388   1.56517267   0.64107966]\n",
      "('Epoch:', '40', 'cost', '26.066624212')\n",
      "latent_loss:\n",
      "[ 0.07704604  0.09362659  0.09644678  0.05456644  0.08102527  0.05867589\n",
      "  0.08468756  0.08178091  0.08115909  0.09064171  0.06643766  0.08405221\n",
      "  0.11249205  0.08098388  0.06665272  0.09130633  0.05921307  0.0685364\n",
      "  0.06464574  0.08092555  0.09549889  0.05822918  0.09467286  0.09035915\n",
      "  0.07621667  0.07578316  0.05774939  0.05636051  0.07625189  0.07997066\n",
      "  0.10139084  0.07709363  0.07292157  0.07439116  0.05628216  0.05647495\n",
      "  0.06780145  0.06928134  0.05785909  0.08780161  0.05385521  0.08698332\n",
      "  0.06378716  0.06818163  0.06075844  0.09046367  0.07096145  0.06099308\n",
      "  0.0768064   0.07953703  0.08041498  0.08053043  0.07220927  0.06603825\n",
      "  0.07239196  0.05443582  0.06949008  0.08436316  0.05453819  0.07746616\n",
      "  0.0726099   0.07110325  0.06152651  0.06797242  0.06060526  0.06321561\n",
      "  0.09271121  0.05486155  0.08525124  0.08231476  0.07538006  0.05961555\n",
      "  0.08136418  0.07537284  0.10209626  0.11102787  0.07320839  0.08291069\n",
      "  0.07497716  0.06632853  0.09225476  0.08427113  0.07897186  0.0520238\n",
      "  0.08761755  0.0848121   0.11881739  0.07086685  0.06249717  0.09739023\n",
      "  0.06159976  0.06047773  0.07100114  0.09011984  0.07733914  0.09116212\n",
      "  0.06204104  0.10553408  0.05398598  0.08969977]\n",
      "reconstr_loss\n",
      "[ 23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503]\n",
      "disp\n",
      "[  0.85452676   0.90658516   0.22480501   1.38920557   0.75297076\n",
      "   0.9192695    0.50438064   0.92125225   0.5813269    0.57953155\n",
      "   4.68121958   0.69530845   1.43342233   0.56687146   2.17290998\n",
      "   0.4984206    1.80547452   1.04340792   0.58753407   2.20905566\n",
      "   3.55896521   3.42607546   1.64375854   0.51642758   3.39844871\n",
      "   2.40892053   0.29651257   2.48616552   3.09547019   2.15522933\n",
      "   0.68798721   1.43282568   1.2035712    1.31636262   1.03173923\n",
      "   0.65296239   0.54858261   1.84291244   0.69215518   2.57898235\n",
      "   0.38923749   0.79846931   0.42415842   6.31524658   1.36883402\n",
      "   1.08920288   1.24591672   0.55892086   0.18290035   1.02005494\n",
      "   0.57361019   1.71757078   1.48423815   2.76744652   0.96988612\n",
      "   0.51383251   0.34648144   1.23862648   4.59889889   1.39988673\n",
      "   0.25206581   1.89801204   1.18804991   2.54003954   2.13027883\n",
      "   0.64057636   0.83212972   2.4147923    0.59199536   1.06961966\n",
      "   0.51922435   0.41177151   3.60615945   1.60202384   1.51524258\n",
      "   3.25298858   0.87777662   0.54138887   1.34076309   0.59149963\n",
      "   0.42957991   3.18351507   1.28257811   0.4233529    1.15143657\n",
      "   1.65576804   0.98342013   0.62098056   2.16244221   3.68561053\n",
      "   1.22677422   1.2346431    0.76198804   0.582838     2.82015944\n",
      "   2.01959229   0.36075741   3.53854299   0.39285046   0.87531042\n",
      "   0.78740656   1.27644014   4.16627884   0.4662962    0.57627112\n",
      "   0.8245399    0.76130962   1.1033324    1.37447441   0.5315367\n",
      "   1.33095467   1.72888792   0.88013232   2.37330317   0.06002329\n",
      "   1.56401765   0.99439424   2.0062654    0.13199672   3.8681345\n",
      "   1.12307775   1.42222464   0.30357662   0.77548432   1.98489726\n",
      "   8.89595318   1.38800287   3.06905723   2.62705994   1.98154676\n",
      "   1.1586839    0.56391674   1.15909922   0.42207482   0.44698343\n",
      "   7.64713144   0.8723855    0.32689372   2.06866956   0.91391748\n",
      "   0.96637255   1.08763182   0.77488244   0.8637377    4.41242504\n",
      "   0.50130981   0.59825408   0.5911119    0.96617204   1.52426445\n",
      "   0.53125924   0.79776639   0.77857083   0.51693988   1.85643053\n",
      "   3.34898996   0.26410797   0.6748836    0.57535344   1.81357896\n",
      "   0.51455438   1.20083416   0.23190832   2.30165148   0.99837911\n",
      "   1.14533699   1.13664699   2.1221323    1.02112591   0.87800545\n",
      "   0.54115552   3.27595139   1.62098145   0.34005123   6.71393394\n",
      "   0.90033185   0.74768555   2.15098453   0.88945919   0.53608155\n",
      "   0.62424672   0.1691018   11.11132336   0.48338124   1.02603209\n",
      "   0.43143052   0.66896081   2.1020577    3.47349882   1.42689264\n",
      "   0.92199111   0.579256     0.33042824   0.23173679   0.44252127\n",
      "   1.81065488   4.23947287   0.77450424   0.64383239   0.36827397\n",
      "   0.85454679   1.52020192   6.9618597    5.97189522   1.90506256\n",
      "   1.57905173   0.67379248   1.57226539   1.48975992   0.22624873\n",
      "   3.17012119   1.42634034   1.90562141   0.46754572   0.82095474\n",
      "   0.72834659   0.61935675   0.37305424   0.08300227   0.3212277\n",
      "   1.35412216   1.02126276   1.79377639   0.881657     1.14387798\n",
      "   1.3494401    0.61567181   0.82430255   1.07745826   4.11668539\n",
      "   0.63951182   3.33600426   2.99621511   0.94142759   0.58175945\n",
      "   1.24264622   1.06698799   0.30466965   0.40635532   0.45115495\n",
      "   0.58815593   1.65719652   2.38907862   0.47893187   2.04375839\n",
      "   0.90347302   0.35143965   5.79646015   2.55246711   2.14500499\n",
      "   1.03692091   1.00890005   1.02374053   0.61793828   0.3380639\n",
      "   2.6666894    0.68807524   1.04536104   0.81520176   1.67605019\n",
      "   0.88635778   1.3010571    1.54438448   1.48165751   0.71858811\n",
      "   2.30059528   0.9690308    1.02955747   0.57237023   0.56000715\n",
      "   0.47187099   0.48695892   1.92998159   1.57109356   1.13437879\n",
      "   0.26983711   3.20570612   1.20608532   1.57737088   0.45379809\n",
      "   0.16506821   5.00322866   0.19234534   0.80882108   0.62146527\n",
      "   2.88808537   1.83736181   1.91629946   0.63550597   2.12533569\n",
      "   0.53867751   1.51387501   0.85499513   1.43674231   1.75490427\n",
      "   1.18796182   0.36961636   2.40090513   0.65442258   1.27926445\n",
      "   0.53476566   0.66408747   1.07231176   0.76510876   0.35726392\n",
      "   1.40354896   3.17764449   0.60256582   1.74914742   4.01682234\n",
      "   0.28180176   1.6848309    1.81585646   0.50613302   0.25636905\n",
      "   2.84823656   0.58672816   0.57872486   0.83548105   0.49162117\n",
      "   0.82062215   0.65858757   1.09673953   2.09836459   7.72842026\n",
      "   0.64684808   0.98327613   1.69758415   0.37101877   0.18962459\n",
      "   1.31064212   1.54706883   0.92966819   0.89579195   1.31956244\n",
      "   0.59562522   2.12996793   0.35821512   0.24969386   0.81564844\n",
      "   1.28794813   2.36544561   0.85616398   0.3484953    2.02622247\n",
      "   1.58377504   2.13871837   1.91134262   0.59761101   3.21502304\n",
      "   2.41343856   0.78142995   0.84279358   0.63475233   0.28151688\n",
      "   1.26988661   0.47854352   0.66977      2.65173078   0.88669258\n",
      "   2.18498874   1.22138858   0.68071246   1.00517833   0.5618785\n",
      "   1.66526711   1.27918482   0.51671994   0.24966814   0.70719618\n",
      "   1.61911571   5.09234905   0.77873456   0.42276314   0.52048063\n",
      "   7.80350924   0.68811774   1.30294311   2.22142529   1.92263341\n",
      "   5.33984661   2.488029     0.71631336   1.8215493    0.63494438\n",
      "   0.53624886   0.86629397   4.60496235   0.40715048   1.28900063\n",
      "   3.32888746   3.09665084   0.82323194   0.94406825   2.03481674\n",
      "   1.030339     1.78144312   1.1761086    2.26996446   0.35490948\n",
      "   2.34656429   0.57134509   0.14293681   1.22672915   0.57013279\n",
      "   0.38399473   0.45782122   1.40570068   0.28134799   0.5472225\n",
      "   1.19198549   4.99734879   0.29047158   2.1247685    0.67110246\n",
      "   0.55395919   0.76525563   1.27049541   0.47481427   1.7719593\n",
      "   0.4837178    0.82485759   1.66833651   0.52582568   0.98160899\n",
      "   3.42828274   1.00456262   0.7893374    0.22973745   0.58426028\n",
      "   0.20089257   1.45208001   1.84176028   0.22944404   1.67819309\n",
      "   1.21132302   0.33296835   6.44637537   0.71579123   0.77704465\n",
      "   0.17509614   1.02884662   1.58574462   0.3807531    0.70712954\n",
      "   0.72534478   1.94122362   1.96836662   1.15753686   0.95231795\n",
      "   5.49045515   1.30838275   0.29519394   1.25256598   4.28884506\n",
      "   1.81882143   3.29544926   1.63568413   2.00850487   1.67492628\n",
      "   5.97208166   1.66495097   0.18472797   1.19484687   1.05949664\n",
      "   0.47940761   0.64178163   1.29059267   2.6858573    1.16174304\n",
      "   0.3738395    1.44649518   0.24809355   1.35381305   1.45348144\n",
      "   1.06046379   0.47605979   0.61409646   0.20716101   0.31409925\n",
      "   3.95172453   0.45485649   0.69307286   4.52348042   0.94418341\n",
      "   1.27291811   2.00137472   1.31251919   1.31105387   0.99034077\n",
      "   1.35069466   1.0523231    1.11213207   0.82267106   0.38185686\n",
      "   1.42334127   0.32648224   0.48877272   1.74412096   5.13696003\n",
      "   0.58763456   0.58814085   1.38859653   2.22095203   4.27750731\n",
      "   0.64181334   0.50411218   1.4477942    0.18132597   2.94074392\n",
      "   0.75055379   0.37450203   3.56465411   1.66130006   1.90860283\n",
      "   0.6347242    0.93005705   0.5900135    0.16085312   3.95685601\n",
      "   1.64331651   2.42075086   0.90436786   0.63180703   1.69028127\n",
      "   1.21073365   0.65488517   0.54393792   0.25645217   3.19816899\n",
      "   0.89058477   2.66924858   2.24840903   1.18810034   0.45946324\n",
      "   0.89703369   0.75874662   1.04603589   3.15423346   4.17168045\n",
      "   0.32006973   0.83995211   2.50266242   8.05170918   0.78959447\n",
      "   1.95499778   1.79955137   4.94034433   1.73468292   1.47997546   1.88395\n",
      "   2.35105062   1.62345791   1.70286369   0.51681936   7.78748274\n",
      "   1.1375984    2.29106593   1.35048985   0.36490023   0.54722041\n",
      "   0.8111468    1.3399477    1.45040536   0.8401795    0.63711584\n",
      "   0.5977338    4.63411427   2.07072973   0.97708058   1.96110988\n",
      "   0.21741723   1.67996383   0.58689076   0.66615689   1.9725256\n",
      "   0.75641763   1.54748666   0.60905105   2.38864732   2.5921514\n",
      "   2.78251743   0.36152938   1.72820139   1.30187762   1.67132938\n",
      "   0.61343157   0.41044948   1.07815051   0.54398215   0.91167235\n",
      "   2.54793406   2.51368976   0.64610612   2.92341685   0.38935483\n",
      "   0.41412565   0.41109526   0.40821788   2.16705632   1.75706923\n",
      "   3.76958752   1.30316865   0.78483856   1.22835255   0.57143623\n",
      "   0.947667     0.3021003    0.76351607   1.33151817   1.62600219\n",
      "   3.2567296    1.79792309   0.42785758   1.36094236   1.48669672\n",
      "   0.95191962   0.38697588   1.40624702   1.50197458   1.73760796\n",
      "   0.74147975   2.2329824    1.16324937   2.33948946   3.09785771\n",
      "   0.2010901    0.35277814   0.62491381   0.98048586   0.23980163\n",
      "   0.10500133   0.42485896   0.50249839   0.29995695   0.51177639\n",
      "   1.52258432   1.10984731   0.30296195   0.29916424   1.07409513\n",
      "   1.05233097   1.82060611   1.5029242    1.41850591   0.32053655\n",
      "   0.7473911    0.36974686   0.62251711   0.72249818   0.59494239\n",
      "   0.31201714   4.4777236    0.70612705   0.51492012   1.73853207\n",
      "   0.74381459   0.85068482   0.55146116   1.42851412   0.19195087\n",
      "   0.33204189   0.59048849   1.31923211   0.42311203   1.38552856\n",
      "   1.72394776   0.94617343   0.33111203   1.15593386   2.01191401\n",
      "   0.48947275   0.63818562   0.48828229   1.55190241   2.52741075\n",
      "   3.50289154   0.566405     4.65176868   0.54262549   1.19016302\n",
      "   0.59680402   1.83680165   3.14564705   1.02628994   1.42480898\n",
      "   2.18224478   3.28419781   0.69920683   0.38869607   0.47401661\n",
      "   7.60503674   0.57851392   1.10842061   1.14377141   1.38105047\n",
      "   5.89249611   0.55494165   1.15150249   1.37268865   3.59369612\n",
      "   0.64983213   2.54339886   1.75081396   1.57518137   1.34694004\n",
      "   0.48801979   1.71215463   0.7123614    0.77013826   2.22557282\n",
      "   1.8106724    1.87475586   2.31255293   0.14393285   0.54267591\n",
      "   1.35042942   0.34920955   1.5274781    1.35164154   0.72439063\n",
      "   0.9444502    0.37410036   0.88243049   0.23237202   1.09740424\n",
      "   3.6374898    1.10633349   1.05669355   1.98153043   0.93689471\n",
      "   1.79263902   0.71279657   4.35898447   0.97470659   1.89514375\n",
      "   0.78299862   1.77162433   2.46052504   0.43011373   4.28895807\n",
      "   0.66569722   1.28403032   1.05244946   0.80198407   3.11845493\n",
      "   0.63794172   2.25144291   0.46917593   0.4516609    0.95360404\n",
      "   1.38541055   0.69430643   0.97872216   0.28480694   3.43216586\n",
      "   2.13417959   0.58968163   2.03218627   0.26032183   0.69500238\n",
      "   1.41864181   1.62746978   1.67308784   4.03257036   0.85900629\n",
      "   0.59468973   1.18712258   3.8133688    0.21282968   0.7197051\n",
      "   0.37893066   0.9947477    0.26433694   1.63867927   1.14369535\n",
      "   1.16124165   4.02641821   0.76865351   1.38777041   0.49821576\n",
      "   0.44258973   1.67889524   0.68173051]\n",
      "('Epoch:', '60', 'cost', '25.062897174')\n",
      "latent_loss:\n",
      "[ 0.00976601  0.01120913  0.01069257  0.01105058  0.01052132  0.00820526\n",
      "  0.01271191  0.00786218  0.02349263  0.00590587  0.01199511  0.00671518\n",
      "  0.01640993  0.01428622  0.01420358  0.01471183  0.01584843  0.02042556\n",
      "  0.01820326  0.02090442  0.01668391  0.00670117  0.00682098  0.01098335\n",
      "  0.00764945  0.0121021   0.01113105  0.01224238  0.01603791  0.01960063\n",
      "  0.01466376  0.0088118   0.01599446  0.02065244  0.02162892  0.01753637\n",
      "  0.02447557  0.00811917  0.0178948   0.0139409   0.01084697  0.01958337\n",
      "  0.00921923  0.01378316  0.01250687  0.0104304   0.01046452  0.01035145\n",
      "  0.01730379  0.00749242  0.0150522   0.00703293  0.01187173  0.00952542\n",
      "  0.01204529  0.00732234  0.0096308   0.01225039  0.0137949   0.01092112\n",
      "  0.0105533   0.0099614   0.0134294   0.00646466  0.01236469  0.0095028\n",
      "  0.01408798  0.00738865  0.01748702  0.00845701  0.01288474  0.01888391\n",
      "  0.02300975  0.00941554  0.01955011  0.01526597  0.00819379  0.01460391\n",
      "  0.0125986   0.00771731  0.01570922  0.00813237  0.01451805  0.01154327\n",
      "  0.02486858  0.00894499  0.01775214  0.01982278  0.02266151  0.01009953\n",
      "  0.01012304  0.01993623  0.01581797  0.00883457  0.02224901  0.01461992\n",
      "  0.01288643  0.00848091  0.01031679  0.00904056]\n",
      "reconstr_loss\n",
      "[ 23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503  23.0258503\n",
      "  23.0258503  23.0258503  23.0258503  23.0258503]\n",
      "disp\n",
      "[  0.73896766   0.90318733   0.23682129   1.40231681   0.76679093\n",
      "   0.87786686   0.55248392   0.94332522   0.62930572   0.52272666\n",
      "   5.15031719   0.67712855   1.5386498    0.51902366   2.26687121\n",
      "   0.51179171   2.11806726   1.01698923   0.56158519   2.25197482\n",
      "   4.13750696   3.41113091   1.61730611   0.48810264   3.74849486\n",
      "   2.22375035   0.31306759   2.27835846   3.17842007   2.15830231\n",
      "   0.744973     1.58334613   1.3482393    1.26727915   0.95321292\n",
      "   0.66434181   0.55088884   1.93580759   0.7112112    2.38857603\n",
      "   0.36889768   0.8870641    0.47344241   6.14124632   1.25636172\n",
      "   1.12575245   1.1717068    0.57560635   0.17743492   0.9451052\n",
      "   0.57354772   1.40484059   1.50488746   2.62222195   0.99601221\n",
      "   0.52487594   0.33665699   1.09767973   5.51249504   1.17092681\n",
      "   0.24870533   1.92868495   1.30094802   2.33476925   2.39289427\n",
      "   0.62786442   0.75871766   2.46675396   0.63293171   1.26720774\n",
      "   0.50403887   0.38524705   3.621315     1.51329398   1.4894371\n",
      "   3.21038437   0.808963     0.50810635   1.26408219   0.61679143\n",
      "   0.53920966   2.90685558   1.2714572    0.40169233   1.00476003\n",
      "   1.54364789   0.98781174   0.61281639   2.15706587   3.22012711\n",
      "   1.22797692   1.1683687    0.81132776   0.63380933   2.79419827\n",
      "   2.00478148   0.34724021   3.65766668   0.37416482   0.92317498\n",
      "   0.81773096   1.26284671   3.51754475   0.46069965   0.58363563\n",
      "   0.9190371    0.78435111   1.07654023   1.42258859   0.55720758\n",
      "   1.37818336   1.8700465    0.90210545   2.35536671   0.05610396\n",
      "   1.73981929   1.02670562   1.67986405   0.12260333   4.08022118\n",
      "   1.09692299   1.50847268   0.30951661   0.86727285   2.01214862\n",
      "   8.8214922    1.34152091   3.28345323   2.46678162   1.97774863\n",
      "   1.14241266   0.59946507   1.04834425   0.39738619   0.48026925\n",
      "   6.7914629    0.78621536   0.35255125   1.90017462   0.99483764\n",
      "   0.96986705   1.00962925   0.82557285   0.87754482   3.99304605\n",
      "   0.5336625    0.58145571   0.5936349    1.06715894   1.77322829\n",
      "   0.55671787   0.79703856   0.79671597   0.44150105   1.93906784\n",
      "   3.26222563   0.27714679   0.67161977   0.6160422    1.68640053\n",
      "   0.45538175   1.18915045   0.23303223   2.44310069   1.04354954\n",
      "   1.08956611   1.08317542   2.0573144    1.06219113   0.79274964\n",
      "   0.53133392   3.23461413   1.52765727   0.36689276   6.12484932\n",
      "   0.85706455   0.73439276   2.08244109   0.9019351    0.57039666\n",
      "   0.60400003   0.16874504  12.04577255   0.46937603   1.09079301\n",
      "   0.43593228   0.60200638   1.97565663   3.27718139   1.38373399\n",
      "   0.80041492   0.63609397   0.35233262   0.23831055   0.41195822\n",
      "   2.01539826   4.39139652   0.81933093   0.65475905   0.3255952\n",
      "   0.85458589   1.27696419   7.18677616   6.42194986   1.70267773\n",
      "   1.72779357   0.64553332   1.71043706   1.51158082   0.21888003\n",
      "   3.42238402   1.34110904   2.11266947   0.44443446   1.0049454\n",
      "   0.68556213   0.68231517   0.37211579   0.08637556   0.31604674\n",
      "   1.21820188   0.97700107   1.5699544    0.8763715    1.32475638\n",
      "   1.46974647   0.55774027   0.78997636   0.97916955   3.96752644\n",
      "   0.72209418   3.0229764    3.05598879   0.93526936   0.60141593\n",
      "   1.14959729   1.00945246   0.31465095   0.40335071   0.47597227\n",
      "   0.56937701   1.94635916   2.24758768   0.4506664    2.05642319\n",
      "   0.84300363   0.30029401   5.60708714   2.28556347   2.19560552\n",
      "   1.03043389   1.07461834   1.1146642    0.57524812   0.30320081\n",
      "   2.12912774   0.68555439   1.20043004   0.81820846   1.5983845\n",
      "   0.82595122   1.466097     1.55845761   1.50649571   0.70559418\n",
      "   2.57442546   0.90648276   0.99443656   0.52405095   0.55030447\n",
      "   0.43282154   0.4976559    1.88937926   1.55569923   1.2838527\n",
      "   0.25621411   3.41980314   1.23061609   1.50194955   0.45940417\n",
      "   0.150857     5.04597569   0.22696948   0.85062397   0.72678077\n",
      "   3.11455369   2.05305028   1.70428157   0.58055079   1.91009355\n",
      "   0.47803989   1.37848198   0.8508783    1.34386313   1.64379144\n",
      "   1.17008591   0.34096554   2.97408438   0.58173424   1.21438038\n",
      "   0.49696293   0.70951062   1.05000615   0.7170397    0.34626907\n",
      "   1.36022198   2.86538553   0.6589309    1.40448189   3.88394761\n",
      "   0.29864997   1.68408144   1.89911699   0.48770276   0.22973107\n",
      "   2.82577133   0.5940181    0.65980375   0.84876436   0.41299021\n",
      "   0.88370502   0.69429475   0.99022472   2.32600498   7.97575426\n",
      "   0.63736206   1.0121268    1.72538686   0.41732514   0.1682401\n",
      "   1.19760919   1.55688226   0.93180662   1.02400386   1.47298098\n",
      "   0.62897921   1.96651244   0.34458846   0.26068732   0.80698961\n",
      "   1.28555048   2.25631189   0.89287329   0.35379946   1.89665127\n",
      "   1.46385908   2.15351582   1.83709335   0.64771318   3.143785\n",
      "   2.37096477   0.81973767   0.90323955   0.5893901    0.29630363\n",
      "   1.26761198   0.48586303   0.67319638   2.74673343   0.8162086\n",
      "   2.19543672   1.23484528   0.66543239   1.06953001   0.49939874\n",
      "   1.54695511   1.11623108   0.53525579   0.25661317   0.66142148\n",
      "   1.66668117   5.17828846   0.88396758   0.40889725   0.49768955\n",
      "   8.04134369   0.68716156   1.38954198   2.0230298    1.8105371\n",
      "   5.69026804   2.53532791   0.74257576   1.54159069   0.50759828\n",
      "   0.55296183   0.89696574   4.24335527   0.39042342   1.16292799\n",
      "   3.83649731   2.83163643   0.89016497   0.96821225   2.28735828\n",
      "   0.91869277   1.63176405   1.21622288   2.16557431   0.40187284\n",
      "   2.26805115   0.55858839   0.1405388    1.25461078   0.55452597\n",
      "   0.45412999   0.41091377   1.32447159   0.25990969   0.55708468\n",
      "   1.36195445   4.859025     0.28460622   1.79512608   0.64774615\n",
      "   0.56187266   0.83432353   1.2101227    0.47608003   1.72428691\n",
      "   0.52031875   0.80261397   1.52007341   0.55334735   1.10013413\n",
      "   3.75624275   1.00371265   0.79215288   0.24849716   0.50563693\n",
      "   0.19663879   1.72769678   2.35542655   0.20423411   1.74444783\n",
      "   1.18225396   0.29905099   6.6193428    0.75380695   0.70544189\n",
      "   0.18834703   1.14103925   1.85786104   0.37797749   0.7357589\n",
      "   0.76654857   2.09321117   1.81425786   1.10312045   0.89618957\n",
      "   6.1230011    1.18807256   0.28796422   1.42609692   4.59708691\n",
      "   1.68726301   3.42895317   1.63955235   2.15283918   1.41154933\n",
      "   5.45396662   1.63271129   0.19590804   1.16335642   1.08387625\n",
      "   0.46275845   0.62466103   1.22806287   2.88090372   1.0141151\n",
      "   0.3124935    1.45759654   0.23822352   1.31857324   1.34692919\n",
      "   1.01437259   0.55459642   0.59987688   0.18703276   0.33902043\n",
      "   3.97983789   0.49599746   0.72225255   4.35435009   0.98522866\n",
      "   1.50573123   1.70794892   1.53650582   1.29173636   1.08413363\n",
      "   1.4988277    1.00774443   1.05778921   0.9215309    0.44953597\n",
      "   1.64223289   0.31922323   0.52304822   1.91510427   5.32929039\n",
      "   0.57002544   0.55874443   1.35591412   2.15370631   4.43815899\n",
      "   0.61270559   0.5693146    1.47171724   0.17242123   2.91675282\n",
      "   0.83725905   0.43786305   3.70305133   1.65408337   1.68628776\n",
      "   0.57534045   0.88665044   0.68019724   0.1482362    4.1922884\n",
      "   1.74999642   2.51098204   0.97446781   0.67499977   1.6659596\n",
      "   1.38611674   0.56092972   0.44285709   0.29795042   3.29689026\n",
      "   0.80647737   2.58150864   2.37130666   1.14390099   0.46728942\n",
      "   0.78633881   0.67242229   1.05856848   3.40534639   3.71825814\n",
      "   0.35549968   0.95315838   2.55610585   7.82639551   0.79576689\n",
      "   1.92472064   1.82947075   4.51275301   1.57991612   1.4329195\n",
      "   1.91578305   2.6263268    1.49148822   1.65223336   0.53139287\n",
      "   8.20353699   1.2562654    2.2217834    1.37001824   0.33808166\n",
      "   0.42254227   0.75414836   1.21086073   1.49082589   0.83035153\n",
      "   0.64345694   0.6360569    4.72802401   2.06232119   0.9745127\n",
      "   1.83022428   0.22479968   1.64012027   0.58135647   0.69722474\n",
      "   2.16683245   0.817011     1.61277151   0.63233626   2.26929665\n",
      "   2.89145756   2.82284117   0.35810542   1.8167417    1.2950263\n",
      "   1.6882385    0.60586095   0.45207146   0.98823577   0.59676754\n",
      "   1.04423022   2.57920051   2.97918987   0.58138108   3.0270443\n",
      "   0.38071728   0.395632     0.40554315   0.37894142   2.45291185\n",
      "   1.692801     3.73969674   1.37269425   0.97280037   1.23044491\n",
      "   0.47575355   1.15857959   0.3055948    0.75540292   1.3421483\n",
      "   1.74093688   3.31310606   1.92911458   0.43547508   1.52601135\n",
      "   1.74881423   0.92511702   0.36699739   1.3053683    1.63993955\n",
      "   1.86960614   0.72438633   2.09546947   1.24650204   2.32681251\n",
      "   3.22355986   0.19132715   0.33508462   0.71387839   0.93019456\n",
      "   0.20290935   0.11178418   0.42557114   0.50955778   0.29635751\n",
      "   0.52529591   1.50820136   1.02434111   0.29304171   0.28268695\n",
      "   1.06643426   1.04862833   1.79176331   1.31934452   1.41503048\n",
      "   0.31246975   0.75512928   0.34517699   0.59073442   0.76375723\n",
      "   0.57655585   0.2614803    4.30909967   0.6510306    0.5638386\n",
      "   1.85112524   0.83021492   0.84571862   0.59943944   1.36789107\n",
      "   0.17824318   0.33902562   0.62253475   1.18539059   0.34443179\n",
      "   1.2381084    1.72138965   0.82147598   0.34461415   1.22862661\n",
      "   2.26896334   0.53147453   0.63912332   0.44622135   1.56496179\n",
      "   2.42688942   3.64472914   0.54453677   4.60512924   0.55493355\n",
      "   1.20697343   0.56119102   1.80434275   3.0264914    0.96955401\n",
      "   1.19206333   2.37535453   3.26843071   0.66731441   0.38367581\n",
      "   0.52618754   8.84906387   0.56803721   1.13123369   1.12804019\n",
      "   1.47207093   6.21126461   0.58246166   1.35432231   1.32071316\n",
      "   4.0671463    0.68464023   2.36983776   1.65924239   1.15862155\n",
      "   1.38402438   0.52161789   1.66234517   0.65873778   0.81180716\n",
      "   2.32891083   1.48861933   1.78307843   2.58774614   0.13620718\n",
      "   0.47405815   1.53418601   0.37772709   1.52358043   1.52916431\n",
      "   0.73791969   0.95407212   0.35559821   0.85036677   0.23132712\n",
      "   0.98186404   3.78362751   1.15290391   1.28213954   2.15797758\n",
      "   0.82890558   1.9970572    0.6741581    4.94412136   1.03618896\n",
      "   1.96316648   0.84833938   1.99182057   2.26493955   0.4249565\n",
      "   4.41172886   0.66349775   1.20930922   1.01827073   0.83761591\n",
      "   3.23469448   0.60613638   1.92504287   0.47014502   0.39973679\n",
      "   1.09247482   1.33877873   0.67073524   0.94163936   0.26208612\n",
      "   3.30866098   2.07919955   0.63056791   2.04319811   0.26111296\n",
      "   0.65685785   1.4971776    1.61757398   1.77114391   4.30218792\n",
      "   0.86632067   0.62364173   1.2295593    4.04006147   0.1976344\n",
      "   0.66892624   0.40482154   1.09177947   0.23625556   1.72449207\n",
      "   1.13397694   1.20881093   4.12704849   0.78031015   1.59256363\n",
      "   0.47022715   0.39454693   1.73119402   0.6989314 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ed93d832c261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m          n_z=20)  # dimensionality of latent space\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_fct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-ed93d832c261>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network_architecture, learning_rate, transfer_fct, batch_size, training_epochs, display_step)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0msum_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-d1a0c148e905>\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m    173\u001b[0m         opt, cost,l,r,disp = self.sess.run((self.optimizer, self.cost, self.latent_loss, self.reconstr_loss, self.disp), \n\u001b[0;32m--> 174\u001b[0;31m                                   feed_dict={self.x: X})\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(network_architecture, learning_rate=0.001, transfer_fct=tf.nn.softplus,\n",
    "          batch_size=100, training_epochs=100, display_step=20):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size,\n",
    "                                 transfer_fct=tf.nn.softplus)\n",
    "    \n",
    "    sum_cost = 0.0\n",
    "    for step in range(training_epochs):\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        cost,l,r,disp = vae.partial_fit(batch_data)\n",
    "        sum_cost += cost\n",
    "        if ((step % display_step == 0) & (step!=0)):\n",
    "            avg_step = sum_cost/step\n",
    "            print(\"Epoch:\", '%d' % (step), \"cost\", \"{:.9f}\".format(avg_step))\n",
    "            print('latent_loss:')\n",
    "            print(l)\n",
    "            print('reconstr_loss')\n",
    "            print(r)\n",
    "            print('disp')\n",
    "            print(disp)\n",
    "            \n",
    "    return vae\n",
    "\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=1000, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=500, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=1000, # 2nd layer decoder neurons\n",
    "         n_input=28*28, # MNIST data input (img shape: 28*28)\n",
    "         n_z=20)  # dimensionality of latent space\n",
    "\n",
    "vae = train(network_architecture, training_epochs=20000, learning_rate=0.001, transfer_fct=tf.nn.softplus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
