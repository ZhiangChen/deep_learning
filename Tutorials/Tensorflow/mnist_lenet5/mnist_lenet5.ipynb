{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST LeNet5\n",
    "---\n",
    "Zhiang Chen\n",
    "\n",
    "July 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LeNet5: Gradient-Based Learning Applied to Document Recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "<img src=\"lenet5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Hyperparameters to Explore\n",
    "---\n",
    "1. the number of convnet layers\n",
    "2. the number of maxpooling layers followed by convnet\n",
    "3. the depth of each convnet\n",
    "4. the size of batch (if Stochastic Gradient Descent)\n",
    "5. the size of patch for each convnet, and same patching or valid patching\n",
    "6. the number of neurons in each fully connected layer\n",
    "7. softmax (or hierarchical softmax)\n",
    "8. constant learning rate (or exponential decay learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Pre-process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (55000, 28, 28, 1) (55000, 10)\n",
      "Validation set (5000, 28, 28, 1) (5000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_channels = 1 # greyscale\n",
    "\n",
    "train_data = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "validation_data = mnist.validation.images\n",
    "validation_labels = mnist.validation.labels\n",
    "test_data = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "def reformat(data):\n",
    "    reformated_data = data.reshape(-1, image_size, image_size, num_channels).astype(np.float32)\n",
    "    return reformated_data\n",
    "\n",
    "train_dataset = reformat(train_data)\n",
    "validation_dataset = reformat(validation_data)\n",
    "test_dataset = reformat(test_data)\n",
    "\n",
    "# print out all data shapes\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', validation_dataset.shape, validation_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define Costumed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "kernel_size = 2\n",
    "depth1 = 6 #the depth of 1st convnet\n",
    "depth2 = 16 #the depth of 2nd convnet\n",
    "C5_units = 120\n",
    "F6_units = 84\n",
    "F7_units = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    # convolution's input is a tensor of shape [batch,in_height,in_width,in_channels]\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 10))\n",
    "    tf_valid_dataset = tf.constant(validation_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables(weights and biases)\n",
    "    C1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "    # convolution's weights are called filter in tensorflow\n",
    "    # it is a tensor of shape [kernel_hight,kernel_width,in_channels,out_channels]\n",
    "    C1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "                            \n",
    "    # S1_weights # Sub-sampling doesn't need weights and biases\n",
    "    # S1_biases\n",
    "    \n",
    "    C3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "    C3_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "                            \n",
    "    # S4_weights\n",
    "    # S4_biases\n",
    "     \n",
    "    # C5 actually is a fully-connected layer                        \n",
    "    C5_weights = tf.Variable(tf.truncated_normal([5 * 5 * depth2, C5_units], stddev=0.1))\n",
    "    C5_biases = tf.Variable(tf.constant(1.0, shape=[C5_units]))\n",
    "         \n",
    "    F6_weights = tf.Variable(tf.truncated_normal([C5_units,F6_units], stddev=0.1))\n",
    "    F6_biases = tf.Variable(tf.constant(1.0, shape=[F6_units]))\n",
    "                                \n",
    "    # FC and logistic regression replace RBF\n",
    "    F7_weights = tf.Variable(tf.truncated_normal([F6_units,F7_units], stddev=0.1))\n",
    "    F7_biases = tf.Variable(tf.constant(1.0, shape=[F7_units]))\n",
    "\n",
    "    # Model\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, C1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + C1_biases) # relu is better than tanh\n",
    "        \n",
    "        max_pool = tf.nn.max_pool(hidden,[1,kernel_size,kernel_size,1],[1,2,2,1],'VALID')\n",
    "        hidden = tf.nn.relu(max_pool)\n",
    "                                \n",
    "        conv = tf.nn.conv2d(hidden, C3_weights, [1, 1, 1, 1], padding='VALID')\n",
    "        hidden = tf.nn.relu(conv + C3_biases)\n",
    "\n",
    "        max_pool = tf.nn.max_pool(hidden,[1,kernel_size,kernel_size,1],[1,2,2,1],'VALID')\n",
    "        hidden = tf.nn.relu(max_pool)\n",
    "                            \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, C5_weights) + C5_biases)\n",
    "                            \n",
    "        fc = tf.matmul(hidden,F6_weights)\n",
    "        hidden = tf.nn.relu(fc + F6_biases)\n",
    "        \n",
    "        fc = tf.matmul(hidden,F7_weights)\n",
    "        output = fc + F7_biases\n",
    "    \n",
    "        return output\n",
    "\n",
    "    \n",
    "    # Training computation.\n",
    "    tf_train_dataset = tf.nn.dropout(tf_train_dataset,0.8) # input dropout\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.616367\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 11.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 500: 0.617377\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.9%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1000: 0.045929\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 94.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1500: 0.254517\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 96.2%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2000: 0.028234\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 96.9%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2500: 0.032960\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 96.8%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3000: 0.010193\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.2%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3500: 0.007643\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.7%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4000: 0.001483\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4500: 0.001738\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.6%\n",
      "--------------------------------------\n",
      "Test accuracy: 98.0%\n",
      "Excution time: 2.33min\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_steps = 5000\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "with tf.Session(graph=graph, config = config) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), validation_labels))\n",
    "      print('--------------------------------------')\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "end_time = time.time()\n",
    "duration = (end_time - start_time)/60\n",
    "print(\"Excution time: %0.2fmin\" % duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
