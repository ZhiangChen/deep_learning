{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning (Tensorflow)\n",
    "---\n",
    "Zhiang Chen\n",
    "\n",
    "June 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deep Learning\n",
    "---\n",
    "### Book\n",
    "http://www.deeplearningbook.org/\n",
    "\n",
    "### Tutorial\n",
    "Udacity\n",
    "\n",
    "Tensorflow Tutorial\n",
    "\n",
    "### Architectures\n",
    "ImageNet Competition Winners:\n",
    "LeNet-5,\n",
    "AlexNet,\n",
    "GoogleNet(Inception Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensorflow\n",
    "---\n",
    "### Atlas9\n",
    "Tensorflow 0.8 (Both Python2/Python3, GPU supported) has been installed on atlas9. Currently, Tensorflow has GPU allocation problems. It can be solved by using ‘BFC’ (Best-fit with coalescing), or manually allocating GPUs. Also see [comments](https://github.com/tensorflow/tensorflow/blob/30b52579f6d66071ac7cdc7179e2c4aae3c9cb88/tensorflow/core/protobuf/config.proto).\n",
    "\n",
    "### Not-MNIST\n",
    "http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\n",
    "\n",
    "### IPython\n",
    "http://jupyter.org/\n",
    "\n",
    "### SkFlow\n",
    "[Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning.](https://github.com/tensorflow/skflow)\n",
    "[CNN example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple ConvNet Example\n",
    "---\n",
    "Input -> ConvNet(Relu) -> MaxPool(Relu) -> FC(Relu) -> FC -> Softmax\n",
    "\n",
    "(Stochastic Gradient Descent & No Addtional Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Define Some Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arithmetic Mean\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Build DNN\n",
    "---\n",
    "Batch: the 'batch' in SGD\n",
    "\n",
    "Patch: the kernel in ConvNet\n",
    "\n",
    "Kernel: the kernel in pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "kernel_size = 2\n",
    "depth = 16  # the output of convnet\n",
    "num_hidden = 64 # the number of hidden units in FC\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  # convolution's input is a tensor of shape [batch,in_height,in_width,in_channels]\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  # convolution's weights are called filter in tensorflow\n",
    "  # it is a tensor of shape [kernel_hight,kernel_width,in_channels,out_channels]\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    \n",
    "    max_pool = tf.nn.max_pool(hidden,[1,kernel_size,kernel_size,1],[1,2,2,1],'SAME')\n",
    "    hidden = tf.nn.relu(max_pool+layer2_biases)\n",
    "    \n",
    "    # 3D -> 2D\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.484023\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 500: 1.412719\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 44.6%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1000: 0.495736\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.5%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1500: 0.796864\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.7%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2000: 0.785993\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2500: 0.365740\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3000: 0.480305\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3500: 0.289803\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4000: 0.421417\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4500: 0.512183\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.3%\n",
      "--------------------------------------\n",
      "Test accuracy: 92.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "'''\n",
    "# use GPU0\n",
    "# allocate CPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "with tf.Session(graph=graph,config=config) as session:\n",
    "'''\n",
    "# use CPU0\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print('--------------------------------------')\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LeNet-5\n",
    "---\n",
    "[LeNet-5](http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf) & input [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.764936\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 500: 0.268959\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1000: 0.437042\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.6%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 1500: 0.628296\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2000: 0.535526\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.4%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 2500: 0.365044\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3000: 0.480113\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.4%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 3500: 0.227741\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.5%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4000: 0.298354\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 4500: 0.500678\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.2%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 5000: 0.642455\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.4%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 5500: 0.303121\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 6000: 0.372572\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 6500: 0.304632\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 7000: 0.417959\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 7500: 0.358068\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 8000: 0.385821\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 8500: 0.085947\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 9000: 0.150334\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 9500: 0.237505\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.3%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 10000: 0.576180\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 10500: 0.405022\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.4%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 11000: 0.692232\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 11500: 0.227261\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 12000: 0.145281\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 12500: 0.031881\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 13000: 0.107632\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 13500: 0.141663\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 14000: 0.521018\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.8%\n",
      "--------------------------------------\n",
      "Minibatch loss at step 14500: 0.092909\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.5%\n",
      "--------------------------------------\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LeNet-5 (Tensorflow, CPU)\n",
    "Zhiang Chen\n",
    "6/2016\n",
    "zxc251@case.edu\n",
    "'''\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "kernel_size = 2\n",
    "depth1 = 6 #the depth of 1st convnet\n",
    "depth2 = 16 #the depth of 2nd convnet\n",
    "C5_units = 120\n",
    "F6_units = 84\n",
    "F7_units = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    # convolution's input is a tensor of shape [batch,in_height,in_width,in_channels]\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables(weights and biases)\n",
    "    C1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "    # convolution's weights are called filter in tensorflow\n",
    "    # it is a tensor of shape [kernel_hight,kernel_width,in_channels,out_channels]\n",
    "    C1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "                            \n",
    "    # S1_weights # Sub-sampling doesn't need weights and biases\n",
    "    # S1_biases\n",
    "    \n",
    "    C3_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "    C3_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "                            \n",
    "    # S4_weights\n",
    "    # S4_biases\n",
    "     \n",
    "    # C5 actually is a fully-connected layer                        \n",
    "    C5_weights = tf.Variable(tf.truncated_normal([5 * 5 * depth2, C5_units], stddev=0.1))\n",
    "    C5_biases = tf.Variable(tf.constant(1.0, shape=[C5_units]))\n",
    "         \n",
    "    F6_weights = tf.Variable(tf.truncated_normal([C5_units,F6_units], stddev=0.1))\n",
    "    F6_biases = tf.Variable(tf.constant(1.0, shape=[F6_units]))\n",
    "                                \n",
    "    # FC and logistic regression replace RBF\n",
    "    F7_weights = tf.Variable(tf.truncated_normal([F6_units,F7_units], stddev=0.1))\n",
    "    F7_biases = tf.Variable(tf.constant(1.0, shape=[F7_units]))\n",
    "\n",
    "    # Model\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, C1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + C1_biases) # relu is better than tanh\n",
    "        \n",
    "        max_pool = tf.nn.max_pool(hidden,[1,kernel_size,kernel_size,1],[1,2,2,1],'VALID')\n",
    "        hidden = tf.nn.relu(max_pool)\n",
    "                                \n",
    "        conv = tf.nn.conv2d(hidden, C3_weights, [1, 1, 1, 1], padding='VALID')\n",
    "        hidden = tf.nn.relu(conv + C3_biases)\n",
    "\n",
    "        max_pool = tf.nn.max_pool(hidden,[1,kernel_size,kernel_size,1],[1,2,2,1],'VALID')\n",
    "        hidden = tf.nn.relu(max_pool)\n",
    "                            \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, C5_weights) + C5_biases)\n",
    "                            \n",
    "        fc = tf.matmul(hidden,F6_weights)\n",
    "        hidden = tf.nn.relu(fc + F6_biases)\n",
    "        \n",
    "        fc = tf.matmul(hidden,F7_weights)\n",
    "        output = fc + F7_biases\n",
    "    \n",
    "        return output\n",
    "\n",
    "    \n",
    "    # Training computation.\n",
    "    tf_train_dataset = tf.nn.dropout(tf_train_dataset,0.8) # input dropout\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "    \n",
    "# training\n",
    "num_steps = 5000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print('--------------------------------------')\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I am doing - Detecting the objects in image\n",
    "---\n",
    "ConvNet is useful to classify objects, but it cannot detect the objects in the image. It cannot tell the position of the objects, or classify two or more objects in one image. Currenty, there are two main solutions.\n",
    "\n",
    "(1) Image Segmentation\n",
    "\n",
    "(2) RNNs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
